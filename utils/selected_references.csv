Paper Title,Conference Directory,Rank,Reference,Author Count
NETNet: Neighbor Erasing and Transferring Network for Better,CVPR_2020,1,"Jifeng Dai, Yi Li, Kaiming He, and Jian Sun. R-FCN: object detection via region-based fully convolutional networks. In NeurIPS , 2016. 1,2",4
NETNet: Neighbor Erasing and Transferring Network for Better,CVPR_2020,2,"Kaiwen Duan, Song Bai, Lingxi Xie, Honggang Qi, Qing- ming Huang, and Qi Tian. Centernet: Keypoint triplets for object detection. In ICCV , 2019. 7",6
NETNet: Neighbor Erasing and Transferring Network for Better,CVPR_2020,3,"Tao Kong, Fuchun Sun, Wen-bing Huang, and Huaping Liu. Deep feature pyramid reconﬁguration for object detection. In ECCV , 2018. 2,7",4
NETNet: Neighbor Erasing and Transferring Network for Better,CVPR_2020,4,"Zhaowei Cai and Nuno Vasconcelos. Cascade R-CNN: delv- ing into high quality object detection. In CVPR , 2018. 2",1
NETNet: Neighbor Erasing and Transferring Network for Better,CVPR_2020,5,"Jiale Cao, Yanwei Pang, Jungong Han, and Xuelong Li. Hi- erarchical shot detector. In ICCV , 2019. 7",4
FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding,CVPR_2020,1,"Hildegard Kuehne, Hueihan Jhuang, Est ´ıbaliz Garrote, Tomaso Poggio, and Thomas Serre. Hmdb: a large video database for human motion recognition. In ICCV , pages 2556–2563. IEEE, 2011. 1,2",5
FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding,CVPR_2020,2,"Moshe Blank, Lena Gorelick, Eli Shechtman, Michal Irani, and Ronen Basri. Actions as space-time shapes. In ICCV , volume 2, pages 1395–1402. IEEE, 2005. 2",5
FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding,CVPR_2020,3,"Fabian Caba Heilbron, Joon-Young Lee, Hailin Jin, and Bernard Ghanem. What do i annotate next? an empirical study of active learning for action localization. In ECCV , 2018. 2",4
FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding,CVPR_2020,4,"Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In CVPR , pages 6299–6308, 2017. 1,2,3,6,8",1
FineGym: A Hierarchical Video Dataset for Fine-grained Action Understanding,CVPR_2020,5,"Guilhem Ch ´eron, Ivan Laptev, and Cordelia Schmid. P-cnn: Pose-based cnn features for action recognition. In ICCV , pages 3218–3226, 2015. 3",3
A Quantum Computational Approach to Correspondence Problems on Point Sets,CVPR_2020,1,"Andris Ambainis and Robert ˇSpalek. Quantum algorithms for matching and network ﬂows. In STACS , 2006.",1
A Quantum Computational Approach to Correspondence Problems on Point Sets,CVPR_2020,2,"Mohammad H. S. Amin. Consistency of the adiabatic theo- rem. Physical Review Letters , 102:220401, 2009.",2
A Quantum Computational Approach to Correspondence Problems on Point Sets,CVPR_2020,3,"Steven H. Adachi and Maxwell P. Henderson. Application of Quantum Annealing to Training of Deep Neural Networks. arXiv e-prints , 2015.",2
A Quantum Computational Approach to Correspondence Problems on Point Sets,CVPR_2020,4,"Dorit Aharonov, Wim van Dam, Julia Kempe, Zeph Landau, Seth Lloyd, and Oded Regev. Adiabatic quantum computa- tion is equivalent to standard quantum computation. SIAM J. Comput. , 37(1):166–194, 2007.",6
A Quantum Computational Approach to Correspondence Problems on Point Sets,CVPR_2020,5,"be aligned to multiple references, the corresponding Φcan be obtained by reusing/bracketleftbig [Q1ym]T[Q2ym]T...[QKym]T/bracketrightbigT (which has to be computed only once). The ﬁrst qubit of q has to be ﬁxed to |1/an}bracketri}ht, since the ﬁrst element of every column contains a reference point which has to be active during the entire optimisation. The unembedding is performed simi- larly to the case of transformation estimation, see Sec. 5. 6.2. Complexity to Prepare P=ΦΦT To prepare Φ,O(KDNξ)andO(KDN¯Lξ)operations are required for the transformation estimation and point set alignment, respectively. ξdenotes the number of op- erations for multiplying ymwith one element of the ad- ditive basis Qk. To obtain the ﬁnal P, we need to trans- poseΦand multiply ΦwithΦTwhich, in the worst case, takesO(K2DN)operations for the transformation estima- tion and O(K2DN¯L)operations for the point set align- ment. There are also slightly faster algorithms for matrix multiplication compared to the na ¨ıve way [ 20].TEK 10 20 30 40 50 e2D0.0230.0260.0410.0780.170.3 σ2D0.0120.0130.0120.0120.0120.013 eR0.0580.0620.0830.220.470.764 σR0.0410.0440.0410.0360.0310.03 Table 2: The accuracy of QA under random initial misalignments, for the transformation estimation (” TE”) and point set alignment ( K >1). 7. Experimental Evaluation The current generation of D-WA VE annealers does not support the precision of weights in Pnecessary for our method [ 22]3. It is foreseeable that future generations will enable a higher accuracy for couplings. We thus implement and test QA with an AQC sampler on a conventional com- puter (Intel i7-6700K CPU with 32GB RAM). All quanti- tative tests are performed with 21binary variables corre- sponding to the size of the Qbasis in 2D. We report two error metrics, i.e., the alignment error e2Dand the transformation discrepancy eR, together with their standard deviations denoted by σ2DandσR, respec- tively. The alignment error e2D=/bardblRY−X/bardblHS /bardblX/bardblHS(/bardbl·/bardblHSde- notes the Hilbert-Schmidt norm) measures how accurately the aligned shape coincides with the reference and requires ground truth correspondences. The transformation discrep- ancy is deﬁned as eR=/vextenddouble/vextenddoubleI−RRT/vextenddouble/vextenddouble HS, whereRis the recovered rotation. It measures how closely the recovered transformation resembles a valid rigid transformation. The usage of two complementary metrics is necessary because a loweRdoes not automatically imply an accurate registra- tion. On the other hand, a low e2Ddoes not quantify how rigid the recovered transformation is. Datasets and Proof of Concept. We use four 2D datasets, i.e., ﬁsh [46],qubit ,kanji andcomposer with cardinalities varying from 91(ﬁsh) to7676 (composer ), see Fig. 1for qualitative registration results. For point sets with up to a few thousand points, the simulation time τP<1sec. For ∼7.7k,τPgrows to20.178sec (by a factor of ∼104). Sim- ulation with n= 30 takes already ∼2.5days. More binary variables allow for more elements in the basis Qresulting in more accurate alignment. Note that even with 80qubits, i.e.,for problems with n= 80 , annealing on AQC takes around100ms. A simulation with n= 80 is not possible even on a conventional supercomputer in a reasonable time. Initial Misalignment and Point Linking. We test how ac- curately our method recovers the transformation under the random angle of initial misalignment θand the different size of the point linking region. We generate 500random trans- formations in the range θ∈[0;2π]of the ﬁshdataset and resolve them with QA, for each K∈ {1,10,20,30}. The results are summarised in Table 2. We see that e2Dcorre- 3the current generation natively supports 9-bit ﬂoating-point numbers 9188 1 1 0 1 9 2 8 3 7 4 6 5 5 6 4 7 3 8 2 9 1 0 0 . 5 1 1 . 5 1 1 0 1 9 2 8 3 7 4 6 5 5 6 4 7 3 8 2 9 1 0 0 . 2 0 . 4 0 . 6 0 . 8 1 K A/ K CoCoCoCoCoCo = 0 0 . 7 9 1 . 5 7 2 . 3 6 3 . 1 4 3 . 9 3 4 . 7 1 5 . 5 0 0 . 2 0 . 4 0 . 6 0 . 8 0 0 . 7 9 1 . 5 7 2 . 3 6 3 . 1 4 3 . 9 3 4 . 7 1 5 . 5 0 0 . 1 0 . 2 0 . 3 0 . 4 4 05 0 051 01 52 02 53 03 54 04 55 0 0 0 . 0 5 0 . 1 0 . 1 5 0 . 2 051 01 52 02 53 03 54 04 55 0 0 0 . 0 2 0 . 0 4 0 . 0 6 0 . 0 8 1 0 K = 5 1 5 2 0 Co CoCo Co1 0 K = 1 2 03 0 B/ no i s e r a t i o , % ( t e mp l a t e ) no i s e r a t i o , % ( t e mp l a t e ) C/ Figure 3: The metrics as the functions of A/:the size of the point inter- action region parametrised by K;B/:the angle of initial misalignment θ; C/:the template noise ratio. lates with eRfor all tested K. ForK= 30 — which cor- responds to one third of the template points — both metrics are still comparably low. We also study how the choice of the point interaction region or Kaffects the accuracy of the transformation recovery and plot e2DandeRas the func- tions ofKfor several angles of initial misalignment θin Fig.3-A. Interacting points are determined with the Knear- est neighbour rule for each xn. Recall that according to the singularity theorem [ 28], the globally multiply-linked align- ment (here, K= 91 ) results in a shrinkage of the template to a single point, which is observed experimentally. Next, we systematically vary the angle of initial mis- alignment θin the range [0;2π]with the angular stepπ 36 and report e2DandeRas the functions of θ, forK∈ {1,10,20,30,40,50}. This test reveals the differences in the transformations caused by θ, which arise due to the com- position and the expressiveness of the chosen basis M, see Fig. 3-B. QA is almost agnostic to θ, which is a desirable property of every point set alignment method. Sensitivity to Noise. We systematically add uniformly dis- tributed noise to the template and test the robustness of the proposed QA to outliers in the data, since real data often contains outliers. The highest template noise ratio amounts to50%. Each metric for every noise ratio and every Kis averaged over 50runs, see Fig. 3-C.σRandσ2Ddo not exceed0.057and0.03, respectively. We observe both the increasing alignment error and the discrepancy in the ob- tained transformations with the increasing noise level. For smallK, nonetheless, even large noise ratios seem not to inﬂuence the metrics signiﬁcantly. Spectral Gap Analysis. Spectral gap ∆(ˆH)is the differ- ence between the energy of the ground state and the second- lowest eigenstate. Each problem has an intrinsic and unique ∆(ˆH). Even though a rigorous analysis of the spectral gap is out of the scope of this paper, we make several qualitative observations about the energy landscape of QA, the differ- ence in the energy values and the corresponding registra- tions for one exemplary problem. In Fig. 4, we plot the se- 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 2500.010.1110100 Column B Column C Column D 1 5 9 13 17 21 25 1 6 11 16 21900110013001500170019002100230025002700 energy-decreasing transition energy-decreasing transitionenergy energyK = 1 K = 3 0 Figure 4: The sequences of energy-decreasing transitions and the corre- sponding energy values observed in our sampler, for transformation esti- mation (K= 1) and point set alignment with K= 30 interactions per xn. Besides the graphs, we visualise alignment results for selected energy values and the angle of initial misalignment θ∈/braceleftbigπ 8,π 4,π 2/bracerightbig . quences of energy-decreasing transitions together with the energy values in the experiment with ﬁsh, for three θvalues. We notice that some solutions have very small differences in the energies and are qualitatively indistinguishable from each other. This is accounted for by the choice of the ad- ditive basis, i.e.,that the same alignment can be encoded in different ways. In contrast, we see signiﬁcant differences in the energy values of the qualitatively different solutions (orders of magnitudes larger in the analysed experiment). We conclude that even though ∆(ˆH)is small, the align- ments corresponding to several few lowest eigenstates are qualitatively similar. This suggests that our selection of the basis leads to problems with sufﬁcient spectral gaps. 8. Conclusions This paper introduces AQC for the computer vision com- munity and shows that fundamental low-level problems can be brought to a representation suitable for solving on AQC. In simulations on a classical computer and in a wide range of scenarios, our QA is shown to successfully recover 2D transformations which are close approximations of globally optimal transformations. With the chosen basis of 20el- ements, the solutions result in low transformation discrep- ancy and alignment errors. Observations on how to avoid singularities as well as the noise sensitivity and spectal gap analysis complement the experimental section. In future work, our technique can be extended to afﬁne transformations and other related computer vision prob- lems. We hope to see more research on computer vision methods with quantum hardware in the next decades. Acknowledgements. This work was supported by the ERC Con- solidator Grant 770784. VG is grateful to Polina Matveeva for many enlightening discussions on the physical foundations of adi- abatic quantum computing. The authors thank Bertram Taetz and Hanno Ackermann for reviewing an earlier version of this paper. 9189 References",2
Search to Distill: Pearls are Everywhere but not the Eyes,CVPR_2020,1,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531 , 2015. 1,2,8",3
Search to Distill: Pearls are Everywhere but not the Eyes,CVPR_2020,2,"Byeongho Heo, Jeesoo Kim, Sangdoo Yun, Hyojin Park, No- jun Kwak, and Jin Young Choi. A comprehensive overhaul of feature distillation, 2019. 1,3",6
Search to Distill: Pearls are Everywhere but not the Eyes,CVPR_2020,3,"Xiao Jin, Baoyun Peng, Yichao Wu, Yu Liu, Jiaheng Liu, Ding Liang, Junjie Yan, and Xiaolin Hu. Knowledge dis- tillation via route constrained optimization. arXiv preprint arXiv:1904.09149 , 2019. 7,8",8
Search to Distill: Pearls are Everywhere but not the Eyes,CVPR_2020,4,"Geoffrey Hinton, Li Deng, Dong Yu, George Dahl, Abdel- rahman Mohamed, Navdeep Jaitly, Andrew Senior, Vin- cent Vanhoucke, Patrick Nguyen, Brian Kingsbury, and Tara Sainath. Deep neural networks for acoustic modeling in speech recognition. IEEE Signal Processing Magazine , 29, November 2012. 1",11
Search to Distill: Pearls are Everywhere but not the Eyes,CVPR_2020,5,"J. Bergstra, D. Yamins, and D. D. Cox. Making a science of model search, 2012. 1",2
In Perfect Shape: Certiﬁably Optimal,CVPR_2020,1,"Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Gir- shick. Mask R-CNN. In Intl. Conf. on Computer Vision (ICCV) , pages 2980–2988, 2017. 1",4
In Perfect Shape: Certiﬁably Optimal,CVPR_2020,2,"Berthold K. P. Horn. Closed-form solution of absolute orien- tation using unit quaternions. J. Opt. Soc. Amer. , 4(4):629– 642, Apr 1987. 4",2
In Perfect Shape: Certiﬁably Optimal,CVPR_2020,3,"MOSEK ApS. The MOSEK optimization toolbox for MAT- LAB manual. Version 8.1. , 2017. 6",2
In Perfect Shape: Certiﬁably Optimal,CVPR_2020,4,"Mathieu Aubry, Daniel Maturana, Alexei A Efros, Bryan C Russell, and Josef Sivic. Seeing 3D chairs: exemplar part- based 2D-3D alignment using a large dataset of CAD mod- els. In IEEE Conf. on Computer Vision and Pattern Recog- nition (CVPR) , pages 3762–3769, 2014. 1",5
In Perfect Shape: Certiﬁably Optimal,CVPR_2020,5,"Andrew Blake and Andrew Zisserman. Visual reconstruc- tion. MIT Press, 1987. 2,6",1
Two-shot Spatially-varying BRDF and Shape Estimation,CVPR_2020,1,"S. Bell, K. Bala, and N. Snavely. Intrinsic images in the wild. InACM Transactions on Graphics (SIGGRAPH) , 2014. 3",2
Two-shot Spatially-varying BRDF and Shape Estimation,CVPR_2020,2,"H. Barrow and J. M. Tenenbaum. Recovering intrinsic scene characteristics from images. Computer Vision Systems , 1978. 3",2
Two-shot Spatially-varying BRDF and Shape Estimation,CVPR_2020,3,"M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghe- mawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y . Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Man ´e, R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V . Vanhoucke, V . Vasudevan, F. Vi ´egas, O. Vinyals, P. War- den, M. Wattenberg, M. Wicke, Y . Yu, and X. Zheng. Tensor- Flow: Large-scale machine learning on heterogeneous sys- tems, 2015. Software available from tensorﬂow.org. 5",2
Two-shot Spatially-varying BRDF and Shape Estimation,CVPR_2020,4,"M. Aittala, T. Aila, and J. Lehtinen. Reﬂectance modeling by neural texture synthesis. In ACM Transactions on Graphics (ToG) , 2018. 2",2
Two-shot Spatially-varying BRDF and Shape Estimation,CVPR_2020,5,"M. Aittala, T. Weyrich, and J. Lehtinen. Practical svbrdf capture in the frequency domain. In ACM Transactions on Graphics (SIGGRAPH) , 2013. 2",2
Decoupled Representation Learning for Skeleton-Based Gesture Recognition,CVPR_2020,1,"Fabio M Caputo, Pietro Prebianca, Alessandro Carcangiu, Lucio D Spano, and Andrea Giachetti. Comparing 3d trajec- tories for simple mid-air gesture recognition. Computers & Graphics , 73:17–25, 2018.",5
Decoupled Representation Learning for Skeleton-Based Gesture Recognition,CVPR_2020,2,"Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, and Tae-Kyun Kim. First-person hand action bench- mark with rgb-d videos and 3d hand pose annotations. In CVPR , pages 409–419, 2018.",4
Decoupled Representation Learning for Skeleton-Based Gesture Recognition,CVPR_2020,3,"Eshed Ohn-Bar and Mohan Manubhai Trivedi. Hand gesture recognition in real time for automotive interfaces: A multi- modal vision-based approach and evaluations. IEEE trans- actions on intelligent transportation systems , 15(6):2368– 2377, 2014.",1
Decoupled Representation Learning for Skeleton-Based Gesture Recognition,CVPR_2020,4,"Junwu Weng, Mengyuan Liu, Xudong Jiang, and Junsong Yuan. Deformable pose traversal convolution for 3d action and gesture recognition. In ECCV , pages 136–152, 2018.",4
Decoupled Representation Learning for Skeleton-Based Gesture Recognition,CVPR_2020,5,"Wentao Zhu, Cuiling Lan, Junliang Xing, Wenjun Zeng, Yanghao Li, Li Shen, and Xiaohui Xie. Co-occurrence fea- ture learning for skeleton based action recognition using reg- ularized deep lstm networks. In AAAI , 2016. 5760",7
CLAREL: Classiﬁcation via retrieval loss for zero-shot learning,CVPR_2020,1,"Yao-Hung Hubert Tsai, Liang-Kang Huang, and Ruslan Salakhutdinov. Learning robust visual-semantic embeddings. InCVPR , 2017. 3",3
CLAREL: Classiﬁcation via retrieval loss for zero-shot learning,CVPR_2020,2,"Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for zero-shot learning. In CVPR , 2018. 3",4
CLAREL: Classiﬁcation via retrieval loss for zero-shot learning,CVPR_2020,3,"Zeynep Akata, Florent Perronnin, Zaid Harchaoui, and Cordelia Schmid. Label-embedding for image classiﬁcation. TPAMI , 2016. 3",4
CLAREL: Classiﬁcation via retrieval loss for zero-shot learning,CVPR_2020,4,"Zeynep Akata, Scott Reed, Daniel Walter, Honglak Lee, and Bernt Schiele. Evaluation of output embeddings for ﬁne- grained image classiﬁcation. In CVPR , 2015. 3",5
CLAREL: Classiﬁcation via retrieval loss for zero-shot learning,CVPR_2020,5,"Debasmit Das and C Lee. Zero-shot image recognition using relational matching, adaptation and calibration. In Interna- tional Joint Conference on Neural Networks , 2019. 1,3",1
RetinaFace: Single-shot Multi-level Face Localisation in the Wild,CVPR_2020,1,"Jiankang Deng, Qingshan Liu, Jing Yang, and Dacheng Tao. M3 csr: Multi-view, multi-scale and multi-component cas- cade shape regression. IVC, 2016. 1",4
RetinaFace: Single-shot Multi-level Face Localisation in the Wild,CVPR_2020,2,"Martin Koestinger, Paul Wohlhart, Peter M Roth, and Horst Bischof. Annotated facial landmarks in the wild: A large- scale, real-world database for facial landmark localization. InICCV workshops , 2011. 2,5,6",4
RetinaFace: Single-shot Multi-level Face Localisation in the Wild,CVPR_2020,3,"Jinpeng Lin, Hao Yang, Dong Chen, Ming Zeng, Fang Wen, and Lu Yuan. Face parsing with roi tanh-warping. In CVPR , 2019. 1",6
RetinaFace: Single-shot Multi-level Face Localisation in the Wild,CVPR_2020,4,"Hongyu Pan, Hu Han, Shiguang Shan, and Xilin Chen. Mean-variance loss for deep age estimation from a face. In CVPR , 2018. 1",4
RetinaFace: Single-shot Multi-level Face Localisation in the Wild,CVPR_2020,5,"Jing Yang, Jiankang Deng, Kaihua Zhang, and Qingshan Liu. Facial shape tracking via spatio-temporal cascade shape re- gression. In ICCV Workshops , 2015. 1",4
Variable Rate Image Compression with Content Adaptive Optimization,CVPR_2020,1,"Liu H, Chen T, Guo P, et al . Non-local attention optimized deep image compression. arXiv preprint arXiv:1904.09757, 2019.",4
Variable Rate Image Compression with Content Adaptive Optimization,CVPR_2020,2,"Wallace G K. The JPEG still picture compression standard. IEEE transactions on consumer electronics, 38(1): xviii-xxxiv, 1992.",2
Variable Rate Image Compression with Content Adaptive Optimization,CVPR_2020,3,"Rabbani M. JPEG2000: Image compression funda- mentals, standards and practice. Journal of Electronic Imaging, 11(2): 286, 2002.",2
Variable Rate Image Compression with Content Adaptive Optimization,CVPR_2020,4,"Bellard F. BPG Image format. URL https://bellard.org/bpg, 2015.",2
Variable Rate Image Compression with Content Adaptive Optimization,CVPR_2020,5,"Ball ´e J, Laparra V , Simoncelli E P. End-to- end optimized image compression. arXiv preprint arXiv:1611.01704, 2016.",3
Learning Weighted Submanifolds with Variational Autoencoders and,CVPR_2020,1,"Mikhail Postnikov. Riemannian Geometry . Encyclopaedia of Mathem. Sciences. Springer, 2001. 6",2
Learning Weighted Submanifolds with Variational Autoencoders and,CVPR_2020,2,"Diederik P. Kingma and Max Welling. Auto-Encoding Vari- ational Bayes. In Proceedings of the 2nd International Con- ference on Learning Representations (ICLR) , 2014. 2,3",2
Learning Weighted Submanifolds with Variational Autoencoders and,CVPR_2020,3,"Michael E. Tipping and Christopher M. Bishop. Probabilis- tic Principal Component Analysis. Source: Journal of the Royal Statistical Society. Series B (Statistical Methodology) , 61(3):611–622, 1999. 2,6",2
Learning Weighted Submanifolds with Variational Autoencoders and,CVPR_2020,4,"Thomas Fletcher, Conglin Lu, Stephen M Pizer, and Sarang Joshi. Principal geodesic analysis for the study of nonlinear statistics of shape. IEEE transactions on medical imaging , 23(8):995–1005, 2004. 2",4
Learning Weighted Submanifolds with Variational Autoencoders and,CVPR_2020,5,"Bernhard Schoelkopf, Alexander Smola, and Klaus-Robert Mueller. Nonlinear Component Analysis as a Kernel Eigen- value Problem. 1998. 2",3
Multi-scale Interactive Network for Salient Object Detection,CVPR_2020,1,"Shuhan Chen, Xiuli Tan, Ben Wang, and Xuelong Hu. Re- verse attention for salient object detection. In ECCV , pages 234–250, 2018.",4
Multi-scale Interactive Network for Salient Object Detection,CVPR_2020,2,"Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detec- tion. In CVPR , pages 3089–3098, 2018.",3
Multi-scale Interactive Network for Salient Object Detection,CVPR_2020,3,"Paul L Rosin and Yu-Kun Lai. Artistic minimal rendering with lines and blocks. Graphical Models , 75(4):208–229, 2013.",1
Multi-scale Interactive Network for Salient Object Detection,CVPR_2020,4,"Karen Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014.",1
Multi-scale Interactive Network for Salient Object Detection,CVPR_2020,5,"Tiantian Wang, Lihe Zhang, Shuo Wang, Huchuan Lu, Gang Yang, Xiang Ruan, and Ali Borji. Detect globally, reﬁne locally: A novel approach to saliency detection. In CVPR , pages 3127–3135, 2018.",7
"FabSoften: Face Beautiﬁcation via Dynamic Skin Smoothing, Guided",CVPR_2020,1,"Ping Luo, Xiaogang Wang, and Xiaoou Tang. Hierarchical face parsing via deep learning. In 2012 IEEE Conference on Computer Vision and Pattern Recognition , pages 2480– 2487. IEEE, 2012. 4[21] D. Min, S. Choi, J. Lu, B. Ham, K. Sohn, and M. N. Do. Fast global image smoothing based on weighted least squares. In IEEE Trans. Image Procsessing , volume 12, pages 5638– 5653, 2014. 1,2,4,6,7,8",3
"FabSoften: Face Beautiﬁcation via Dynamic Skin Smoothing, Guided",CVPR_2020,2,"S. L. Phung, A. Bouzerdoum, and D. Chai. Skin segmenta- tion using color pixel classiﬁcation: Analysis and compari- son. IEEE Trans. Pattern Anal. Mach. Intell. , 27(1):148–154, 2005. 3",2
"FabSoften: Face Beautiﬁcation via Dynamic Skin Smoothing, Guided",CVPR_2020,3,"Tsung-Shian Huang Wen-Chieh Lin Jung-Hong Chuang Tsung-Ying Lin, Yu-Ting Tsai. Exemplar-based freckle re- touching and skin tone adjustment. Comput. Graph , pages 54–63, 2019. 2",2
"FabSoften: Face Beautiﬁcation via Dynamic Skin Smoothing, Guided",CVPR_2020,4,"Modiface - augmented reality, 2019. Accessed on: Feb 22, 2020. [Online]. Available: http://modiface.com/ .1, 2,6,7,8",2
"FabSoften: Face Beautiﬁcation via Dynamic Skin Smoothing, Guided",CVPR_2020,5,"B612 beauty and ﬁlter camera, 2020. Accessed on: Feb 22, 2020. [Online]. Available: https://b612.snow.me/ . 1,2,6,7,8",1
Visual-textual Capsule Routing for Text-based Video Segmentation,CVPR_2020,1,"Hueihan Jhuang, Juergen Gall, Silvia Zufﬁ, Cordelia Schmid, and Michael J Black. Towards understanding ac- tion recognition. In Computer Vision (ICCV), 2013 IEEE International Conference on , pages 3192–3199. IEEE, 2013.",5
Visual-textual Capsule Routing for Text-based Video Segmentation,CVPR_2020,2,"Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in pho- tographs of natural scenes. In Proceedings of the 2014 con- ference on empirical methods in natural language processing (EMNLP) , pages 787–798, 2014.[16] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.",4
Visual-textual Capsule Routing for Text-based Video Segmentation,CVPR_2020,3,"Rodney LaLonde and Ulas Bagci. Capsules for object seg- mentation. arXiv preprint arXiv:1804.04241 , 2018.",1
Visual-textual Capsule Routing for Text-based Video Segmentation,CVPR_2020,4,"Zhenyang Li, Ran Tao, Efstratios Gavves, Cees GM Snoek, Arnold WM Smeulders, et al. Tracking by natural language speciﬁcation. In CVPR , volume 1, page 5, 2017.",6
Visual-textual Capsule Routing for Text-based Video Segmentation,CVPR_2020,5,"Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision , pages 2425– 2433, 2015.",7
Breaking the cycle—Colleagues are all you need,CVPR_2020,1,"P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-image translation with conditional adversarial networks. In The IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR) , 2017. 2",2
Breaking the cycle—Colleagues are all you need,CVPR_2020,2,"A. Almahairi, S. Rajeshwar, A. Sordoni, P. Bachman, and A. Courville. Augmented CycleGAN: learning many-to- many mappings from unpaired data. In 35th International Conference on Machine Learning (ICML) , 2018. 5",2
Breaking the cycle—Colleagues are all you need,CVPR_2020,3,"A. Anoosheh, E. Agustsson, and R. Timofte. Combo- gan: Unrestrained scalability for image domain translation. InIEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , pages 896–8967, 2018. 1",2
Breaking the cycle—Colleagues are all you need,CVPR_2020,4,"M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gener- ative adversarial networks. In 34th International Conference on Machine Learning (ICML) , 2017. 2",2
Breaking the cycle—Colleagues are all you need,CVPR_2020,5,"S. Benaim and L. Wolf. One-sided unsupervised domain mapping. In Advances in neural information processing sys- tems, 2017. 2",2
Multi-Modal Graph Neural Network for Joint Reasoning,CVPR_2020,1,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans- formers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human LanguageTechnologies, Volume 1 (Long and Short Papers) , pages 4171–4186, 2019. 6",4
Multi-Modal Graph Neural Network for Joint Reasoning,CVPR_2020,2,"Ross Girshick. Fast r-cnn. In Proceedings of the IEEE In- ternational Conference on Computer Vision (ICCV) , pages 1440–1448, 2015. 3",2
Multi-Modal Graph Neural Network for Joint Reasoning,CVPR_2020,3,"Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR) , pages 39–48, 2016. 2",4
Multi-Modal Graph Neural Network for Joint Reasoning,CVPR_2020,4,"Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780, 1997. 4,5",1
Multi-Modal Graph Neural Network for Joint Reasoning,CVPR_2020,5,"Ronghang Hu, Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Kate Saenko. Learning to reason: End-to-end module networks for visual question answering. In Proceed- ings of the IEEE International Conference on Computer Vi- sion (ICCV) , pages 804–813, 2017. 2",5
Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud,CVPR_2020,1,"X. Chen, H. Ma, J. Wan, B. Li, and T. Xia. Multi-view 3d object detection network for autonomous driving. In 2017 IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) , pages 6526–6534, July 2017. 2",2
Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud,CVPR_2020,2,"Loic Landrieu and Martin Simonovsky. Large-scale point cloud semantic segmentation with superpoint graphs. In The IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) , June 2018. 1,2",1
Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud,CVPR_2020,3,"B. Yang, W. Luo, and R. Urtasun. Pixor: Real-time 3d object detection from point clouds. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 7652– 7660, June 2018. 2",2
Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud,CVPR_2020,4,"Jon L. Bentley, Donald F. Stanat, and E.Hollins Williams. The complexity of ﬁnding ﬁxed-radius near neighbors. In- formation Processing Letters , 6(6):209 – 212, 1977. 3",2
Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud,CVPR_2020,5,"Yin Bi, Aaron Chadha, Alhabib Abbas, Eirina Bourtsoulatze, and Yiannis Andreopoulos. Graph-based object classiﬁca- tion for neuromorphic vision sensing. In The IEEE Inter- national Conference on Computer Vision (ICCV) , October 2019. 1,2",5
Camera Trace Erasing,CVPR_2020,1,"Gerald C Holst. CCD arrays, cameras, and displays. 1998. 3",1
Camera Trace Erasing,CVPR_2020,2,"Xiangui Kang, Matthew C Stamm, Anjie Peng, and KJ Ray Liu. Robust median ﬁltering forensics using an autoregres- sive model. IEEE Transactions on Information Forensics and Security , 8(9):1456–1468, 2013. 2",4
Camera Trace Erasing,CVPR_2020,3,"Zhenxing Qian and Xinpeng Zhang. Improved anti-forensics of JPEG compression. Journal of Systems and Software , 91:100–108, 2014. 2",1
Camera Trace Erasing,CVPR_2020,4,"Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-D transform- domain collaborative ﬁltering. IEEE Transactions on Image Processing , 16(8):2080–2095, 2007. 2",4
Camera Trace Erasing,CVPR_2020,5,"Abdelrahman Abdelhamed, Stephen Lin, and Michael S. Brown. A high-quality denoising dataset for smartphone cameras. In CVPR , 2018. 2",3
A sparse resultant based method for efﬁcient minimal solvers,CVPR_2020,1,"Martin Bujnak, Zuzana Kukelova, and Tomas Pajdla. 3d re- construction from image collections with a single known fo- cal length. In International Conference on Computer Vision (ICCV) , pages 1803–1810. IEEE, 2009.",3
A sparse resultant based method for efﬁcient minimal solvers,CVPR_2020,2,"Martin Byr ¨od, Klas Josephson, and Kalle ˚Astr¨om. Improv- ing numerical accuracy of gr ¨obner basis polynomial equa- tion solvers. In International Conference on Computer Vi- sion (ICCV) . IEEE, 2007.",3
A sparse resultant based method for efﬁcient minimal solvers,CVPR_2020,3,"John F. Canny and Ioannis Z. Emiris. A subdivision-based algorithm for the sparse resultant. J. ACM , 47(3):417–451, 2000.",2
A sparse resultant based method for efﬁcient minimal solvers,CVPR_2020,4,"Ond ˇrej Chum, Ji ˇr´ı Matas, and Josef Kittler. Locally op- timized ransac. In Pattern Recognition , pages 236–243. Springer Berlin Heidelberg, 2003.",3
A sparse resultant based method for efﬁcient minimal solvers,CVPR_2020,5,"Viktor Larsson, Zuzana Kukelova, and Yinqiang Zheng. Making minimal solvers for absolute pose estimation com- pact and robust. In International Conference on Computer Vision (ICCV) , 2017. 1778",3
HyperSTAR: Task-Aware Hyperparameters for Deep Networks,CVPR_2020,1,"Alessandro Achille, Michael Lam, Rahul Tewari, Avinash Ravichandran, Subhransu Maji, Charless C Fowlkes, Stefano Soatto, and Pietro Perona. Task2vec: Task embedding for meta-learning. In Proc. of the IEEE International Confer- ence on Computer Vision , pages 6430–6439, 2019. 3",8
HyperSTAR: Task-Aware Hyperparameters for Deep Networks,CVPR_2020,2,"R ´emi Bardenet, M ´aty´as Brendel, Bal ´azs K ´egl, and Michele Sebag. Collaborative hyperparameter tuning. In Proc. of the International Conference on Machine Learning , pages 199– 207, 2013. 1,2,3,6",4
HyperSTAR: Task-Aware Hyperparameters for Deep Networks,CVPR_2020,3,"James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. Journal of Machine Learn- ing Research , 13(Feb):281–305, 2012. 1,2",1
HyperSTAR: Task-Aware Hyperparameters for Deep Networks,CVPR_2020,4,"Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. In Proc. of the European Conference on Computer Vision , 2014. 5",3
HyperSTAR: Task-Aware Hyperparameters for Deep Networks,CVPR_2020,5,"Wei Chen, Tie-Yan Liu, Yanyan Lan, Zhi-Ming Ma, and Hang Li. Ranking measures and loss functions in learning to rank. In Proc. of the Advances in Neural Information Pro- cessing Systems , pages 315–323, 2009. 4",5
Adaptive Weighted Attention Network with Camera Spectral Sensitivity Prior,CVPR_2020,1,"Sriharsha Koundinya, Himanshu Sharma, Manoj Sharma, Avinash Upadhyay, Raunak Manekar, Rudrabha Mukhopad- hyay, Abhijit Karmakar, and Santanu Chaudhury. 2d-3d cnn based architectures for spectral reconstruction from rgb im- ages. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops , pages 844–851, 2018.",8
Adaptive Weighted Attention Network with Camera Spectral Sensitivity Prior,CVPR_2020,2,"Hyeokhyen Kwon and Yu-Wing Tai. Rgb-guided hyperspec- tral image upsampling. In Proceedings of the IEEE Inter- national Conference on Computer Vision , pages 307–315, 2015.",1
Adaptive Weighted Attention Network with Camera Spectral Sensitivity Prior,CVPR_2020,3,"Jiaojiao Li, Qian Du, Yunsong Li, and Wei Li. Hyperspectral image classiﬁcation with imbalanced data based on orthogo- nal complement subspace projection. IEEE Transactions on Geoscience and Remote Sensing , 56(7):3838–3851, 2018.",4
Adaptive Weighted Attention Network with Camera Spectral Sensitivity Prior,CVPR_2020,4,"Tarek Stiebel, Simon Koppers, Philipp Seltsam, and Dorit Merhof. Reconstructing spectral images from rgb-images using a convolutional neural network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion Workshops , pages 948–953, 2018.",4
Adaptive Weighted Attention Network with Camera Spectral Sensitivity Prior,CVPR_2020,5,"Ying Fu, Tao Zhang, Yinqiang Zheng, Debing Zhang, and Hua Huang. Joint camera spectral sensitivity selection and hyperspectral image recovery. In Proceedings of the Euro- pean Conference on Computer Vision (ECCV) , pages 788– 804, 2018.",5
IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning,CVPR_2020,1,"Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano- lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University — Princeton University — Toyota Tech- nological Institute at Chicago, 2015. 2,6",2
IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning,CVPR_2020,2,Autism Brain Imaging Data Exchange (ABIDE). http://fcon_1000.projects.nitrc.org/ indi/abide/ .2,2
IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning,CVPR_2020,3,Indian Diabetic Retinopathy Image Dataset (IDRiD). https://idrid.grand-challenge.org/ .2,2
IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning,CVPR_2020,4,LIDC-IDRI. https://wiki. cancerimagingarchive.net/display/Public/ LIDC-IDRI .2,2
IntrA: 3D Intracranial Aneurysm Dataset for Deep Learning,CVPR_2020,5,Medpix. https://medpix.nlm.nih.gov/home .2,2
Gate-Shift Networks for Video Action Recognition,CVPR_2020,1,"I. Loshchilov and F. Hutter. SGDR: Stochastic Gradient De- scent with Warm Restarts. In Proc. ICLR , 2017. 5",2
Gate-Shift Networks for Video Action Recognition,CVPR_2020,2,"Z. Shou, X. Lin, Y . Kalantidis, L. Sevilla-Lara, M. Rohrbach, S. Chang, and Z. Yan. DMC-Net: Generating discriminative motion cues for fast compressed video action recognition. In Proc. CVPR , 2019. 1,2",2
Gate-Shift Networks for Video Action Recognition,CVPR_2020,3,"H. Wang, D. Tran, L. Torresani, and M. Feiszli. Video Modeling with Correlation Networks. arXiv preprint arXiv:1906.03349 , 2019. 7,8",2
Gate-Shift Networks for Video Action Recognition,CVPR_2020,4,"C. Zach, T. Pock, and H. Bischof. A duality based approach for realtime TV-L 1 optical ﬂow. In Joint pattern recognition symposium , pages 214–223, 2007. 2",2
Gate-Shift Networks for Video Action Recognition,CVPR_2020,5,"G. Bertasius, C. Feichtenhofer, D. Tran, J. Shi, and L. Tor- resani. Learning Discriminative Motion Features Through Detection. arXiv preprint arXiv:1812.04172 , 2018. 8",2
Local Context Normalization: Revisiting Local Normalization,CVPR_2020,1,"Yann A LeCun, L ´eon Bottou, Genevieve B Orr, and Klaus- Robert M ¨uller. Efﬁcient backprop. In Neural networks: Tricks of the trade , pages 9–48. Springer, 1998. 2",4
Local Context Normalization: Revisiting Local Normalization,CVPR_2020,2,Chesapeake land cover. Maryland split. 8,2
Local Context Normalization: Revisiting Local Normalization,CVPR_2020,3,"Mart ´ın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Is- ard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Man- junath Kudlur, Josh Levenberg, Dan Man ´e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Tal- war, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fer- nanda Vi ´egas, Oriol Vinyals, Pete Warden, Martin Watten- berg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. Tensor- Flow: Large-scale machine learning on heterogeneous sys- tems, 2015. Software available from tensorﬂow.org. 4",7
Local Context Normalization: Revisiting Local Normalization,CVPR_2020,4,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic image seg- mentation with deep convolutional nets and fully connected CRFs. arXiv preprint arXiv:1412.7062 , 2014. 4",5
Local Context Normalization: Revisiting Local Normalization,CVPR_2020,5,"Chesapeake Conservancy. Land cover data project 2013/2014. https://chesapeakeconservancy. org/conservation-innovation-center/ high-resolution-data/ land-cover-data-project/ , 2016. 8",2
Analyzing and Improving the Image Quality of StyleGAN,CVPR_2020,1,"Rameen Abdal, Yipeng Qin, and Peter Wonka. Im- age2StyleGAN: How to embed images into the StyleGAN latent space? In ICCV , 2019. 7",3
Analyzing and Improving the Image Quality of StyleGAN,CVPR_2020,2,"Michael Albright and Scott McCloskey. Source generator attribution via inversion. In CVPR Workshops , 2019. 8",1
Analyzing and Improving the Image Quality of StyleGAN,CVPR_2020,3,"Carl Bergstrom and Jevin West. Which face is real? http://www.whichfaceisreal.com/learn.html, Accessed November 15, 2019. 1",1
Analyzing and Improving the Image Quality of StyleGAN,CVPR_2020,4,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural image synthesis. CoRR , abs/1809.11096, 2018. 1",3
Analyzing and Improving the Image Quality of StyleGAN,CVPR_2020,5,"Yann N. Dauphin, Harm de Vries, and Yoshua Bengio. Equi- librated adaptive learning rates for non-convex optimization. CoRR , abs/1502.04390, 2015. 5",2
IPG-Net: Image Pyramid Guidance Network for Small Object Detection,CVPR_2020,1,"Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. ICCV , Oct 2017.",7
IPG-Net: Image Pyramid Guidance Network for Small Object Detection,CVPR_2020,2,"Tao Kong, Fuchun Sun, Wenbing Huang, and Huaping Liu. Deep feature pyramid reconﬁguration for object detection. Lecture Notes in Computer Science , page 172–188, 2018.",4
IPG-Net: Image Pyramid Guidance Network for Small Object Detection,CVPR_2020,3,"Joseph Redmon and Ali Farhadi. Yolo9000: Better, faster, stronger. CVPR , Jul 2017.",1
IPG-Net: Image Pyramid Guidance Network for Small Object Detection,CVPR_2020,4,"Yousong Zhu, Chaoyang Zhao, Jinqiao Wang, Xu Zhao, Yi Wu, and Hanqing Lu. Couplenet: Coupling global structure with local parts for object detection. ICCV , Oct 2017.",6
IPG-Net: Image Pyramid Guidance Network for Small Object Detection,CVPR_2020,5,"Jiangmiao Pang, Kai Chen, Jianping Shi, Huajun Feng, Wanli Ouyang, and Dahua Lin. Libra r-cnn: Towards bal- anced learning for object detection, 2019.",6
Context-Aware Attention Network for Image-Text Retrieval,CVPR_2020,1,"Andrej Karpathy and Fei-Fei Li. Deep visual-semantic align- ments for generating image descriptions. In CVPR , 2015.",1
Context-Aware Attention Network for Image-Text Retrieval,CVPR_2020,2,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015.[18] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV , 2017.",2
Context-Aware Attention Network for Image-Text Retrieval,CVPR_2020,3,"Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV , 2014.",3
Context-Aware Attention Network for Image-Text Retrieval,CVPR_2020,4,"V olodymyr Mnih, Nicolas Heess, Alex Graves, and Koray Kavukcuoglu. Recurrent models of visual attention. In NeurIPS , 2014.",4
Context-Aware Attention Network for Image-Text Retrieval,CVPR_2020,5,"Liwei Wang, Yin Li, and Svetlana Lazebnik. Learning deep structure-preserving image-text embeddings. In CVPR , 2016.",3
HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation,CVPR_2020,1,"Markus Oberweger, Paul Wohlhart, and Vincent Lepetit. Generalized feedback loop for joint hand-object pose esti- mation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 2019. 2",3
HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation,CVPR_2020,2,"Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul Baek, and Tae-Kyun Kim. First-person hand action bench- mark with rgb-d videos and 3d hand pose annotations. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) , 2018. 5,6",4
HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation,CVPR_2020,3,"Yujun Cai, Liuhao Ge, Jun Liu, Jianfei Cai, Tat-Jen Cham, Junsong Yuan, and Nadia Magnenat Thalmann. Exploit- ing spatial-temporal relationships for 3d pose estimation via graph convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision , pages 2272– 2281, 2019. 2",7
HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation,CVPR_2020,4,"Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012 , 2015. 5",11
HOPE-Net: A Graph-based Model for Hand-Object Pose Estimation,CVPR_2020,5,"Chiho Choi, Sang Ho Yoon, Chin-Ning Chen, and Karthik Ramani. Robust hand pose estimation during the interaction with an unknown object. In IEEE International Conference on Computer Vision (ICCV) , pages 3123–3132, 2017. 2",4
Polarized Reﬂection Removal with Perfect Alignment in the Wild,CVPR_2020,1,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale hierarchical image database. In CVPR , 2009. 6",6
Polarized Reﬂection Removal with Perfect Alignment in the Wild,CVPR_2020,2,"H. Farid and E. H. Adelson. Separating reﬂections and light- ing using independent components analysis. In CVPR , 1999. 2,6,7",2
Polarized Reﬂection Removal with Perfect Alignment in the Wild,CVPR_2020,3,"Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV , 2016. 6",3
Polarized Reﬂection Removal with Perfect Alignment in the Wild,CVPR_2020,4,"Amit K. Agrawal, Ramesh Raskar, Shree K. Nayar, and Yuanzhen Li. Removing photography artifacts using gradient projection and ﬂash-exposure sampling. TOG , 24(3):828– 835, 2005. 3",2
Polarized Reﬂection Removal with Perfect Alignment in the Wild,CVPR_2020,5,"Nikolaos Arvanitopoulos, Radhakrishna Achanta, and Sabine Susstrunk. Single image reﬂection suppression. In CVPR , 2017. 2",3
Self-Supervised Learning of Local Features in 3D Point Clouds,CVPR_2020,1,"Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classiﬁca- tion and segmentation. Proc. Computer Vision and Pattern Recognition (CVPR), IEEE , 1(2):4, 2017. 1,4",4
Self-Supervised Learning of Local Features in 3D Point Clouds,CVPR_2020,2,"Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in Neural Informa- tion Processing Systems , pages 5099–5108, 2017. 2",4
Self-Supervised Learning of Local Features in 3D Point Clouds,CVPR_2020,3,"Hugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Franc ¸ois Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. ArXiv , abs/1904.08889, 2019. 2",2
Self-Supervised Learning of Local Features in 3D Point Clouds,CVPR_2020,4,"Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springen- berg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE transactions on pattern analysis and machine intelligence , 38(9):1734–1747, 2016. 2",5
Self-Supervised Learning of Local Features in 3D Point Clouds,CVPR_2020,5,"Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un- supervised representation learning by predicting image rota- tions. arXiv preprint arXiv:1803.07728 , 2018. 2",3
Detecting CNN-Generated Facial Images in Real-World Scenarios,CVPR_2020,1,"Haodong Li, Bin Li, Shunquan Tan, and Jiwu Huang. Detec- tion of deep network generated images using disparities in color components. arXiv preprint arXiv:1808.07276 , 2018. 2,3,4",4
Detecting CNN-Generated Facial Images in Real-World Scenarios,CVPR_2020,2,"Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. Mesonet: a compact facial video forgery detection network. In WIFS , 2018. 2",4
Detecting CNN-Generated Facial Images in Real-World Scenarios,CVPR_2020,3,"Michael Albright and Scott McCloskey. Source generator at- tribution via inversion. In CVPR Workshop on Media Foren- sics, pages 96–103, 2019. 3",1
Detecting CNN-Generated Facial Images in Real-World Scenarios,CVPR_2020,4,"Belhassen Bayar and Matthew C Stamm. A deep learning approach to universal image manipulation detection using a new convolutional layer. In ACM IHMS , 2016. 2",1
Detecting CNN-Generated Facial Images in Real-World Scenarios,CVPR_2020,5,"Bolin Chen, Haodong Li, and Weiqi Luo. Image processing operations identiﬁcation via convolutional neural network. arXiv preprint arXiv:1709.02908 , 2017. 4",3
Context-Guided Super-Class Inference for Zero-Shot Detection,CVPR_2020,1,"Christoph H Lampert, Hannes Nickisch, and Stefan Harmel- ing. Learning to detect unseen object classes by between- class attribute transfer. In 2009 IEEE Conference on Com- puter Vision and Pattern Recognition , pages 951–958. IEEE, 2009. 4",3
Context-Guided Super-Class Inference for Zero-Shot Detection,CVPR_2020,2,"Rahman Shaﬁn, Khan Salman, and Barnes Nick. Transduc- tive learning for zero-shot object detection. In Proceedings of the IEEE International Conference on Computer Vision , pages 6082–6091, 2019. 1,2,3",3
Context-Guided Super-Class Inference for Zero-Shot Detection,CVPR_2020,3,"Yu Fisher, Koltun Vladlen, and Funkhouser Thomas. Dilated residual networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 472–480, 2017. 2",3
Context-Guided Super-Class Inference for Zero-Shot Detection,CVPR_2020,4,"Bansal Ankan, Sikka Karan, Sharma Gaurav, Chellappa Rama, and Divakaran Ajay. Zero-shot object detection. In Proceedings of the European Conference on Computer Vi- sion (ECCV) , pages 384–400, 2018. 1,3",5
Context-Guided Super-Class Inference for Zero-Shot Detection,CVPR_2020,5,"Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional trans- formers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the As- sociation for Computational Linguistics: Human Language Technologies , pages 4171–4186, 2019. 4",4
Mitigating Bias in Face Recognition using Skewness-Aware Reinforcement,CVPR_2020,1,"UN General Assembly. Universal declaration of human rights. UN General Assembly , 302(2), 1948.",2
Mitigating Bias in Face Recognition using Skewness-Aware Reinforcement,CVPR_2020,2,"Flavio Calmon, Dennis Wei, Bhanukiran Vinzamuri, Karthikeyan Natesan Ramamurthy, and Kush R Varshney. Optimized pre-processing for discrimination prevention. In Advances in Neural Information Processing Systems , pages 3992–4001, 2017.",5
Mitigating Bias in Face Recognition using Skewness-Aware Reinforcement,CVPR_2020,3,"Meina Kan, Shiguang Shan, and Xilin Chen. Bi-shifting auto-encoder for unsupervised domain adaptation. In ICCV , pages 3846–3854, 2015.",3
Mitigating Bias in Face Recognition using Skewness-Aware Reinforcement,CVPR_2020,4,"Bingyu Liu, Weihong Deng, Yaoyao Zhong, Mei Wang, Jiani Hu, Xunqiang Tao, and Yaohai Huang. Fair loss: Margin-aware reinforcement learning for deep face recog- nition. In Proceedings of the IEEE International Conference on Computer Vision , pages 10052–10061, 2019.",7
Mitigating Bias in Face Recognition using Skewness-Aware Reinforcement,CVPR_2020,5,"Asem Othman and Arun Ross. Privacy of facial soft biomet- rics: Suppressing gender but retaining identity. In European Conference on Computer Vision , pages 682–696. Springer, 2014.",1
A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising,CVPR_2020,1,"Chen Chen, Qifeng Chen, Minh N. Do, and Vladlen Koltun. Seeing motion in the dark. In The IEEE International Con- ference on Computer Vision (ICCV) , October 2019.",3
A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising,CVPR_2020,2,"Alessandro Foi, Mejdi Trimeche, Vladimir Katkovnik, and Karen Egiazarian. Practical poissonian-gaussian noise mod- eling and ﬁtting for single-image raw-data. IEEE Transac- tions on Image Processing , 17(10):1737–1754, 2008.",4
A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising,CVPR_2020,3,"Alessandro Foi. Clipped noisy images: Heteroskedas- tic modeling and practical denoising. Signal Processing , 89(12):2609–2629, 2009.",2
A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising,CVPR_2020,4,"Roberto Costantini and Sabine Susstrunk. Virtual sensor de- sign. Proceedings of SPIE - The International Society for Optical Engineering , 5301:408–419, 2004.",1
A Physics-based Noise Formation Model for Extreme Low-light Raw Denoising,CVPR_2020,5,"Markku Makitalo and Alessandro Foi. Optimal inversion of the anscombe transformation in low-count poisson image de- noising. IEEE Transactions on Image Processing , 20(1):99– 109, 2011.",1
4354,CVPR_2020,1,"D. Gonzlez, J. Prez, V . Milans, and F. Nashashibi. A review of motion planning techniques for automated vehicles. T-ITS , April 2016.",2
4354,CVPR_2020,2,"Yeping Hu, Wei Zhan, and Masayoshi Tomizuka. Proba- bilistic prediction of vehicle semantic intention and motion. arXiv preprint arXiv:1804.03629 , 2018.",3
4354,CVPR_2020,3,"Yuke Li. Which way are you going? imitative decision learn- ing for path forecasting in dynamic scenes. In CVPR , June 2019.",2
4354,CVPR_2020,4,"Yuexin Ma, Xinge Zhu, Sibo Zhang, Ruigang Yang, Wen- ping Wang, and Dinesh Manocha. Trafﬁcpredict: Trajectory prediction for heterogeneous trafﬁc-agents. In AAAI , pages 6120–6127, 2019.",6
4354,CVPR_2020,5,"Osama Makansi, Eddy Ilg, Ozgun Cicek, and Thomas Brox. Overcoming limitations of mixture density networks: A sam- pling and ﬁtting framework for multimodal future prediction. InCVPR , June 2019.",4
Deep Generative Adversarial Residual Convolutional Networks for Real-World,CVPR_2020,1,"Mario Bertero and Patrizia Boccacci. Introduction to inverse problems in imaging . CRC press, 1998. 3",1
Deep Generative Adversarial Residual Convolutional Networks for Real-World,CVPR_2020,2,"Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Foundations and Trends in Machine Learning , pages 1–122, 2011. 3",5
Deep Generative Adversarial Residual Convolutional Networks for Real-World,CVPR_2020,3,"Yunjin Chen and Thomas Pock. Trainable nonlinear reaction diffusion: A ﬂexible framework for fast and effective image restoration. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence , pages 1256–1272, 2017. 3,4",1
Deep Generative Adversarial Residual Convolutional Networks for Real-World,CVPR_2020,4,"Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. ECCV , pages 184–199, 2014. 2",4
Deep Generative Adversarial Residual Convolutional Networks for Real-World,CVPR_2020,5,"Ruicheng Feng, Jinjin Gu, Yu Qiao, and Chao Dong. Sup- pressing model overﬁtting for image super-resolution net- works. IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , pages 0–0, 2019. 6",4
VIOLIN : A Large-Scale Dataset for Video-and-Language Inference,CVPR_2020,1,"Jin-Hwa Kim, Kyoung-Woon On, Woosang Lim, Jeonghee Kim, Jung-Woo Ha, and Byoung-Tak Zhang. Hadamard product for low-rank bilinear pooling. arXiv preprint arXiv:1610.04325 , 2016. 3",6
VIOLIN : A Large-Scale Dataset for Video-and-Language Inference,CVPR_2020,2,"Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu- man action video dataset. arXiv preprint arXiv:1705.06950 , 2017. 3",11
VIOLIN : A Large-Scale Dataset for Video-and-Language Inference,CVPR_2020,3,"Zhou Yu, Jun Yu, Jianping Fan, and Dacheng Tao. Multi- modal factorized bilinear pooling with co-attention learning for visual question answering. In ICCV , 2017. 3",4
VIOLIN : A Large-Scale Dataset for Video-and-Language Inference,CVPR_2020,4,"Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube-8m: A large- scale video classiﬁcation benchmark. arXiv preprint arXiv:1609.08675 , 2016. 3",7
VIOLIN : A Large-Scale Dataset for Video-and-Language Inference,CVPR_2020,5,"Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR , 2016. 3",4
Single-Side Domain Generalization for Face Anti-Spooﬁng,CVPR_2020,1,"Andr ´e Anjos, Murali Mohan Chakka, and S ´ebastien Mar- cel. Motion-based counter-measures to photo attacks in face recognition. Biometrics IET , pages 147–158, 2013.",3
Single-Side Domain Generalization for Face Anti-Spooﬁng,CVPR_2020,2,"Klaus Kollreider, Hartwig Fronthaler, Maycel Isaac Faraj, and Josef Bigun. Real-time face detection and motion anal- ysis with application in “liveness” assessment. Transactions on Information Forensics and Security (TIFS) , pages 548– 558, 2007.",4
Single-Side Domain Generalization for Face Anti-Spooﬁng,CVPR_2020,3,"Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Conference on Computer Vision and Pat- tern Recognition (CVPR) , pages 4690–4699, 2019.",4
Single-Side Domain Generalization for Face Anti-Spooﬁng,CVPR_2020,4,"Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2- constrained softmax loss for discriminative face veriﬁcation. arXiv preprint arXiv:1703.09507 , 2017.",3
Single-Side Domain Generalization for Face Anti-Spooﬁng,CVPR_2020,5,"Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In International Conference on Computer Vision (ICCV) , pages 618–626, 2017.",6
WISH: efﬁcient 3D biological shape classiﬁcation through,CVPR_2020,1,"Guido Gerig, Martin Styner, D Jones, Daniel Weinberger, and Jeffrey Lieberman. Shape analysis of brain ventricles using spharm. In Proc. MMBIA , pages 171–178, 2001. 3",5
WISH: efﬁcient 3D biological shape classiﬁcation through,CVPR_2020,2,"Marco Agus, C. Cali, A. Tapia Morales, H.O. Lehvaslaiho, Pierre Magistretti, Enrico Gobbetti, and Markus Hadwiger. Hyperquadrics for shape analysis of 3D nanoscale reconstruc- tions of brain cell nuclear envelopes. In Proc. Smart Tools and Apps for Graphics , pages 115–122, October 2018. 2",2
WISH: efﬁcient 3D biological shape classiﬁcation through,CVPR_2020,3,"Marco Agus, Maria Veloz Castillo, Javier F. Garnica Molina, Enrico Gobbetti, Heikki Lehvaslaiho, Alex Morales Tapia, Pierre Magistretti, Markus Hadwiger, and Corrado Cal ´ı. Shape analysis of 3d nanoscale reconstructions of brain cell nuclear envelopes by implicit and explicit parametric repre- sentations. Computers & Graphics , 2019. 1,2,8",3
WISH: efﬁcient 3D biological shape classiﬁcation through,CVPR_2020,4,"Salah Althloothi, Mohammad H Mahoor, and Richard M V oyles. A robust method for rotation estimation using spheri- cal harmonics representation. IEEE TIP , 22(6):2306–2316, 2013. 3",3
WISH: efﬁcient 3D biological shape classiﬁcation through,CVPR_2020,5,"Alexander I Bobenko and Peter Schr ¨oder. Discrete Willmore ﬂow. In Proc. SGP , pages 101–110, 2005. 3",1
Seeing Through Fog Without Seeing Fog:,CVPR_2020,1,"C. O. Ancuti, C. Ancuti, R. Timofte, and C. D. Vleeschouwer. O-haze: A dehazing benchmark with real hazy and haze-free outdoor images. Proceedings of the IEEE Conference on Computer Vision and Pattern Recog- nition Workshops , pages 867–8678, 2018. 2",2
Seeing Through Fog Without Seeing Fog:,CVPR_2020,2,"V . Badrinarayanan, A. Kendall, and R. Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(12):2481–2495, Dec 2017. 2",2
Seeing Through Fog Without Seeing Fog:,CVPR_2020,3,"M. Bijelic, T. Gruber, and W. Ritter. Benchmarking image sensors under adverse weather conditions for autonomous driving. In 2018 IEEE Intelligent Vehicles Symposium (IV) , pages 1773–1779, 2018. 5",2
Seeing Through Fog Without Seeing Fog:,CVPR_2020,4,"M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316 , 2016. 2",2
Seeing Through Fog Without Seeing Fog:,CVPR_2020,5,"G. J. Brostow, J. Shotton, J. Fauqueur, and R. Cipolla. Seg- mentation and recognition using structure from motion point clouds. In Proceedings of the IEEE European Conference on Computer Vision , pages 44–57. Springer, 2008. 2",2
C-SURE: Shrinkage Estimator and Prototype Classiﬁer for Complex-Valued Deep Learning,CVPR_2020,1,"Rudrasis Chakraborty, Jose Bouza, Jonathan Manton, and Baba C Vemuri. Manifoldnet: A deep network framework for manifold-valued data. arXiv preprint arXiv:1809.06211 , 2018. 1",4
C-SURE: Shrinkage Estimator and Prototype Classiﬁer for Complex-Valued Deep Learning,CVPR_2020,2,"Bradley Efron and Carl Morris. Stein’s estimation rule and its competitors—an empirical bayes approach. Journal of the American Statistical Association , 68(341):117–130, 1973. 1",1
C-SURE: Shrinkage Estimator and Prototype Classiﬁer for Complex-Valued Deep Learning,CVPR_2020,3,"Greg M Fleishman, P Thomas Fletcher, Boris A Gutman, Gau- tam Prasad, Yingnian Wu, and Paul M Thompson. Geodesic reﬁnement using james-stein estimators. Mathematical Foun- dations of Computational Anatomy , 60, 2015. 2",6
C-SURE: Shrinkage Estimator and Prototype Classiﬁer for Complex-Valued Deep Learning,CVPR_2020,4,"Olivier Ledoit and Michael Wolf. A well-conditioned esti- mator for large-dimensional covariance matrices. Journal of multivariate analysis , 88(2):365–411, 2004. 2",1
C-SURE: Shrinkage Estimator and Prototype Classiﬁer for Complex-Valued Deep Learning,CVPR_2020,5,"Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Geometric means in a novel vector space structure on symmetric positive-deﬁnite matrices. SIAM journal on matrix analysis and applications , 29(1):328–347, 2007. 3",4
PointGMM: a Neural GMM Network for Point Clouds,CVPR_2020,1,"Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fis- cher. Nesti-net: Normal estimation for unstructured 3d point clouds using convolutional neural networks. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 10112–10120, 2019. 2",3
PointGMM: a Neural GMM Network for Point Clouds,CVPR_2020,2,"Bing Jian and Baba C Vemuri. Robust point set registra- tion using gaussian mixture models. IEEE transactions on pattern analysis and machine intelligence , 33(8):1633–1645, 2010. 7,8",1
PointGMM: a Neural GMM Network for Point Clouds,CVPR_2020,3,"Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and gen- erative models for 3d point clouds. arXiv preprint arXiv:1707.02392 , 2017. 2",4
PointGMM: a Neural GMM Network for Point Clouds,CVPR_2020,4,"Dror Aiger, Niloy J Mitra, and Daniel Cohen-Or. 4-points congruent sets for robust pairwise surface registration. In ACM transactions on graphics (TOG) , volume 27, page 85. Acm, 2008. 2",3
PointGMM: a Neural GMM Network for Point Clouds,CVPR_2020,5,"Yasuhiro Aoki, Hunter Goforth, Rangaprasad Arun Srivat- san, and Simon Lucey. Pointnetlk: Robust & efﬁcient point cloud registration using pointnet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7163–7172, 2019. 2,7,8",4
Unity Style Transfer for Person Re-Identiﬁcation,CVPR_2020,1,"S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML , 2015.",2
Unity Style Transfer for Person Re-Identiﬁcation,CVPR_2020,2,"J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV , 2016.",2
Unity Style Transfer for Person Re-Identiﬁcation,CVPR_2020,3,"K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR , 2015.",2
Unity Style Transfer for Person Re-Identiﬁcation,CVPR_2020,4,"L. Zheng, Y . Yang, and A. G. Hauptmann. Person rei- dentiﬁcation: Past, present and future. arXiv preprint arXiv:1610.02984 , 2016.",2
Unity Style Transfer for Person Re-Identiﬁcation,CVPR_2020,5,"Xiaobin Chang, Timothy M. Hospedales, and Tao Xiang. Multi-level factorisation net for person re-identiﬁcation. In CVPR , 2018.",2
Semantics-Guided Neural Networks for Efﬁcient Skeleton-Based,CVPR_2020,1,"Yong Du, Yun Fu, and Liang Wang. Skeleton based action recognition with convolutional neural network. In ACPR , 2015.",3
Semantics-Guided Neural Networks for Efﬁcient Skeleton-Based,CVPR_2020,2,"Yong Du, Wei Wang, and Liang Wang. Hierarchical recur- rent neural network for skeleton based action recognition. In CVPR , 2015.",3
Semantics-Guided Neural Networks for Efﬁcient Skeleton-Based,CVPR_2020,3,"Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun- yuan Xie, and Mu Li. Bag of tricks for image classiﬁcation with convolutional neural networks. In CVPR , 2019.",6
Semantics-Guided Neural Networks for Efﬁcient Skeleton-Based,CVPR_2020,4,"Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, and Farid Boussaid. A new representation of skeleton sequences for 3d action recognition. In CVPR , 2017.[19] Qiuhong Ke, Mohammed Bennamoun, Senjian An, Ferdous Sohel, and Farid Boussaid. Learning clip representations for skeleton-based 3d action recognition. TIP, 2018.",5
Semantics-Guided Neural Networks for Efﬁcient Skeleton-Based,CVPR_2020,5,"Jun Liu, Amir Shahroudy, Mauricio Lisboa Perez, Gang Wang, Ling-Yu Duan, and Alex Kot Chichung. Ntu rgb+ d 120: A large-scale benchmark for 3d human activity un- derstanding. TPAMI , 2019.",6
Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of,CVPR_2020,1,"Alexandre Boulch, Bertrand Le Saux, and Nicolas Audebert. Unstructured point cloud semantic labeling using deep seg- mentation networks. 3DOR , 2:7, 2017. 1",3
Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of,CVPR_2020,2,"Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition , pages 3354–3361. IEEE, 2012. 3",3
Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of,CVPR_2020,3,"Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in neural informa- tion processing systems , pages 5099–5108, 2017. 1,6,7",4
Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of,CVPR_2020,4,"Charles R Qi, Hao Su, Matthias Nießner, Angela Dai, Mengyuan Yan, and Leonidas J Guibas. V olumetric and multi-view cnns for object classiﬁcation on 3d data. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition , pages 5648–5656, 2016. 1",6
Toronto-3D: A Large-scale Mobile LiDAR Dataset for Semantic Segmentation of,CVPR_2020,5,"Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Randla-net: Efﬁcient semantic segmentation of large-scale point clouds. arXiv preprint arXiv:1911.11236 , 2019. 7",8
ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene,CVPR_2020,1,"Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision , pages 2961–2969, 2017.",4
ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene,CVPR_2020,2,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.",1
ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene,CVPR_2020,3,"Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang, and Wenyu Liu. Textboxes: A fast text detector with a single deep neural network. In Thirty-First AAAI Conference on Artiﬁcial Intelligence , 2017.[15] Minghui Liao, Zhen Zhu, Baoguang Shi, Gui-Song Xia, and Xiang Bai. Rotation-sensitive regression for oriented scene text detection. In CVPR , pages 5909–5918, 2018.",5
ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene,CVPR_2020,4,"An-An Liu, Yu-Ting Su, Wei-Zhi Nie, and Mohan Kankan- halli. Hierarchical clustering multi-task learning for joint hu- man action grouping and recognition. IEEE transactions on pattern analysis and machine intelligence , 39(1):102–114, 2016.",4
ContourNet: Taking a Further Step toward Accurate Arbitrary-shaped Scene,CVPR_2020,5,"C. Liu, H. Xie, Z. Zha, L. Yu, Z. Chen, and Y . Zhang. Bidi- rectional attention-recognition model for ﬁne-grained object classiﬁcation. IEEE Transactions on Multimedia , pages 1–1, 2019.",2
Where Does It End? – Reasoning About Hidden Surfaces by Object Intersection,CVPR_2020,1,"Martin R ¨unz, Maud Bufﬁer, and Lourdes Agapito. MaskFu- sion: Real-time recognition, tracking and reconstruction of multiple moving objects. In 2018 IEEE International Sym- posium on Mixed and Augmented Reality (ISMAR) , pages 10–20. IEEE, oct 2018. 2,3",3
Where Does It End? – Reasoning About Hidden Surfaces by Object Intersection,CVPR_2020,2,"M. Alexa, J. Behr, D. Cohen-Or, S. Fleishman, D. Levin, and C.T. Silva. Computing and rendering point set surfaces. IEEE Transactions on Visualization and Computer Graphics , 9(1):3–15, jan 2003. 2",2
Where Does It End? – Reasoning About Hidden Surfaces by Object Intersection,CVPR_2020,3,"Fatih Calakli and Gabriel Taubin. SSD: Smooth signed dis- tance surface reconstruction. Computer Graphics Forum , 30(7):1993–2002, sep 2011. 2,3",1
Where Does It End? – Reasoning About Hidden Surfaces by Object Intersection,CVPR_2020,4,"Daniel Cremers and Kalin Kolev. Multiview stereo and sil- houette consistency via convex functionals over convex do- mains. IEEE Transactions on Pattern Analysis and Machine Intelligence , 33(6):1161–1174, jun 2011. 2",1
Where Does It End? – Reasoning About Hidden Surfaces by Object Intersection,CVPR_2020,5,"Brian Curless and Marc Levoy. A volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer graphics and in- teractive techniques , SIGGRAPH ’96, pages 303–312, New York, NY , USA, 1996. ACM Press. 2,3",1
CycleISP: Real Image Restoration via Improved Data Synthesis,CVPR_2020,1,"Alessandro Foi, Sakari Alenius, Vladimir Katkovnik, and Karen Egiazarian. Noise measurement for raw-data of digital imaging sensors by automatic segmentation of nonuniform targets. Sensors , 2007. 2",4
CycleISP: Real Image Restoration via Improved Data Synthesis,CVPR_2020,2,"Dongwei Ren, Wangmeng Zuo, Qinghua Hu, Pengfei Zhu, and Deyu Meng. Progressive image deraining networks: a better and simpler baseline. In CVPR , 2019. 4",5
CycleISP: Real Image Restoration via Improved Data Synthesis,CVPR_2020,3,"Hossein Talebi and Peyman Milanfar. Global image denois- ing. TIP, 2013. 6,7",1
CycleISP: Real Image Restoration via Improved Data Synthesis,CVPR_2020,4,"Abdelrahman Abdelhamed, Stephen Lin, and Michael S Brown. A high-quality denoising dataset for smartphone cameras. In CVPR , 2018. 2,5,6,7,8",3
CycleISP: Real Image Restoration via Improved Data Synthesis,CVPR_2020,5,"Abdelrahman Abdelhamed, Radu Timofte, and Michael S Brown. Ntire 2019 challenge on real image denoising: Meth- ods and results. In CVPRW , 2019. 2",3
Removal of Image Obstacles for Vehicle-mounted,CVPR_2020,1,"Martin Roser, Julian Kurz, and Andreas Geiger. Realistic modeling of water droplets for monocular adherent raindrop recognition using bezier curves. In Asian Conference on Computer Vision , pages 235–244. Springer, 2010. 5",3
Removal of Image Obstacles for Vehicle-mounted,CVPR_2020,2,"Mathieu Aubry, Daniel Maturana, Alexei A Efros, Bryan C Russell, and Josef Sivic. Seeing 3d chairs: exemplar part- based 2d-3d alignment using a large dataset of cad models. InProceedings of the IEEE conference on computer vision and pattern recognition , pages 3762–3769, 2014. 3",5
Removal of Image Obstacles for Vehicle-mounted,CVPR_2020,3,"Yochai Blau and Tomer Michaeli. The perception-distortion tradeoff. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 6228–6237, 2018. 7",1
Removal of Image Obstacles for Vehicle-mounted,CVPR_2020,4,"Thomas Brox, Andr ´es Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy optical ﬂow estimation based on a theory for warping. In European conference on computer vision , pages 25–36. Springer, 2004. 4",4
Removal of Image Obstacles for Vehicle-mounted,CVPR_2020,5,"Andr ´es Bruhn, Joachim Weickert, and Christoph Schn ¨orr. Lucas/kanade meets horn/schunck: Combining local and global optic ﬂow methods. International journal of computer vision , 61(3):211–231, 2005. 4",3
LSM: Learning Subspace Minimization for Low-level Vision,CVPR_2020,1,"S. D. Babacan, R. Molina, and A. K. Katsaggelos. Total vari- ation super resolution using a variational approach. In 2008 15th IEEE International Conference on Image Processing , pages 641–644, 2008. 2",2
LSM: Learning Subspace Minimization for Low-level Vision,CVPR_2020,2,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Con- ference on Computer Vision and Pattern Recognition (CVPR) , 2016. 2,4",4
LSM: Learning Subspace Minimization for Low-level Vision,CVPR_2020,3,"Won-Dong Jang and Chang-Su Kim. Interactive image seg- mentation via backpropagating reﬁnement scheme. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019. 2,5",1
LSM: Learning Subspace Minimization for Low-level Vision,CVPR_2020,4,"Emanuel Laude, Jan-Hendrik Lange, Jonas Schüpfer, Csaba Domokos, Laura Leal-Taixé, Frank R. Schmidt, Bjoern An- dres, and Daniel Cremers. Discrete-continuous admm for transductive inference in higher-order mrfs. In IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR) , June 2018. 1,5",6
LSM: Learning Subspace Minimization for Low-level Vision,CVPR_2020,5,"Ilya Loshchilov and Frank Hutter. Decoupled weight de- cay regularization. In International Conference on Learning Representations (ICLR) , 2019. 5",1
Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision,CVPR_2020,1,"Geoffrey E Hinton and Richard S. Zemel. Autoencoders, minimum description length and Helmholtz free energy. In Proceedings of the 6th Conference on Neural Information Processing Systems , pages 3–10, 1994.",1
Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision,CVPR_2020,2,"Syed Husain and Miroslaw Bober. Remap: Multi-layer entropy-guided pooling of dense CNN features for image retrieval. IEEE Transactions on Image Processing , pages 5201–5213, 2019.[15] Tommi Jaakkola and David Haussler. Exploiting generative models in discriminative classiﬁers. In M. J. Kearns, S. A. Solla, and D. A. Cohn, editors, Proceedings of the 11th Con- ference on Neural Information Processing Systems , pages 487–493, 1999.",1
Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision,CVPR_2020,3,"Gr ´egoire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M ¨uller. Ex- plaining nonlinear classiﬁcation decisions with deep taylor decomposition. Pattern Recognition , pages 211–222, 2017.",5
Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision,CVPR_2020,4,"Alessandro Achille and Stefano Soatto. Emergence of invari- ance and disentanglement in deep representations. Journal of Machine Learning Research , pages 1947–1980, 2018.",1
Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision,CVPR_2020,5,"Relja Arandjelovi ´c, Petr Gron ´at, Akihiko Torii, Tom ´as Pa- jdla, and Josef Sivic. NetVLAD: CNN architecture for weakly supervised place recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) , pages 5297–5307, 2016.",5
Deep Homography Estimation for Dynamic Scenes,CVPR_2020,1,"Rich Caruana. Multitask learning: A knowledge-based source of inductive bias. In Proceedings of the Tenth Interna- tional Conference on International Conference on Machine Learning , pages 41–48, 1993. 5",2
Deep Homography Estimation for Dynamic Scenes,CVPR_2020,2,"Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi- novich. Deep image homography estimation. arXiv preprint arXiv:1606.03798 , 2016. 1,2,3,4,6,7",3
Deep Homography Estimation for Dynamic Scenes,CVPR_2020,3,"Farzan Erlik Nowruzi, Robert Laganiere, and Nathalie Jap- kowicz. Homography estimation from image pairs with hi- erarchical convolutional networks. In Proceedings of The IEEE International Conference on Computer Vision Work- shop , pages 913–920, 2017. 1,3,4",3
Deep Homography Estimation for Dynamic Scenes,CVPR_2020,4,"Shuaicheng Liu, Lu Yuan, Ping Tan, and Jian Sun. Bundled camera paths for video stabilization. ACM Transactions on Graphics , 32(4):78, 2013. 7,8",4
Deep Homography Estimation for Dynamic Scenes,CVPR_2020,5,"Simon Baker and Iain Matthews. Lucas-kanade 20 years on: A unifying framework. International journal of computer vision , 56(3):221–255, 2004. 4",1
Deep 3D Capture: Geometry and Reﬂectance from Sparse Multi-View Images,CVPR_2020,1,"Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, and Kwan-Yee K Wong. Self-calibrating deep photometric stereo networks. In ECCV , 2018. 3",5
Deep 3D Capture: Geometry and Reﬂectance from Sparse Multi-View Images,CVPR_2020,2,"Abhishek Kar, Christian H ¨ane, and Jitendra Malik. Learning a multi-view stereo machine. In NIPS , pages 365–376, 2017. 4",3
Deep 3D Capture: Geometry and Reﬂectance from Sparse Multi-View Images,CVPR_2020,3,Brian Karis and Epic Games. Real shading in unreal engine 4.1,1
Deep 3D Capture: Geometry and Reﬂectance from Sparse Multi-View Images,CVPR_2020,4,"Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Mor- ishima, Angjoo Kanazawa, and Hao Li. PIFu: Pixel-aligned implicit function for high-resolution clothed human digitiza- tion. In ICCV , October 2019. 4",6
Deep 3D Capture: Geometry and Reﬂectance from Sparse Multi-View Images,CVPR_2020,5,"Sunghoon Im, Hae-Gon Jeon, Stephen Lin, and In So Kweon. DPSNet: end-to-end deep plane sweep stereo. ICLR , 2019. 3",4
Reverse Perspective Network for Perspective-Aware Object Counting,CVPR_2020,1,"Zenglin Shi, Le Zhang, Yun Liu, Xiaofeng Cao, Yangdong Ye, Mingming Cheng, and Guoyan Zheng. Crowd counting with deep negative correlation learning. Computer Vision and Pattern Recognition , pages 5382–5390, 2018. 1,3",7
Reverse Perspective Network for Perspective-Aware Object Counting,CVPR_2020,2,"Paulo Almeida, Luiz S Oliveira, Alceu De Souza Britto, Eu- nelson J Silva, and Alessandro L Koerich. Pklot-a robust dataset for parking lot classiﬁcation. Expert Systems With Applications , 42(11):4937–4949, 2015. 6",5
Reverse Perspective Network for Perspective-Aware Object Counting,CVPR_2020,3,"Zhang Anran, Yue Lei, Shen Jiayi, Zhu Fan, Zhen Xiantong, Cao Xianbin, and Shao Ling. Attentional neural ﬁelds for crowd counting. IEEE International Conference on Com- puter Vision , 2019. 6",7
Reverse Perspective Network for Perspective-Aware Object Counting,CVPR_2020,4,"David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio Torralba. Network dissection: Quantifying inter- pretability of deep visual representations. Computer Vision and Pattern Recognition , 2017. 4",5
Reverse Perspective Network for Perspective-Aware Object Counting,CVPR_2020,5,"Xinkun Cao, Zhipeng Wang, Yanyun Zhao, and Fei Su. Scale aggregation network for accurate and efﬁcient crowd count- ing. Computer Vision and Pattern Recognition , pages 757– 773, 2018. 1,2,3,6,7",4
Effective Deep-Learning-Based Depth Data Analysis,CVPR_2020,1,"Ahmed Abobakr, Mohammed Hossny, and Saeid Nahavandi. A Skeleton-free Fall Detection System from Depth Images using Random Decision Forest. IEEE Systems Journal , 12(3):2994–3005, 2017. 2",3
Effective Deep-Learning-Based Depth Data Analysis,CVPR_2020,2,"Rachit Dubey, Bingbing Ni, and Pierre Moulin. A Depth Camera Based Fall Recognition System for the Elderly. In International Conference Image Analysis and Recognition , pages 106–113, 2012. 2",3
Effective Deep-Learning-Based Depth Data Analysis,CVPR_2020,3,"Michael Harville and Dalong Li. Fast, Integrated Person Tracking and Activity Recognition with Plan-view Tem- plates from a Single Stereo Camera. In IEEE Conference on Computer Vision and Pattern Recognition , 2004. 2,4",1
Effective Deep-Learning-Based Depth Data Analysis,CVPR_2020,4,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity Mappings in Deep Residual Networks. In Euro- pean Conference on Computer Vision (ECCV) , pages 630– 645, 2016. 1,2,3,6",4
Effective Deep-Learning-Based Depth Data Analysis,CVPR_2020,5,"Yihui He, Xiangyu Zhang, and Jian Sun. Channel Prun- ing for Accelerating Very Deep Neural Networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 1389–1397, 2017. 2",3
Rethinking Zero-shot Video Classiﬁcation:,CVPR_2020,1,"Chuang Gan, Ming Lin, Y . Yang, Y . Zhuang, and Alexan- der Hauptmann. Exploring semantic inter-class relation- ships (sir) for zero-shot action recognition. Proceedings ofthe Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence , pages 3769–3775, 01 2015.",3
Rethinking Zero-shot Video Classiﬁcation:,CVPR_2020,2,"Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu- man action video dataset. arXiv preprint arXiv:1705.06950 , 2017.",11
Rethinking Zero-shot Video Classiﬁcation:,CVPR_2020,3,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.",1
Rethinking Zero-shot Video Classiﬁcation:,CVPR_2020,4,"Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recogni- tion, pages 6450–6459, 2018.",6
Rethinking Zero-shot Video Classiﬁcation:,CVPR_2020,5,"Ioannis Alexiou, Tao Xiang, and Shaogang Gong. Exploring synonyms as context in zero-shot action recognition. In 2016 IEEE International Conference on Image Processing (ICIP) , pages 4190–4194. IEEE, 2016.",3
Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings,CVPR_2020,1,"Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Uniﬁed genera- tive adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 8789–8797, 2018.",6
Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings,CVPR_2020,2,"Franc ¸ois Chollet et al. Keras. https://github.com/ fchollet/keras , 2015.",2
Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings,CVPR_2020,3,"Savage Interactive. Procreate Artists’ Handbook . Savage, 2016.",2
Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings,CVPR_2020,4,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adver- sarial networks. arXiv preprint arXiv:1611.07004 , 2016.",4
Painting Many Pasts: Synthesizing Time Lapse Videos of Paintings,CVPR_2020,5,"Biao Jia, Jonathan Brandt, Radom ´ır Mech, Byungmoon Kim, and Dinesh Manocha. Lpaintb: Learning to paint from self-supervision. CoRR , abs/1906.06841, 2019.",5
Generative Feature Replay For Class-Incremental Learning,CVPR_2020,1,"Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV , pages 139–154, 2018. 2,6",5
Generative Feature Replay For Class-Incremental Learning,CVPR_2020,2,"Eden Belouadah and Adrian Popescu. Il2m: Class incremen- tal learning with dual memory. In ICCV , pages 583–592, 2019. 3",1
Generative Feature Replay For Class-Incremental Learning,CVPR_2020,3,"Andrew Brock, Jeff Donahuey, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural image synthesis. InICLR , 2019. 3",3
Generative Feature Replay For Class-Incremental Learning,CVPR_2020,4,"Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan- than, and Philip HS Torr. Riemannian walk for incremen- tal learning: Understanding forgetting and intransigence. In ECCV , pages 532–547, 2018. 2,6",4
Generative Feature Replay For Class-Incremental Learning,CVPR_2020,5,"Arslan Chaudhry, Marc’Aurelio Ranzato, Marcus Rohrbach, and Mohamed Elhoseiny. Efﬁcient lifelong learning with A- GEM. In ICLR , 2019. 3",4
Assessing Eye Aesthetics for Automatic Multi-Reference Eye In-Painting,CVPR_2020,1,"Eye In-Painting with Eye Aesthetic Assess- ment 4.1. Overview We introduce the eye aesthetic assessment into the eye in-painting task, and propose the eye aesthetic and face semantic guided multi-reference eye in-painting method (AesGAN). Given an incomplete image, our goal is to pro- duce natural and attractive eyes that are both semantically consistent with the whole object and visually realistic. Fig- ure 5 shows the proposed network that consists of one gen- erator, two discriminators, an eye aesthetic assessment net- work and a parsing network. We use the eye aesthetics assessment network and struc- tural similarity index (SSIM) to automatically select the best reference. In order to highlight the role of eye aes- thetic assessment, we introduced a new aesthetic loss. At the same time, a parsing loss is added to ensure the ﬁdelity and semantic consistency of pixels. The parameters of pars- ing network and eye assessment network are ﬁxed when training. 4.2. Guidance of Eye Aesthetic Assessment People’s growing pursuit of beauty gives a new idea to the image restoration task. It is instructive to introduce eye aesthetic assessment to eye in-painting task. The guidance of eye aesthetic assessment is manifested in three aspects. Firstly, based on eye aesthetics, we propose a multi- reference selection mechanism. We use the AesNet to score the eyes of the references. Then we calculate the SSIM val- ues of each image except for the eye region in order to select the image which is most similar to the structure of the image to be in-painted in shape and motion. The selected reference 13512 Figure 6. The segmentation result of face semantic parsing net- work testing on Celeb-ID Dataset. Different colors of pixels rep- resent different face components. can provide additional eye information for the generator, es- pecially eye aesthetic features. Secondly, in addition to providing aesthetic prior knowl- edge, we also need to constrain beauty in the training of the network. We use the eye feature extraction module of AesNet to extract the eye aesthetic features of the generated eyes and the reference. The aesthetic loss calculates the L2 distance between these two features, making the generator learn the concept of eye aesthetic better. Last but not least, we can use the eye aesthetic assess- ment network to compare our results with those of other advanced methods to verify the effectiveness of introducing eye aesthetics. 4.3. Face Semantic Embedding Traditional GAN models independently generate facial components which may not be suitable for the original face. As mentioned in [7], if a part of the eyes is obscured, the new eyes take on strange shapes or have blurred eye de- tails. Therefore, inspired by [16], we introduce a parsing network, which is implemented by changing the last layer of object contour detection network proposed in [27] to 11 outputs. We train a segmentation model on the CelebA [17] dataset, which achieves the f-score of 0.822. Figure 6 shows the segmentation result of face semantic parsing network testing on Celeb-ID Dataset. The parsing result of the gen- erated image is compared with that of the original image, and softmax cross-entropy loss is used as the parsing loss of the in-painting network to make the generated eye details more consistent with the overall coherence of the image. 4.4. Loss Functions The global loss function of the network is deﬁned as L=LGAN+λrLr+λpLp+λaesLaes (4) whereLGAN is the adversarial loss, and Lris the recon- struction loss used in [7]. Lpis the parsing loss, which is the softmax cross-entropy loss. Laesis the aesthetic loss, which is the activation of the residual blocks’ ﬁnal layer de- ﬁned as Laes=/bardblF(gi)−F(ei)/bardbl2 (5)whereF(gi)andF(ei)are the eye feature layers of the generated image and the reference, respectively. By short- ening the aesthetic distance between the generated eyes and the reference eyes, we make the generated eyes look more aesthetically pleasing. λr,λpandλaesare the weights to balance the effects of different losses.",2
Assessing Eye Aesthetics for Automatic Multi-Reference Eye In-Painting,CVPR_2020,2,"This paper ﬁrst shows the effectiveness of aesthetic as- sessment in the multi-reference eye in-painting task, which may inspire the introduction of aesthetic assessment to other work in the future.",2
Assessing Eye Aesthetics for Automatic Multi-Reference Eye In-Painting,CVPR_2020,3,"Through the eye in-painting task, the effectiveness of the eye aesthetic assessment network is proved. We pro- pose a novel eye aesthetic and face semantic guided multi- reference eye in-painting GAN approach (AesGAN). By us- ing the high quality reference and aesthetic features pro- vided by the eye aesthetic assessment network, the perfor- mance of our eye in-painting method is better than those of state-of-the-art methods in both qualitative and quantitative results.",2
Assessing Eye Aesthetics for Automatic Multi-Reference Eye In-Painting,CVPR_2020,4,"Experiment This section provides a detailed assessment of the eye aesthetic and its effectiveness for eye in-painting. Speciﬁ- cally, we ﬁrst analyze the inﬂuence of different modules of AesNet on network performance. Then we conduct the ab- lation study to analyze the effectiveness of different designs of our AesGAN, including the different settings of loss functions and eye aesthetic assessment. We also demon- strate the experiment between Single Example VS. Aesthetic Assessment Guided Eye In-painting andMulti Examples VS. Aesthetic Assessment Guided Eye In-painting . Finally, we compare the latest and representative eye in-painting meth- ods. For the eye in-painting task, we use the Celeb-ID [7] dataset to train and test our model, which contains about 17k personal identities and a total of 100k photos. Each celebrity has at least 3 photos. We split the dataset accord- ing to the following criteria: for any celebrity, if there is a closed-eye photo in his samples, all his photos will be classiﬁed as the test set, otherwise classiﬁed as the train- ing set. So every image in the training set contains a per- son with eyes opened, forcing the network to produce open eyes. Each training image has a reference of identity. All experiments are conducted on a machine with an Nvidia GTX 1080Ti GPU with learning rate 1e-4. The pa- rameters were optimized by ADAM [13] with parameters β1= 0.5,β2= 0.999. To balance the effects of different losses, we use λr= 1,λp= 0.03andλaes= 1 in our ex-",1
Assessing Eye Aesthetics for Automatic Multi-Reference Eye In-Painting,CVPR_2020,5,"of manual marking due to people’s preferences. Currently, there has been no research on the assessment of eye aes- thetic quality. In addition, the eye aesthetic quality assess- ment can help us know the quality of the eyes in a face im- age. Not satisﬁed with just knowing the aesthetic quality of the eyes, we also hope that we can use this as a guide to make the poor eyes more realistic. As a result, eye in- painting is produced, which can be seen as an application of eye aesthetics. There are few researches on eye in-painting. Figure 1 13509 (a) No Reference Eye In-painting (d) Aesthetics Guided Eye In-painting Encoder- Decoder Incomplete Image In-painted Image (b) Single Reference Eye In-painting (c ) Multi-Reference Eye In-painting Encoder- Decoder Selected Reference Eye Aesthetic Assessment Incomplete Image In-painted Image Encoder- Decoder In-painted Image Simple-Reference Incomplete Image Encoder- Decoder In-painted Image Multi-Reference Incomplete ImageFigure 2. Comparison of different eye in-painting frameworks. (a) Traditional eye in-painting methods do not use references, which only use incomplete images as input and ﬁnally output repaired images through encoders and decoders. (b) Single reference based eye in- painting methods use one reference image to assist in-painting, while (c) multi-reference based eye in-painting methods take multiple references. (d) Our proposed aesthetic guided eye in-painting method takes aesthetics as the criterion of reference selection, and uses the ﬁnal selected image as the input. shows some results. Eye in-painting is a branch of face restoration problem, which is mainly applied in closed eyes and squint eyes situations, in order to produce real and nat- ural new eyes. The current approaches can be summarized as the three types of frameworks shown in Figure 2. The previous three frameworks pose attention on whether to use reference samples or how many reference samples to use. ExGAN [7] method has well shown that the identity pre- served eye in-painting result can be obtained by referring to the same identity example. Nevertheless, these three frame- works do not consider the selection of reference examples, which is a practical problem. In addition, eye aesthetic at- tributes that are crucial to eye in-painting, have not been considered in these frameworks. These observations mo- tivate us to develop a new framework: Aesthetics Guided Eye In-painting, which addresses the selection problem of multiple reference examples based on our proposed eye aes- thetic assessment. Due to the lack of research on eye aesthetic, there is no dataset that can be directly used in eye aesthetic assessment. On the basis of the existing face dataset, we cut out the eye area and invite 22 volunteers to assess the eye quality aes- thetically. The marked dataset contains 1,040 eye images, all of which are divided into two categories according to the average level of manual tagging: aesthetically pleasing or not. Based on this dataset, we train the eye aesthetic assessment network. We introduce the eye aesthetic assess- ment system into the work of eye in-painting. First, it can automatically select the appropriate reference for the eye generation. Then we introduce aesthetic loss to force the in-painting network to learn the eye aesthetic features, and promote the generated eyes to be more realistic. This paper mainly has the following contributions:",2
Adaptive Posit: Parameter aware numerical format for deep learning inference,CVPR_2020,1,"Matthieu Courbariaux, Yoshua Bengio, and Jean-Pierre David. Low precision arithmetic for deep learning. In Workshop Track Proceedings of the 3rd International Conference on Learning Representations, ICLR , San Diego, CA, USA, May 2015.",3
Adaptive Posit: Parameter aware numerical format for deep learning inference,CVPR_2020,2,"Philipp Gysel, Jon Pimentel, Mohammad Motamedi, and Soheil Ghiasi. Ristretto: A framework for empir- ical study of resource-efﬁcient inference in convolu- tional neural networks. IEEE Transactions on Neural Networks and Learning Systems , 2018.",4
Adaptive Posit: Parameter aware numerical format for deep learning inference,CVPR_2020,3,"Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, et al. Quantization and training of neural net- works for efﬁcient integer-arithmetic-only inference. InThe IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR) , June 2018.",5
Adaptive Posit: Parameter aware numerical format for deep learning inference,CVPR_2020,4,"Hamed F Langroudi, Zachariah Carmichael, David Pas- tuch, and Dhireesha Kudithipudi. Cheetah: Mixed low- precision hardware & software co-design framework for dnns on the edge. arXiv preprint arXiv:1908.02386 , 2019.",4
Adaptive Posit: Parameter aware numerical format for deep learning inference,CVPR_2020,5,"Seyed HF Langroudi, Tej Pandit, and Dhireesha Kudithipudi. Deep learning inference on embed- ded devices: Fixed-point vs posit. arXiv preprint arXiv:1805.08624 , 2018.",3
Deep Facial Non-Rigid Multi-View Stereo,CVPR_2020,1,"Brian Amberg, Sami Romdhani, and Thomas Vetter. Optimal step nonrigid icp algorithms for surface registration. In Proc. of Computer Vision and Pattern Recognition (CVPR) , pages 1–8. IEEE, 2007. 6",3
Deep Facial Non-Rigid Multi-View Stereo,CVPR_2020,2,"Stirling/ESRC 3D face database .6,8",2
Deep Facial Non-Rigid Multi-View Stereo,CVPR_2020,3,"Antonio Agudo and Francesc Moreno-Noguer. Dust: Dual union of spatio-temporal subspaces for monocular multiple object 3d reconstruction. In Proc. of Computer Vision and Pattern Recognition (CVPR) , pages 6262–6270, 2017. 2",1
Deep Facial Non-Rigid Multi-View Stereo,CVPR_2020,4,"Ijaz Akhter, Yaser Sheikh, and Sohaib Khan. In defense of orthonormality constraints for nonrigid structure from mo- tion. In Proc. of Computer Vision and Pattern Recognition (CVPR) , pages 1534–1541. IEEE, 2009. 2",3
Deep Facial Non-Rigid Multi-View Stereo,CVPR_2020,5,"Brian Amberg, Sami Romdhani, and Thomas Vetter. Optimal step nonrigid icp algorithms for surface registration. In Proc. of Computer Vision and Pattern Recognition (CVPR) , pages 1–8. IEEE, 2007. 6",3
Channel Attention based Iterative Residual Learning for Depth Map,CVPR_2020,1,"Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional net- works. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1646–1654, 2016.",3
Channel Attention based Iterative Residual Learning for Depth Map,CVPR_2020,2,"Liu Liu, Hongdong Li, and Yuchao Dai. Efﬁcient global 2d-3d matching for camera localization in a large-scale 3d map. In Proceedings of the IEEE International Conference on Computer Vision , pages 2372–2381, 2017.",3
Channel Attention based Iterative Residual Learning for Depth Map,CVPR_2020,3,"Stephen T Barnard and Martin A Fischler. Computational stereo. Technical report, SRI INTERNATIONAL MENLO PARK CA ARTIFICIAL INTELLIGENCE CENTER, 1982.",1
Channel Attention based Iterative Residual Learning for Depth Map,CVPR_2020,4,"Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In European conference on computer vi- sion, pages 184–199. 2014.",4
Channel Attention based Iterative Residual Learning for Depth Map,CVPR_2020,5,"Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single im- age super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 11065– 11074, 2019.",5
On the Regularization Properties of Structured Dropout,CVPR_2020,1,"Xavier Gastaldi. Shake-shake regularization. In arXiv preprint arXiv:1705.07485 , 2017. 1",2
On the Regularization Properties of Structured Dropout,CVPR_2020,2,"J. Cavazza, B.D. Haeffele, C. Lane, P. Morerio, V . Murino, and R. Vidal. Dropout as a low-rank regularizer for matrix factorization. In International Conference on Artiﬁcial Intel- ligence and Statistics , volume 84, pages 435–444, 2018. 1, 3,6,7",2
On the Regularization Properties of Structured Dropout,CVPR_2020,3,"Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock: A regularization method for convolutional networks. In Advances in Neural Information Processing Systems , pages 10750–10760, 2018. 1,5,8",3
On the Regularization Properties of Structured Dropout,CVPR_2020,4,"Andrew M McDonald, Massimiliano Pontil, and Dimitris Stamos. Spectral k-support norm regularization. In Ad- vances in Neural Information Processing Systems , pages 3644–3652, 2014. 4,5",3
On the Regularization Properties of Structured Dropout,CVPR_2020,5,"Benjamin David Haeffele and Ren ´e Vidal. Structured low- rank matrix factorization: Global optimality, algorithms, and applications. IEEE transactions on pattern analysis and ma- chine intelligence , 2019.",1
Fine grained pointing recognition for natural drone guidance,CVPR_2020,1,"R. A. S. Fernandez, J. L. Sanchez-Lopez, C. Sampedro, H. Bavle, M. Molina, and P. Campoy. Natural user interfaces for human-drone multi-modal interaction. In 2016 International Conference on Unmanned Aircraft Systems (ICUAS) , pages 1013–1022, June 2016. 2",2
Fine grained pointing recognition for natural drone guidance,CVPR_2020,2,"Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir- shick. Mask R-CNN. In IEEE Int. Conf. on Computer Vision , pages 2980–2988, 2017. 2,3,4",4
Fine grained pointing recognition for natural drone guidance,CVPR_2020,3,"Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Re- altime multi-person 2D pose estimation using part afﬁnity ﬁelds. In CVPR , 2017. 2",4
Fine grained pointing recognition for natural drone guidance,CVPR_2020,4,"Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767 , 2018. 4",1
Fine grained pointing recognition for natural drone guidance,CVPR_2020,5,"Sepehr MohaimenianPour and Richard Vaughan. Hands and faces, fast: Mono-camera user detection robust enough to di- rectly control a uav in ﬂight. In IEEE/RSJ Int. Conf. on Intel- ligent Robots and Systems (IROS) , pages 5224–5231, 2018. 2,4",1
Syn2Real Transfer Learning for Image Deraining using Gaussian Processes,CVPR_2020,1,"Samuli Laine and Timo Aila. Temporal ensembling for semi- supervised learning. arXiv preprint arXiv:1610.02242 , 2016. 3[16] Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. 3",1
Syn2Real Transfer Learning for Image Deraining using Gaussian Processes,CVPR_2020,2,"Minghan Li, Qi Xie, Qian Zhao, Wei Wei, Shuhang Gu, Jing Tao, and Deyu Meng. Video rain streak removal by multi- scale convolutional sparse coding. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2018. 3",7
Syn2Real Transfer Learning for Image Deraining using Gaussian Processes,CVPR_2020,3,"Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji- tendra Malik. Contour detection and hierarchical image seg- mentation. IEEE transactions on pattern analysis and ma- chine intelligence , 33(5):898–916, 2010. 6",4
Syn2Real Transfer Learning for Image Deraining using Gaussian Processes,CVPR_2020,4,"H S Bhadauria and M L Dewal. Online dictionary learning for sparse coding. In: International Conference on Machine Learning(ICML) , pages 689–696, 2009. 3",1
Syn2Real Transfer Learning for Image Deraining using Gaussian Processes,CVPR_2020,5,"Y . Chen and C. Hsu. A generalized low-rank appearance model for spatio-temporally correlated rain streaks. In 2013 IEEE International Conference on Computer Vision , pages 1968–1975, Dec 2013. 3",2
CRNet: Cross-Reference Networks for Few-Shot Segmentation,CVPR_2020,1,"Wei-Yu Chen, Yen-Cheng Liu, Zsolt Kira, Yu-Chiang Wang, and Jia-Bin Huang. A closer look at few-shot classiﬁcation. InInternational Conference on Learning Representations , 2019. 2",5
CRNet: Cross-Reference Networks for Few-Shot Segmentation,CVPR_2020,2,"Armand Joulin, Francis Bach, and Jean Ponce. Multi-class cosegmentation. In 2012 IEEE Conference on Computer Vi- sion and Pattern Recognition , pages 542–549. IEEE, 2012. 2,3",3
CRNet: Cross-Reference Networks for Few-Shot Segmentation,CVPR_2020,3,"Chi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen. Canet: Class-agnostic segmentation networks with it- erative reﬁnement and attentive few-shot learning. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 5217–5226, 2019. 3,4,5,7,8",5
CRNet: Cross-Reference Networks for Few-Shot Segmentation,CVPR_2020,4,"Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters–improve semantic segmen- tation by global convolutional network. In Proceedings of the IEEE conference on computer vision and pattern recog- nition , pages 4353–4361, 2017. 5",5
CRNet: Cross-Reference Networks for Few-Shot Segmentation,CVPR_2020,5,"Hong Chen, Yifei Huang, and Hideki Nakayama. Semantic aware attention based deep object co-segmentation. arXiv preprint arXiv:1810.06859 , 2018. 2,3",3
Learning Identity-Invariant Motion Representations,CVPR_2020,1,"Karen Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014. 4",1
Learning Identity-Invariant Motion Representations,CVPR_2020,2,"Xiang Wu, Ran He, Zhenan Sun, and Tieniu Tan. A light cnn for deep face representation with noisy labels. IEEE Trans- actions on Information Forensics and Security , 13(11):2884– 2896, 2018. 5",4
Learning Identity-Invariant Motion Representations,CVPR_2020,3,"Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser Sheikh. Recycle-gan: Unsupervised video retargeting. In Proceedings of the European Conference on Computer Vi- sion (ECCV) , pages 119–135, 2018. 1,2,3",4
Learning Identity-Invariant Motion Representations,CVPR_2020,4,"Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4690– 4699, 2019. 7,8",4
Learning Identity-Invariant Motion Representations,CVPR_2020,5,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems , pages 2672–2680, 2014. 1",8
Graph-guided Architecture Search for Real-time Semantic Segmentation,CVPR_2020,1,"Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Di- rect neural architecture search on target task and hardware. arXiv:1812.00332 , 2018.",3
Graph-guided Architecture Search for Real-time Semantic Segmentation,CVPR_2020,2,"Renato Negrinho and Geoff Gordon. Deeparchitect: Automatically designing and training deep architectures. arXiv:1704.08792 , 2017.",1
Graph-guided Architecture Search for Real-time Semantic Segmentation,CVPR_2020,3,"Asaf Noy, Niv Nayman, Tal Ridnik, Nadav Zamir, Sivan Doveh, Itamar Friedman, Raja Giryes, and Lihi Zelnik- Manor. Asap: Architecture search, anneal and prune. arXiv:1904.04123 , 2019.",8
Graph-guided Architecture Search for Real-time Semantic Segmentation,CVPR_2020,4,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: An imperative style, high-performance deep learning library. In NIPS , 2019.",21
Graph-guided Architecture Search for Real-time Semantic Segmentation,CVPR_2020,5,"Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE trans. PAMI , 39(12):2481– 2495, 2017.",3
DA VD-Net: Deep Audio-aided Video Decompression of Talking Heads,CVPR_2020,1,instead of directly decoded frames as temporal references.,2
DA VD-Net: Deep Audio-aided Video Decompression of Talking Heads,CVPR_2020,2,"Recently, Xu et al. [ 34] introduced an non-local strategy in ConvLSTM to trace the spatiotemporal dependency in a video sequence, and achieved the state-of-the-art perfor- mance.",2
DA VD-Net: Deep Audio-aided Video Decompression of Talking Heads,CVPR_2020,3,Joint audio-video generation and processing. Suwa- janakorn et al. [ 26] proposed an interesting technique to automatically edit a video of a given speaker with accu- rate lip synchronization guided by his own audio in a dif- ferent speech. Chung et al. [ 5] presented a method to animate a face image according to audio signals. V ou- gioukas et al. [ 29] proposed to do speech-driven animation with temporal GANS. Chen et al. [ 4] presented a method to do speech-driven facial animation with spatial attention.,1
DA VD-Net: Deep Audio-aided Video Decompression of Talking Heads,CVPR_2020,4,Afouras et al. [ 1] designed a deep audio-visual speech sep-,2
DA VD-Net: Deep Audio-aided Video Decompression of Talking Heads,CVPR_2020,5,Alignment & Fusion,2
"D3VO: Deep Depth, Deep Pose and Deep Uncertainty",CVPR_2020,1,"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016. 6,15",9
"D3VO: Deep Depth, Deep Pose and Deep Uncertainty",CVPR_2020,2,"Jia-Wang Bian, Zhichao Li, Naiyan Wang, Huangying Zhan, Chunhua Shen, Ming-Ming Cheng, and Ian Reid. Unsuper- vised scale-consistent depth and ego-motion learning from monocular video. In Thirty-third Conference on Neural In- formation Processing Systems (NeurIPS) , 2019. 7",7
"D3VO: Deep Depth, Deep Pose and Deep Uncertainty",CVPR_2020,3,"Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J Davison. CodeSLAM-learning a compact, optimisable representation for dense visual SLAM. arXiv preprint arXiv:1804.00874 , 2018. 3",5
"D3VO: Deep Depth, Deep Pose and Deep Uncertainty",CVPR_2020,4,"Michael Bloesch, Sammy Omari, Marco Hutter, and Roland Siegwart. Robust visual inertial odometry using a direct EKF-based approach. In 2015 IEEE/RSJ international con- ference on intelligent robots and systems (IROS) , pages 298– 304. IEEE, 2015. 8",4
"D3VO: Deep Depth, Deep Pose and Deep Uncertainty",CVPR_2020,5,"G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools , 2000. 14",2
ARShadowGAN: Shadow Generative Adversarial Network for Augmented,CVPR_2020,1,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Proceed- ings of the Advances in neural information processing sys- tems (NeurIPS) , pages 2672–2680, 2014.",8
ARShadowGAN: Shadow Generative Adversarial Network for Augmented,CVPR_2020,2,"Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image completion. ACM Transactions on Graphics (TOG) , 36(4):107, 2017.",3
ARShadowGAN: Shadow Generative Adversarial Network for Augmented,CVPR_2020,3,"Martin Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875 , 2017.",3
ARShadowGAN: Shadow Generative Adversarial Network for Augmented,CVPR_2020,4,"Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012 , 2015.",11
ARShadowGAN: Shadow Generative Adversarial Network for Augmented,CVPR_2020,5,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE conference on com- puter vision and pattern recognition (CVPR) , pages 248– 255. IEEE, 2009.",6
DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes,CVPR_2020,1,"Abhijit Kundu, Yin Li, and James M Rehg. 3d-rcnn: Instance-level 3d object reconstruction via render-and- compare. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 3559–3568, 2018. 2",3
DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes,CVPR_2020,2,"Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi- ous: Real time end-to-end 3d detection, tracking and motion forecasting with a single convolutional net. In Proceedings of the IEEE conference on Computer Vision and Pattern Recog- nition , pages 3569–3577, 2018. 2",3
DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes,CVPR_2020,3,"D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. Proc. symposium on discrete algorithms , 2007. 4",2
DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes,CVPR_2020,4,"A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, and H. Su. Shapenet: An information-rich 3d model repository. In arXiv:1512.03012 , 2015. 7",2
DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes,CVPR_2020,5,"Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object detection network for autonomous driving. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 1907–1915, 2017. 2",5
PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames,CVPR_2020,1,"M. M. Bronstein, J. Bruna, Y . LeCun, A. Szlam, and P. Van- dergheynst. Geometric deep learning: Going beyond eu- clidean data. IEEE Signal Processing Magazine , 34(4):18– 42, July 2017. 1,3",2
PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames,CVPR_2020,2,"Matan Atzmon, Haggai Maron, and Yaron Lipman. Point convolutional neural networks by extension operators. ACM Trans. Graph. , 37(4):71:1–71:12, July 2018. 2",3
PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames,CVPR_2020,3,"Federica Bogo, Javier Romero, Matthew Loper, and Michael J. Black. FAUST: Dataset and evaluation for 3D mesh registration. In CVPR , June 2014. 6",4
PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames,CVPR_2020,4,"Davide Boscaini, Jonathan Masci, Emanuele Rodol `a, and Michael Bronstein. Learning shape correspondence with anisotropic convolutional neural networks. In NIPS , pages 3189–3197. 2016. 1,2,4,6,7 13585",4
PFCNN: Convolutional Neural Networks on 3D Surfaces Using Parallel Frames,CVPR_2020,5,"Max Budninskiy, Gloria Yin, Leman Feng, Yiying Tong, and Mathieu Desbrun. Parallel transport unfolding: A connection-based manifold learning approach. SIAM J. Appl. Algebra Geom. , 3:266–291, 2018. 4",5
Enhancing Generic Segmentation with Learned Region Representations,CVPR_2020,1,"L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille. Semantic image segmentation with deep con- volutional nets and fully connected crfs. In ICLR , 2015.",2
Enhancing Generic Segmentation with Learned Region Representations,CVPR_2020,2,"R. Hadsell, S. Chopra, and Y . LeCun. Dimensionality reduc- tion by learning an invariant mapping. In Computer vision and pattern recognition, 2006 IEEE computer society con- ference on , volume 2, pages 1735–1742. IEEE, 2006.",2
Enhancing Generic Segmentation with Learned Region Representations,CVPR_2020,3,"E. Hoffer and N. Ailon. Deep metric learning using triplet network. In International Workshop on Similarity-Based Pattern Recognition , pages 84–92. Springer, 2015.[16] S. Horiguchi, D. Ikami, and K. Aizawa. Signiﬁcance of softmax-based features in comparison to distance metric learning-based features. IEEE transactions on pattern anal- ysis and machine intelligence , 2019.",2
Enhancing Generic Segmentation with Learned Region Representations,CVPR_2020,4,"P. Arbelaez. Boundary extraction in natural images using ul- trametric contour maps. In 2006 Conference on Computer Vision and Pattern Recognition Workshop (CVPRW’06) , pages 182–182. IEEE, 2006.",2
Enhancing Generic Segmentation with Learned Region Representations,CVPR_2020,5,"P. Arbelaez, M. Maire, C. Fowlkes, and J. Malik. Contour detection and hierarchical image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. , 33(5):898–916, May 2011.",2
Transform and Tell: Entity-Aware News Image Captioning,CVPR_2020,1,"Ali Furkan Biten, Lluis Gomez, Marcal Rusinol, and Di- mosthenis Karatzas. Good news, everyone! context driven entity-aware captioning for news images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2019.",4
Transform and Tell: Entity-Aware News Image Captioning,CVPR_2020,2,"Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional net- works. In Proceedings of the 34th International Conference on Machine Learning , volume 70 of Proceedings of Machine Learning Research , pages 933–941, Sydney, Australia, 06– 11 Aug 2017. PMLR.",2
Transform and Tell: Entity-Aware News Image Captioning,CVPR_2020,3,"Dianqi Li, Qiuyuan Huang, Xiaodong He, Lei Zhang, and Ming-Ting Sun. Generating diverse and accurate visual cap- tions by comparative adversarial learning. arXiv preprint arXiv:1804.00861 , 2018.",5
Transform and Tell: Entity-Aware News Image Captioning,CVPR_2020,4,"Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) , June 2018.",7
Transform and Tell: Entity-Aware News Image Captioning,CVPR_2020,5,"Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2015.",1
Action Modiﬁers: Learning from Adverbs in Instructional Videos,CVPR_2020,1,"De-An Huang, Joseph J Lim, Li Fei-Fei, and Juan Car- los Niebles. Unsupervised visual-linguistic reference reso- lution in instructional videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 2183–2192, 2017. 2",4
Action Modiﬁers: Learning from Adverbs in Instructional Videos,CVPR_2020,2,"Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsu- pervised learning from narrated instruction videos. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 4575–4583, 2016. 1,2",6
Action Modiﬁers: Learning from Adverbs in Instructional Videos,CVPR_2020,3,"Jean-Baptiste Alayrac, Ivan Laptev, Josef Sivic, and Simon Lacoste-Julien. Joint discovery of object states and manip- ulation actions. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) , pages 2127–2136, 2017. 2",4
Action Modiﬁers: Learning from Adverbs in Instructional Videos,CVPR_2020,4,"Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa- jdla, and Josef Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 5297–5307, 2016. 2",5
Action Modiﬁers: Learning from Adverbs in Instructional Videos,CVPR_2020,5,"Damian Borth, Rongrong Ji, Tao Chen, Thomas Breuel, and Shih-Fu Chang. Large-scale visual sentiment ontology and detectors using adjective noun pairs. In Proceedings of the 21st ACM international conference on Multimedia , pages 223–232, 2013. 2",5
Dynamic Face Video Segmentation via Reinforcement Learning,CVPR_2020,1,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.",4
Dynamic Face Video Segmentation via Reinforcement Learning,CVPR_2020,2,"Kuang-chih Lee, Dragomir Anguelov, Baris Sumengen, and Salih Burak Gokturk. Markov random ﬁeld models for hair and face segmentation. In Automatic Face & Gesture Recog- nition, 2008. FG’08. 8th IEEE International Conference on , pages 1–6. IEEE, 2008.",4
Dynamic Face Video Segmentation via Reinforcement Learning,CVPR_2020,3,"Jie Shen, Stefanos Zafeiriou, Grigoris G Chrysos, Jean Kos- saiﬁ, Georgios Tzimiropoulos, and Maja Pantic. The ﬁrst facial landmark tracking in-the-wild challenge: Benchmark and results. In Proceedings of the IEEE International Confer- ence on Computer Vision Workshops , pages 50–58, 2015.",6
Dynamic Face Video Segmentation via Reinforcement Learning,CVPR_2020,4,"Victor do Nascimento Silva and Luiz Chaimowicz. Moba: a new arena for game ai. arXiv preprint arXiv:1705.10443 , 2017.",1
Dynamic Face Video Segmentation via Reinforcement Learning,CVPR_2020,5,"Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jef- frey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfel- low, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorﬂow.org.",7
ActBERT: Learning Global-Local Video-Text Representations,CVPR_2020,1,"Benjamin Klein, Guy Lev, Gil Sadeh, and Lior Wolf. Asso- ciating neural word embeddings with deep image represen- tations using ﬁsher vectors. In CVPR , 2015. 8",4
ActBERT: Learning Global-Local Video-Text Representations,CVPR_2020,2,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV , 2014. 4",8
ActBERT: Learning Global-Local Video-Text Representations,CVPR_2020,3,"Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu- man action video dataset. arXiv preprint arXiv:1705.06950 , 2017. 6",11
ActBERT: Learning Global-Local Video-Text Representations,CVPR_2020,4,"Karen Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556 , 2014. 7",1
ActBERT: Learning Global-Local Video-Text Representations,CVPR_2020,5,"Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Josef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsu- pervised learning from narrated instruction videos. In CVPR , 2016. 2,8",6
Predicting Cognitive Declines Using Longitudinally Enriched Representations,CVPR_2020,1,"Lyujian Lu, Hua Wang, Xiaohui Yao, Shannon Risacher, An- drew Saykin, and Li Shen. Predicting progressions of cog- nitive outcomes via high-order multi-modal multi-task fea- ture learning. In 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018) , pages 545–548. IEEE, 2018.",6
Predicting Cognitive Declines Using Longitudinally Enriched Representations,CVPR_2020,2,"Feiping Nie, Hua Wang, Heng Huang, and Chris Ding. Early active learning via robust representation and structured spar- sity. In Twenty-Third International Joint Conference on Ar- tiﬁcial Intelligence , 2013.",4
Predicting Cognitive Declines Using Longitudinally Enriched Representations,CVPR_2020,3,"Cynthia M Stonnington, Carlton Chu, Stefan Kl ¨oppel, Clif- ford R Jack Jr, John Ashburner, Richard SJ Frackowiak, Alzheimer Disease Neuroimaging Initiative, et al. Predicting clinical scores from magnetic resonance scans in alzheimer’s disease. Neuroimage , 51(4):1405–1413, 2010.",8
Predicting Cognitive Declines Using Longitudinally Enriched Representations,CVPR_2020,4,"Hua Wang, Feiping Nie, and Heng Huang. Robust dis- tance metric learning via simultaneous ℓ1-norm minimiza- tion and maximization. In International Conference on Ma- chine Learning (ICML 2014) , pages 1836–1844, 2014.",3
Predicting Cognitive Declines Using Longitudinally Enriched Representations,CVPR_2020,5,"Hua Wang, Feiping Nie, Heng Huang, Shannon Risacher, Chris Ding, Andrew J Saykin, Li Shen, et al. Sparse multi- task regression and feature selection to identify brain imag- ing predictors for memory performance. In ICCV , pages 557–562. IEEE, 2011.",8
Fine-grained Image-to-Image Transformation towards Visual Recognition,CVPR_2020,1,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR , pages 770–778, 2016. 2,3,4,6[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib- rium. In Advances in Neural Information Processing Sys- tems, pages 6626–6637, 2017. 5",4
Fine-grained Image-to-Image Transformation towards Visual Recognition,CVPR_2020,2,"Christian Ledig, Lucas Theis, Ferenc Husz ´ar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo- realistic single image super-resolution using a generative ad- versarial network. In CVPR , pages 4681–4690, 2017. 2",11
Fine-grained Image-to-Image Transformation towards Visual Recognition,CVPR_2020,3,"Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340 , 2017. 1",3
Fine-grained Image-to-Image Transformation towards Visual Recognition,CVPR_2020,4,"Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. Cvae-gan: ﬁne-grained image generation through asymmetric training. In Proceedings of the IEEE Interna- tional Conference on Computer Vision , pages 2745–2754, 2017. 1,2",5
Fine-grained Image-to-Image Transformation towards Visual Recognition,CVPR_2020,5,"Jianmin Bao, Dong Chen, Fang Wen, Houqiang Li, and Gang Hua. Towards open-set identity preserving face synthesis. In CVPR , pages 6713–6722, 2018. 1",5
Convulsive Movement Detection using Low-Resolution,CVPR_2020,1,"Sandor Beniczky, Tilman Polster, Troels W Kjaer, and Helle Hjalgrim. Detection of generalized tonic–clonic seizures by a wireless wrist accelerometer: a prospective, multicenter study. Epilepsia , 54(4):e58–e61, 2013.",4
Convulsive Movement Detection using Low-Resolution,CVPR_2020,2,"Fatih Erden, E Birey Soyer, B Ugur Toreyin, and A Enis Cetin. V oc gas leak detection using pyro-electric infrared sensors. In 2010 IEEE International Conference on Acous- tics, Speech and Signal Processing , pages 1682–1685. IEEE, 2010.",4
Convulsive Movement Detection using Low-Resolution,CVPR_2020,3,"Fatih Erden, B Ugur Toreyin, E Birey Soyer, Ihsan Inac, Os- man Gunay, Kivanc Kose, and A Enis Cetin. Wavelet based ﬂickering ﬂame detector using differential pir sensors. Fire Safety Journal , 53:13–18, 2012.",7
Convulsive Movement Detection using Low-Resolution,CVPR_2020,4,"Jean Gotman. Automatic seizure detection: improvements and evaluation. Electroencephalography and clinical Neuro- physiology , 76(4):317–324, 1990.",2
Convulsive Movement Detection using Low-Resolution,CVPR_2020,5,"Zhanjian Li, Ant ´onio Martins da Silva, and Jo ˜ao Paulo Silva Cunha. Movement quantiﬁcation in epileptic seizures: a new approach to video-eeg analysis. IEEE Transactions on Biomedical Engineering , 49(6):565–573, 2002.",3
Connect-and-Slice: an hybrid approach for reconstructing 3D objects,CVPR_2020,1,"M. Berger, A. Tagliasacchi, L. Seversky, P. Alliez, G. Guen- nebaud, J. Levine, A. Sharf, and C. Silva. A survey of sur- face reconstruction from point clouds. Computer Graphics Forum , 36(1), 2017.",2
Connect-and-Slice: an hybrid approach for reconstructing 3D objects,CVPR_2020,2,"A. Boulch, M. De La Gorce, and R. Marlet. Piecewise-planar 3d reconstruction with edge and corner regularization. Com- puter Graphics Forum , 33(5), 2014.",2
Connect-and-Slice: an hybrid approach for reconstructing 3D objects,CVPR_2020,3,"A.-L. Chauve, P. Labatut, and J.-P. Pons. Robust piecewise- planar 3D reconstruction and completion from large-scale unstructured point data. In Proc. of Computer Vision and Pattern Recognition (CVPR) , 2010.",2
Connect-and-Slice: an hybrid approach for reconstructing 3D objects,CVPR_2020,4,"F. Lafarge and P. Alliez. Surface reconstruction through point set structuring. In Computer Graphics Forum , vol- ume 32, 2013.",2
Connect-and-Slice: an hybrid approach for reconstructing 3D objects,CVPR_2020,5,"Y . Li, X. Wu, Y . Chrysanthou, A. Sharf, D. Cohen-Or, and N. Mitra. Globﬁt: Consistently ﬁtting primitives by discovering global relations. Trans. on Graphics , 30(4), 2011.",2
Continual Learning with Extended Kronecker-factored Approximate Curvature,CVPR_2020,1,"Aleksandar Botev, Hippolyt Ritter, and David Barber. Prac- tical Gauss–Newton optimisation for deep learning. In Pro- ceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 557–565. JMLR. org, 2017.",3
Continual Learning with Extended Kronecker-factored Approximate Curvature,CVPR_2020,2,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.",4
Continual Learning with Extended Kronecker-factored Approximate Curvature,CVPR_2020,3,"Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 139–154, 2018.",5
Continual Learning with Extended Kronecker-factored Approximate Curvature,CVPR_2020,4,"Jimmy Ba, Roger Grosse, and James Martens. Distributed second-order optimization using Kronecker-factored approx- imations. In International Conference on Learning Repre- sentations , 2017.",3
Continual Learning with Extended Kronecker-factored Approximate Curvature,CVPR_2020,5,"Chris Bishop. Exact calculation of the Hessian matrix for the multilayer perceptron. Neural Computation , 4(4):494–501, 1992.",2
Sequential 3D Human Pose and Shape Estimation from Point Clouds,CVPR_2020,1,"Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In European Conference on Computer Vision , pages 561–578, 2016.",6
Sequential 3D Human Pose and Shape Estimation from Point Clouds,CVPR_2020,2,"Kaiwen Guo, Feng Xu, Tao Yu, Xiaoyang Liu, Qionghai Dai, and Yebin Liu. Real-time geometry, albedo and motion re- construction using a single rgbd camera. ACM Transactions on Graphics , 2017.",6
Sequential 3D Human Pose and Shape Estimation from Point Clouds,CVPR_2020,3,"Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. PointNet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in Neural Informa- tion Processing Systems , pages 5099–5108, 2017.",4
Sequential 3D Human Pose and Shape Estimation from Point Clouds,CVPR_2020,4,"Federica Bogo, Javier Romero, Gerard Pons-Moll, and Michael J. Black. Dynamic FAUST: Registering human bod- ies in motion. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , July 2017.",4
Sequential 3D Human Pose and Shape Estimation from Point Clouds,CVPR_2020,5,"Angjoo Kanazawa, Michael J. Black, David W. Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7122–7131, 2018.",2
Don’t Even Look Once: Synthesizing Features for Zero-Shot Detection,CVPR_2020,1,"Ali Farhadi, Ian Endres, Derek Hoiem, and David Forsyth. Describing objects by their attributes. In 2009 IEEE Con- ference on Computer Vision and Pattern Recognition , pages 1778–1785. IEEE, 2009. 2,7",4
Don’t Even Look Once: Synthesizing Features for Zero-Shot Detection,CVPR_2020,2,"Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chel- lappa, and Ajay Divakaran. Zero-shot object detection. In Proceedings of the European Conference on Computer Vi- sion (ECCV) , pages 384–400, 2018. 2,3,5,6,7",5
Don’t Even Look Once: Synthesizing Features for Zero-Shot Detection,CVPR_2020,3,"Gregory Castanon, Mohamed Elgharib, Venkatesh Saligrama, and Pierre-Marc Jodoin. Retrieval in long- surveillance videos using user-described motion and object attributes. IEEE Transactions on Circuits and Systems for Video Technology , 26(12):2313–2327, 2016. 2",4
Don’t Even Look Once: Synthesizing Features for Zero-Shot Detection,CVPR_2020,4,"Wei-Lun Chao, Soravit Changpinyo, Boqing Gong, and Fei Sha. An Empirical Study and Analysis of Generalized Zero- Shot Learning for Object Recognition in the Wild , pages 52– 68. Springer International Publishing, Cham, 2016. 2",4
Don’t Even Look Once: Synthesizing Features for Zero-Shot Detection,CVPR_2020,5,"Long Chen, Hanwang Zhang, Jun Xiao, Wei Liu, and Shih- Fu Chang. Zero-shot visual recognition using semantics- preserving adversarial embedding network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , volume 2, 2018. 2",5
UCTGAN: Diverse Image Inpainting based on Unsupervised Cross-Space,CVPR_2020,1,"Carl Doersch, Saurabh Singh, Abhinav Gupta, Josef Sivic, and Alexei A. Efros. What makes paris look like paris? Acm Transactions on Graphics , 31(4):1–9, 2012.",5
UCTGAN: Diverse Image Inpainting based on Unsupervised Cross-Space,CVPR_2020,2,"Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In Proceedings of the European Conference on Computer Vi- sion (ECCV) , pages 172–189, 2018.",4
UCTGAN: Diverse Image Inpainting based on Unsupervised Cross-Space,CVPR_2020,3,"Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 85–100, 2018.",6
UCTGAN: Diverse Image Inpainting based on Unsupervised Cross-Space,CVPR_2020,4,"Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei A Efros. Context encoders: Featurelearning by inpainting. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 2536–2544, 2016.",5
UCTGAN: Diverse Image Inpainting based on Unsupervised Cross-Space,CVPR_2020,5,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems , pages 2672–2680, 2014.",8
Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image,CVPR_2020,1,"Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: a simple way to prevent neural networks from overﬁtting. J. Mach. Learning Research , 15(1):1929–1958, 2014. 2",5
Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image,CVPR_2020,2,"Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In Proc. ICML , pages 1050–1059, 2016. 2,5",1
Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image,CVPR_2020,3,"Joshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. Proc. ICML , 2019. 1,2,3,5",1
Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image,CVPR_2020,4,"Chenglong Bao, Hui Ji, Yuhui Quan, and Zuowei Shen. Dictionary learning for sparse coding: Algorithms and convergence analysis. Trans. Pattern Anal. Mach. Intell. , 38(7):1356–1369, 2015. 3,5",4
Self2Self With Dropout: Learning Self-Supervised Denoising From Single Image,CVPR_2020,5,"Alexander Krull, Tim-Oliver Buchholz, and Florian Jug. Noise2void-learning denoising from single noisy images. In Proc. CVPR , pages 2129–2137, 2019. 1,2,3,5",3
Imitative Non-Autoregressive Modeling for Trajectory,CVPR_2020,1,"Dheeru Dua and Casey Graff. UCI machine learning reposi- tory, 2017.",1
Imitative Non-Autoregressive Modeling for Trajectory,CVPR_2020,2,"Jerome Friedman, Trevor Hastie, and Robert Tibshirani. The elements of statistical learning , volume 1. Springer series in statistics New York, 2001.",3
Imitative Non-Autoregressive Modeling for Trajectory,CVPR_2020,3,"Matteo Lisotto, Pasquale Coscia, and Lamberto Ballan. So- cial and scene-aware trajectory prediction in crowded spaces. InProc. ICCVW . IEEE, 2019.",3
Imitative Non-Autoregressive Modeling for Trajectory,CVPR_2020,4,"Katerina Fragkiadaki, Pulkit Agrawal, Sergey Levine, and Jitendra Malik. Learning visual predictive models of physics for playing billiards. arXiv preprint arXiv:1511.07404 , 2015.",4
Imitative Non-Autoregressive Modeling for Trajectory,CVPR_2020,5,"Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimiza- tion. In Proc. ICML . JMLR, 2016.",3
In Defense of the Triplet Loss Again: Learning Robust Person Re-Identiﬁcation,CVPR_2020,1,"Weihua Chen, Xiaotang Chen, Jianguo Zhang, and Kaiqi Huang. Beyond triplet loss: A deep quadruplet network for person re-identiﬁcation. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) , July 2017.",4
In Defense of the Triplet Loss Again: Learning Robust Person Re-Identiﬁcation,CVPR_2020,2,"Kai Li, Zhengming Ding, Kunpeng Li, Yulun Zhang, and Yun Fu. Support neighbor loss for person re-identiﬁcation. In2018 ACM Multimedia Conference on Multimedia Con- ference , pages 1492–1500. ACM, 2018.",5
In Defense of the Triplet Loss Again: Learning Robust Person Re-Identiﬁcation,CVPR_2020,3,"Xiang Bai, Mingkun Yang, Tengteng Huang, Zhiyong Dou, Rui Yu, and Yongchao Xu. Deep-person: Learning dis- criminative deep features for person re-identiﬁcation. arXiv preprint arXiv:1711.10658 , 2017.",6
In Defense of the Triplet Loss Again: Learning Robust Person Re-Identiﬁcation,CVPR_2020,4,"Samuel Rota Bul `o, Lorenzo Porzi, and Peter Kontschieder. Dropout distillation. In Proceedings of the 33nd Interna- tional Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016 , pages 99–107, 2016.",3
In Defense of the Triplet Loss Again: Learning Robust Person Re-Identiﬁcation,CVPR_2020,5,"Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang Chen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd- net: Attentive but diverse person re-identiﬁcation. ICCV , 2019.",8
Fixed-Point Back-Propagation Training,CVPR_2020,1,"Olga Russakovsky, Jia Deng, et al. Imagenet large scale vi- sual recognition challenge. IJCV , 115(3):211–252, 2015.",3
Fixed-Point Back-Propagation Training,CVPR_2020,2,"Tao Sheng and Chen Feng. A quantization-friendly separable convolution for mobilenets. In EMC2 , pages 14–18. IEEE, 2018.",1
Fixed-Point Back-Propagation Training,CVPR_2020,3,"Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv:1409.1556 , 2014.",1
Fixed-Point Back-Propagation Training,CVPR_2020,4,"Tianqi Chen, Mu Li, and Yutian Li. Mxnet: A ﬂexible and efﬁcient machine learning library for heterogeneous dis- tributed systems. arXiv:1512.01274 , 2015.",3
Fixed-Point Back-Propagation Training,CVPR_2020,5,"Felix Hieber and Tobias Domhan. Sockeye: A Toolkit for Neural Machine Translation. arXiv:1712.05690 , Dec. 2017.",1
Discrete Model Compression with Resource Constraint,CVPR_2020,1,"Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, Cite- seer, 2009.",1
Discrete Model Compression with Resource Constraint,CVPR_2020,2,"Shaohui Lin, Rongrong Ji, Chenqian Yan, Baochang Zhang, Liujuan Cao, Qixiang Ye, Feiyue Huang, and David Doer- mann. Towards optimal structured cnn pruning via genera- tive adversarial learning. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 2790–2799, 2019.",8
Discrete Model Compression with Resource Constraint,CVPR_2020,3,"Christos Louizos, Max Welling, and Diederik P. Kingma. Learning sparse neural networks through l0regularization. InInternational Conference on Learning Representations , 2018.",3
Discrete Model Compression with Resource Constraint,CVPR_2020,4,"Dmitry Molchanov, Arsenii Ashukha, and Dmitry Vetrov. Variational dropout sparsiﬁes deep neural networks. In Pro- ceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 2498–2507. JMLR. org, 2017.",3
Discrete Model Compression with Resource Constraint,CVPR_2020,5,"Kirill Neklyudov, Dmitry Molchanov, Arsenii Ashukha, and Dmitry P Vetrov. Structured bayesian pruning via log-normal multiplicative noise. In Advances in Neural Information Pro- cessing Systems , pages 6775–6784, 2017.",4
Progressive Mirror Detection,CVPR_2020,1,"Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet:edge guidance network for salient object detection. In ICCV , Oct 2019.",6
Progressive Mirror Detection,CVPR_2020,2,"Radhakrishna Achanta, Sheila Hemami, Francisco Estrada, and Sabine S ¨usstrunk. Frequency-tuned salient region detec- tion. In CVPR , pages 1597–1604, 2009.",4
Progressive Mirror Detection,CVPR_2020,3,"Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S ¨underhauf, Ian Reid, Stephen Gould, and 43283704 Anton van den Hengel. Vision-and-language navigation: In- terpreting visually-grounded navigation instructions in real environments. In CVPR , pages 3674–3683, 2018.",9
Progressive Mirror Detection,CVPR_2020,4,"Sean Bell, Paul Upchurch, Noah Snavely, and Kavita Bala. Material recognition in the wild with the materials in context database. CVPR , 2015.",4
Progressive Mirror Detection,CVPR_2020,5,"Maxim Berman, Amal Rannen Triki, and Matthew B Blaschko. The lov ´asz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In CVPR , pages 4413–4421, 2018.",3
Spatially Attentive Output Layer for Image Classiﬁcation,CVPR_2020,1,"Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un- supervised representation learning by predicting image rota- tions. In International Conference on Learning Representa- tions , 2018. 2,4",3
Spatially Attentive Output Layer for Image Classiﬁcation,CVPR_2020,2,"Aditya Chattopadhay, Anirban Sarkar, Prantik Howlader, and Vineeth N Balasubramanian. Grad-cam++: General- ized gradient-based visual explanations for deep convolu- tional networks. In 2018 IEEE Winter Conference on Ap- plications of Computer Vision (WACV) , 2018. 2",4
Spatially Attentive Output Layer for Image Classiﬁcation,CVPR_2020,3,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Deeplab: Semantic im- age segmentation with deep convolutional nets, atrous con- volution, and fully connected crfs. IEEE Transactions on Pattern Analysis and Machine Intelligence , 40(4):834 – 848, 2018. 1,7",5
Spatially Attentive Output Layer for Image Classiﬁcation,CVPR_2020,4,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV) , pages 801–818, 2018. 2,4,7",5
Spatially Attentive Output Layer for Image Classiﬁcation,CVPR_2020,5,"Junsuk Choe and Hyunjung Shim. Attention-based dropout layer for weakly supervised object localization. In Proceed- ings of the IEEE conference on computer vision and pattern recognition (CVPR) , pages 2219–2228, 2019. 8",1
Computing Valid p-values for Image Segmentation by Selective Inference,CVPR_2020,1,"N. Otsu. A threshold selection method from gray-level his- tograms. IEEE transactions on systems, man, and cybernet- ics, 9(1):62–66, 1979. 1,2,4,5",2
Computing Valid p-values for Image Segmentation by Selective Inference,CVPR_2020,2,"F. Rousseau, F. Blanc, J. de Seze, L. Rumbach, and J.-P. Armspach. An a contrario approach for outliers segmen- tation: Application to multiple sclerosis in mri. In 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro , pages 9–12. IEEE, 2008. 2",2
Computing Valid p-values for Image Segmentation by Selective Inference,CVPR_2020,3,"R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society: Series B (Methodological) , 58(1):267–288, 1996. 2",2
Computing Valid p-values for Image Segmentation by Selective Inference,CVPR_2020,4,"K. T. Bae, M. L. Giger, C.-T. Chen, and C. E. Kahn. Auto- matic segmentation of liver structure in ct images. Medical physics , 20(1):71–78, 1993. 7",2
Computing Valid p-values for Image Segmentation by Selective Inference,CVPR_2020,5,"Y . Boykov and G. Funka-Lea. Graph cuts and efﬁcient nd image segmentation. International journal of computer vi- sion, 70(2):109–131, 2006. 1,2,4,5",2
Regularizing Class-wise Predictions via Self-knowledge Distillation,CVPR_2020,1,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR , 2009. 2,3",6
Regularizing Class-wise Predictions via Self-knowledge Distillation,CVPR_2020,2,"Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for ﬁne-grained image categorization. In CVPR , 2011. 2,3",4
Regularizing Class-wise Predictions via Self-knowledge Distillation,CVPR_2020,3,"Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational information dis- tillation for knowledge transfer. In CVPR , 2019. 1,8",5
Regularizing Class-wise Predictions via Self-knowledge Distillation,CVPR_2020,4,"Philip Bachman, Ouais Alsharif, and Doina Precup. Learn- ing with pseudo-ensembles. In NeurIPS , 2014. 2,7,8",3
Regularizing Class-wise Predictions via Self-knowledge Distillation,CVPR_2020,5,"Christopher Bishop. Regularization and complexity control in feed-forward networks. In ICANN , 1995. 1,8",2
DA-cGAN: A Framework for Indoor Radio Design Using a Dimension-Aware,CVPR_2020,1,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27 , pages 2672– 2680. Curran Associates, Inc., 2014. 2",8
DA-cGAN: A Framework for Indoor Radio Design Using a Dimension-Aware,CVPR_2020,2,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adver- sarial networks. CoRR , abs/1611.07004, 2016. 2",4
DA-cGAN: A Framework for Indoor Radio Design Using a Dimension-Aware,CVPR_2020,3,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. CoRR , abs/1505.04597, 2015. 2",3
DA-cGAN: A Framework for Indoor Radio Design Using a Dimension-Aware,CVPR_2020,4,"Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision , 2016. 4",3
DA-cGAN: A Framework for Indoor Radio Design Using a Dimension-Aware,CVPR_2020,5,"Xiaolong Wang and Abhinav Gupta. Generative image mod- eling using style and structure adversarial networks. In ECCV , 2016. 3",1
Generalized Class Incremental Learning,CVPR_2020,1,"Robert M French. Catastrophic forgetting in connection- ist networks. Trends in Cognitive Sciences , pages 128–135, 1999. 1,2",1
Generalized Class Incremental Learning,CVPR_2020,2,"Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 9268–9277, 2019. 4",5
Generalized Class Incremental Learning,CVPR_2020,3,"David Lopez-Paz and Marc’Aurelio Ranzato. Gradient episodic memory for continual learning. In Advances in Neural Information Processing Systems , pages 6467–6476, 2017. 2,3",1
Generalized Class Incremental Learning,CVPR_2020,4,"Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuytelaars. Task-free continual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 11254–11263, 2019. 2",3
Generalized Class Incremental Learning,CVPR_2020,5,"Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classi- ﬁer and representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2001–2010, 2017. 1,2,3,4",4
Mesh Variational Autoencoders with Edge Contraction Pooling,CVPR_2020,1,"Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Se- bastian Thrun, Jim Rodgers, and James Davis. SCAPE: shape completion and animation of people. ACM Transac- tions on Graphics (TOG) , 24(3):408–416, 2005. 4,5",6
Mesh Variational Autoencoders with Edge Contraction Pooling,CVPR_2020,2,"Davide Boscaini, Jonathan Masci, Emanuele Rodol `a, and Michael Bronstein. Learning shape correspondence with anisotropic convolutional neural networks. In NIPS , pages 3189–3197, 2016. 2",4
Mesh Variational Autoencoders with Edge Contraction Pooling,CVPR_2020,3,"Davide Boscaini, Jonathan Masci, Emanuele Rodol `a, Michael M Bronstein, and Daniel Cremers. Anisotropic dif- fusion descriptors. Computer Graphics Forum , 35(2):431– 441, 2016. 2",5
Mesh Variational Autoencoders with Edge Contraction Pooling,CVPR_2020,4,"Mario Botsch and Leif Kobbelt. A remeshing approach to multiresolution modeling. In SGP, pages 185–192, 2004. 2, 4,5",1
Mesh Variational Autoencoders with Edge Contraction Pooling,CVPR_2020,5,"Michael M. Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond Euclidean data. IEEE Signal Processing Mag- azine , 34(4):18–42, 2017. 2",2
Fold Electrocardiogram Into a Fingerprint,CVPR_2020,1,"George B Moody and Roger G Mark. The impact of the mit- bih arrhythmia database. IEEE Engineering in Medicine and Biology Magazine , 20(3):45–50, 2001. 5",1
Fold Electrocardiogram Into a Fingerprint,CVPR_2020,2,"Lena Biel, Ola Pettersson, Lennart Philipson, and Peter Wide. Ecg analysis: a new approach in human identiﬁca- tion. IEEE Transactions on Instrumentation and Measure- ment , 50(3):808–812, 2001. 3",4
Fold Electrocardiogram Into a Fingerprint,CVPR_2020,3,"Yeong-Hyeon Byeon and Keun-Chang Kwak. Pre- conﬁgured deep convolutional neural networks with various time-frequency representations for biometrics from ecg sig- nals. Applied Sciences , 9(22):4810, 2019. 2",1
Fold Electrocardiogram Into a Fingerprint,CVPR_2020,4,"Carlos Carreiras, Andr ´e Lourenc ¸o, Hugo Silva, Ana Fred, and Rui Ferreira. Evaluating template uniqueness in ecg bio- metrics. In Informatics in Control, Automation and Robotics , pages 111–123. Springer, 2016. 1",5
Fold Electrocardiogram Into a Fingerprint,CVPR_2020,5,"Ying Chen and Wenxi Chen. Finger ecg based two-phase authentication using 1d convolutional neural networks. In 2018 40th Annual International Conference of the IEEE En-gineering in Medicine and Biology Society (EMBC) , pages 336–339. IEEE, 2018. 2",1
LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking,CVPR_2020,1,"J. Zhu, H. Yang, N. Liu, M. Kim, W. Zhang, and M.-H. Yang, “Online multi-object tracking with dual matching attention networks,” in ECCV , 2018. 3",2
LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking,CVPR_2020,2,"M. Andriluka, U. Iqbal, E. Insafutdinov, L. Pishchulin, A. Milan, J. Gall, and B. Schiele, “Posetrack: A benchmark for human pose estimation and tracking,” in CVPR , 2018. 1, 2,5,8",2
LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking,CVPR_2020,3,"L. Ke, M.-C. Chang, H. Qi, and S. Lyu, “Multi-scale structure-aware network for human pose estimation,” ECCV , 2018. 2",2
LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking,CVPR_2020,4,“Posetrack challenge 2018.” https://posetrack. net/workshops/eccv2018/ . Accessed: 2019-02-10. 5,2
LightTrack: A Generic Framework for Online Top-Down Human Pose Tracking,CVPR_2020,5,"K. Bernardin and R. Stiefelhagen, “Evaluating multiple ob- ject tracking performance: the clear mot metrics,” Journal on Image and Video Processing , vol. 2008, p. 1, 2008. 5",2
A Non-invasive Vision Based Approach to Velocity Measurement of Skeleton,CVPR_2020,1,"Paolo De Leva. Adjustments to zatsiorsky-seluyanov’s segment inertia parameters. Journal of Biomechanics , 29(9):1223–1230, 1996. 2",2
A Non-invasive Vision Based Approach to Velocity Measurement of Skeleton,CVPR_2020,2,"T. Kaichi, S. Mori, H. Saito, K. Takahashi, D. Mikami, M. Isogawa, and H. Kimata. Estimation of center of mass for sports scene using weighted visual hull. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW) , pages 1890–18906, June 2018. 3",2
A Non-invasive Vision Based Approach to Velocity Measurement of Skeleton,CVPR_2020,3,"Alparslan Yildiz and Yusuf Sinan Akgul. A fast method for tracking people with multiple cameras. In Kiriakos N. Kutu- lakos, editor, Trends and Topics in Computer Vision: ECCV 2010 Workshops, Heraklion, Crete, Greece, September 10- 11, 2010, Revised Selected Papers, Part I , pages 128–138, Berlin, Heidelberg, 2010. Springer Berlin Heidelberg. 3",1
A Non-invasive Vision Based Approach to Velocity Measurement of Skeleton,CVPR_2020,4,"Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. OpenPose: realtime multi-person 2D pose estimation using Part Afﬁnity Fields. In arXiv preprint arXiv:1812.08008 , 2018. 1,3",5
A Non-invasive Vision Based Approach to Velocity Measurement of Skeleton,CVPR_2020,5,"Hao-Shu Fang, Shuqin Xie, Yu-Wing Tai, and Cewu Lu. RMPE: Regional multi-person pose estimation. In ICCV , 2017. 1",4
SAPIEN: A SimulAted Part-based Interactive ENvironment,CVPR_2020,1,"Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. OpenAI Gym. arXiv preprint arXiv:1606.01540 , 2016. 2,3, 4",7
SAPIEN: A SimulAted Part-based Interactive ENvironment,CVPR_2020,2,"Simon Brodeur, Ethan Perez, Ankesh Anand, Florian Golemo, Luca Celotti, Florian Strub, Jean Rouat, Hugo Larochelle, and Aaron Courville. HoME: A household multimodal environment. arXiv preprint arXiv:1711.11017 , 2017. 1,2,3",9
SAPIEN: A SimulAted Part-based Interactive ENvironment,CVPR_2020,3,"Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S ¨underhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-Language Navigation: Interpreting visually-grounded navigation instructions in real environments. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 1, 2",9
SAPIEN: A SimulAted Part-based Interactive ENvironment,CVPR_2020,4,"Berk Calli, Arjun Singh, James Bruce, Aaron Walsman, Kurt Konolige, Siddhartha Srinivasa, Pieter Abbeel, and Aaron M Dollar. Yale-CMU-Berkeley dataset for robotic manipulation research. The International Journal of Robotics Research , 36(3):261–268, 2017. 1",8
SAPIEN: A SimulAted Part-based Interactive ENvironment,CVPR_2020,5,"Alexandre Campeau-Lecours, Hugo Lamontagne, Simon Latour, Philippe Fauteux, V ´eronique Maheu, Franc ¸ois Boucher, Charles Deguire, and Louis-Joseph Caron L’Ecuyer. Kinova modular robot arms for service robotics applications. In Rapid Automation: Concepts, Methodolo- gies, Tools, and Applications , pages 693–719. IGI Global, 2019. 7",8
VSGNet: Spatial Attention Network for Detecting Human Object Interactions,CVPR_2020,1,"Saurabh Gupta and Jitendra Malik. Visual semantic role la- beling. arXiv preprint arXiv:1505.04474 , 2015. 1,2,5,6, 7",1
VSGNet: Spatial Attention Network for Detecting Human Object Interactions,CVPR_2020,2,"Tanmay Gupta, Alexander Schwing, and Derek Hoiem. No- frills human-object interaction detection: Factorization, lay- out encodings, and training techniques. In Proceedings of the IEEE International Conference on Computer Vision , pages 9677–9685, 2019. 6",3
VSGNet: Spatial Attention Network for Detecting Human Object Interactions,CVPR_2020,3,"Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia Deng. Learning to detect human-object interactions. In 2018 ieee winter conference on applications of computer vision (wacv) , pages 381–389. IEEE, 2018. 1,2,4,5,6,8",5
VSGNet: Spatial Attention Network for Detecting Human Object Interactions,CVPR_2020,4,"SM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton, et al. Attend, infer, repeat: Fast scene understanding with generative mod- els. In Advances in Neural Information Processing Systems , pages 3225–3233, 2016. 1",7
VSGNet: Spatial Attention Network for Detecting Human Object Interactions,CVPR_2020,5,"Haoqi Fan and Jiatong Zhou. Stacked latent attention for multimodal reasoning. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 1072–1080, 2018. 1",1
Abstract,CVPR_2020,1,"Vaswani, Ashish, Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. ""Attention is all you need."" Advances in neural information processing systems. pp. 5998-6008. 2017.",12
Abstract,CVPR_2020,2,"Ronneberger, Olaf; Fischer, Philipp; Brox, Thomas. U-net: Convolutional networks for biomedical image segmentation. In: International Conference on Medical image computing and computer-assisted intervention . Springer, pp. 234-241. 2015.",4
Abstract,CVPR_2020,3,"Zhou, Zongwei, M. M. R., Tajbakhsh, N., & Liang, J. ""Unet++: A nested u-net architecture for medical image Table 4: Ablation study on the Mouse cell image dataset. Table 3: Ablation study on the Drosophila cell image dataset. segmentation."" Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support. Springer, pp. 3-11. 2018.",3
Abstract,CVPR_2020,4,"Gers, Felix A., Jürgen Schmidhuber, and Fred Cummins. ""Learning to forget: Continual prediction with LSTM."" pp. 850-855. 1999.",4
Abstract,CVPR_2020,5,"He, Kaiming, X., Ren, S., & Sun, J. ""Deep residual learning for image recognition."" Proceedings of the IEEE conference on computer vision and pattern recognition . pp 770-778. 2016.",3
Self-supervised Monocular Trained Depth Estimation using,CVPR_2020,1,"David Eigen and Rob Fergus. Predicting depth, surface nor- mals and semantic labels with a common multi-scale convo- lutional architecture. In ICCV , 2015. 1,3,5,6,7",1
Self-supervised Monocular Trained Depth Estimation using,CVPR_2020,2,"Samuel Rota Bul `o, Lorenzo Porzi, and Peter Kontschieder. In-place activated batchnorm for memory-optimized training of dnns. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 5639–5647, 2018. 6",3
Self-supervised Monocular Trained Depth Estimation using,CVPR_2020,3,"David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning representations by back-propagating errors. Cognitive modeling , 5(3):1, 1988. 3",4
Self-supervised Monocular Trained Depth Estimation using,CVPR_2020,4,"Markus Achtelik, Abraham Bachrach, Ruijie He, Samuel Prentice, and Nicholas Roy. Stereo vision and laser odome- try for autonomous helicopters in gps-denied indoor environ- ments. In Unmanned Systems Technology XI , volume 7332, page 733219. International Society for Optics and Photonics, 2009. 3",5
Self-supervised Monocular Trained Depth Estimation using,CVPR_2020,5,"Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens, and Quoc V Le. Attention augmented convolutional net- works. In Proceedings of the IEEE International Conference on Computer Vision , pages 3286–3295, 2019. 3",5
Visual Parsing with Query-Driven Global Graph Attention (QD-GGA):,CVPR_2020,1,"B. Davis, B. Morse, S. Cohen, B. Price, and C. Tens- meyer, “Deep visual template-free form parsing,” arXiv preprint arXiv:1909.02576 , 2019. 2",2
Visual Parsing with Query-Driven Global Graph Attention (QD-GGA):,CVPR_2020,2,"T. Yao, Y . Pan, Y . Li, and T. Mei, “Exploring visual relationship for image captioning,” in Proceedings of the European conference on computer vision (ECCV) , 2018, pp. 684–699. 2,3",2
Visual Parsing with Query-Driven Global Graph Attention (QD-GGA):,CVPR_2020,3,"R. Zanibbi and D. Blostein, “Recognition and re- trieval of mathematical expressions,” International Journal on Document Analysis and Recognition (IJ- DAR) , vol. 15, no. 4, pp. 331–357, 2012. 1",2
Visual Parsing with Query-Driven Global Graph Attention (QD-GGA):,CVPR_2020,4,"R. Zanibbi and A. Orakwue, “Math search for the masses: Multimodal search interfaces and appearance-based retrieval,” in Conferences on Intel- ligent Computer Mathematics . Springer, 2015, pp. 18–36. 1",2
Visual Parsing with Query-Driven Global Graph Attention (QD-GGA):,CVPR_2020,5,"L. Hu and R. Zanibbi, “MST-based visual parsing of online handwritten mathematical expressions,” in 2016 15th International Conference on Frontiers in Handwriting Recognition (ICFHR) . IEEE, 2016, pp. 337–342. 1,3",2
Deep Distance Transform for Tubular Structure Segmentation in CT Scans,CVPR_2020,1,"Andreas Baak, Meinard M ¨uller, Gaurav Bharaj, Hans-Peter Seidel, and Christian Theobalt. A data-driven approach for real-time full body pose reconstruction from a depth camera. InProc. ICCV , 2011.",5
Deep Distance Transform for Tubular Structure Segmentation in CT Scans,CVPR_2020,2,"Fabian Isensee, Jens Petersen, Simon A. A. Kohl, Paul F. J¨ager, and Klaus H. Maier-Hein. nnu-net: Breaking the spell on successful medical image segmentation. CoRR , abs/1904.08128, 2019.",3
Deep Distance Transform for Tubular Structure Segmentation in CT Scans,CVPR_2020,3,"Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully Convolutional Networks for Semantic Segmentation. Proc. CVPR , 2015.",3
Deep Distance Transform for Tubular Structure Segmentation in CT Scans,CVPR_2020,4,"Delphine Nain, Anthony J. Yezzi, and Greg Turk. Vessel segmentation using a shape driven ﬂow. In Proc. MICCAI , 2004.",2
Deep Distance Transform for Tubular Structure Segmentation in CT Scans,CVPR_2020,5,"Mathias Perslev, Erik Bjørnager Dam, Akshay Pai, and Christian Igel. One network to segment them all: A general, lightweight system for accurate 3d medical image segmenta- tion. In MICCAI , 2019.",4
Color-Constrained Dehazing Model,CVPR_2020,1,"Raanan Fattal. Dehazing using color-lines. TOG , 34(1):13, 2014. 3",2
Color-Constrained Dehazing Model,CVPR_2020,2,"Raanan Fattal. Single image dehazing. TOG , 27(3):72, 2008. 2",2
Color-Constrained Dehazing Model,CVPR_2020,3,"Kaiming He, Jian Sun, and Xiaoou Tang. Single image haze removal using dark channel prior. In CVPR , 2009. 2",3
Color-Constrained Dehazing Model,CVPR_2020,4,"Dana Berman, Tali Treibitz, and Shai Avidan. Non-local im- age dehazing. In CVPR , 2016. 2,3,5,6",3
Color-Constrained Dehazing Model,CVPR_2020,5,"Robby T Tan. Visibility in bad weather from a single image. InCVPR , 2008. 2",1
Explaining Autonomous Driving by Learning End-to-End Visual Attention,CVPR_2020,1,"M. Bojarski, D. Del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba. End to end learn- ing for self-driving cars. arXiv preprint arXiv:1604.07316 , 2016.",2
Explaining Autonomous Driving by Learning End-to-End Visual Attention,CVPR_2020,2,"Y . Cai, D. Du, L. Zhang, L. Wen, W. Wang, Y . Wu, and S. Lyu. Guided attention network for object detection and counting on drones. arXiv preprint arXiv:1909.113071 , 2019.",2
Explaining Autonomous Driving by Learning End-to-End Visual Attention,CVPR_2020,3,"C. Cao, X. Liu, Y . Yang, Yu Y ., Z Wang J. Wang, Y . Huang, L. Wang, C. Huang, W. Xu, D. Ramanan, and T. S. Huang. Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks. Proceedings of the IEEE international conference on computer vision , pages 2956–2964, 2015.",2
Explaining Autonomous Driving by Learning End-to-End Visual Attention,CVPR_2020,4,"S. Chen, S. Zhang, J. Shang, B. Chen, and N. Zheng. Brain- inspired cognitive model with attention for self-driving cars. IEEE Transactions on Cognitive and Developmental Sys- tems, 2017.",2
Explaining Autonomous Driving by Learning End-to-End Visual Attention,CVPR_2020,5,"L. George, T. Buhet, E. Wirbel, G. Le-Gall, and X. Perrotton. Imitation learning for end to end vehicle longitudinal con- trol with forward camera. arXiv preprint arXiv:1812.05841 , 2018.",2
Multi-Stream CNN for Spatial Resource Allocation: a Crop Management,CVPR_2020,1,"S.L. Osborne, J.s Schepers, D Francis, and Michael R. Schlemmer. Use of spectral radiance to estimate in-season biomass and grain yield in nitrogen- and water-stressed corn. Crop science , 42:165–171, 2002. 1",2
Multi-Stream CNN for Spatial Resource Allocation: a Crop Management,CVPR_2020,2,"Lynette K. Abbott and Daniel V . Murphy. What is Soil Bi- ological Fertility? , pages 1–15. Springer Netherlands, Dor- drecht, 2007. 1",2
Multi-Stream CNN for Spatial Resource Allocation: a Crop Management,CVPR_2020,3,"P.K. Aggarwal. Uncertainties in crop, soil and weather inputs used in growth models: Implications for simulated outputs and their applications. Agricultural Systems , 48:361–384, 1995. 1",2
Multi-Stream CNN for Spatial Resource Allocation: a Crop Management,CVPR_2020,4,"John M. Antle, James W. Jones, and Cynthia E. Rosenzweig. Next generation agricultural system data, models and knowl- edge products: Introduction. Agricultural Systems , 155:186– 190, 2017. 1",2
Multi-Stream CNN for Spatial Resource Allocation: a Crop Management,CVPR_2020,5,"Badi H. Baltagi. A companion to theoretical econometrics . Blackwell, 2003. 1",2
Enhanced Transport Distance for Unsupervised Domain Adaptation,CVPR_2020,1,"Nicolas Courty, R ´emi Flamary, Devis Tuia, and Alain Rako- tomamonjy. Optimal transport for domain adaptation. IEEE Transactions on Pattern Analysis and Machine Intelligence , 39(9):1853–1865, 2017.",4
Enhanced Transport Distance for Unsupervised Domain Adaptation,CVPR_2020,2,"Yann LeCun, L ´eon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based learning applied to document recogni- tion. Proceedings of the IEEE , 86(11):2278–2324, 1998.",5
Enhanced Transport Distance for Unsupervised Domain Adaptation,CVPR_2020,3,"Martin Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasserstein generative adversarial networks. In ICML , pages 214–223, 2017.",3
Enhanced Transport Distance for Unsupervised Domain Adaptation,CVPR_2020,4,"Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. InNIPS , pages 137–144, 2007.",4
Enhanced Transport Distance for Unsupervised Domain Adaptation,CVPR_2020,5,"Bharath Bhushan Damodaran, Benjamin Kellenberger, R ´emi Flamary, Devis Tuia, and Nicolas Courty. DeepJDOT: Deep joint distribution optimal transport for unsupervised domain adaptation. In ECCV , pages 447–463, 2018.",5
Facial Action Unit Recognition in the Wild with Multi-Task CNN Self-Training,CVPR_2020,1,"Q. Xie, M.-T. Luong, E. Hovy, and Q. V . Le. Self- training with Noisy Student improves ImageNet classiﬁcation. arXiv:1911.04252v2 [cs.LG] , 2020. 1,2",2
Facial Action Unit Recognition in the Wild with Multi-Task CNN Self-Training,CVPR_2020,2,"A. Howard, M. Sandler, G. Chu, L.-C. Chen, B. Chen, M. Tan, W. Wang, Y . Zhu, R. Pang, V . Vasudevan, Q. V . Le, and H. Adam. Searching for MobileNetV3. In ICCV , 2019. 2",2
Facial Action Unit Recognition in the Wild with Multi-Task CNN Self-Training,CVPR_2020,3,"M. Tan and Q. V . Le. EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural Networks. In ICML , 2019. 2",2
Facial Action Unit Recognition in the Wild with Multi-Task CNN Self-Training,CVPR_2020,4,"Jiankang Deng, Jia Guo, Yuxiang Zhou, Jinke Yu, Irene Kot- sia, and Stefanos Zafeiriou. RetinaFace: Single-stage Dense Face Localisation in the Wild. arXiv:1905.00641 [cs.CV] , may 2019. 1",6
Facial Action Unit Recognition in the Wild with Multi-Task CNN Self-Training,CVPR_2020,5,"T. Baltrusaitis, A. Zadeh, Y . C. Lim, and L. P. Morency. Open- Face 2.0: Facial behavior analysis toolkit. In FG, 2018. 1,3",2
Challenges in Recognizing Spontaneous and Intentionally Expressed Reactions,CVPR_2020,1,"Tadas Baltru ˇsaitis, Amir Zadeh, Yao Chong Lim, and Louis- Philippe Morency. Openface 2.0: Facial behavior analysis toolkit. In 2018 18th IEEE International Conference on Au- tomatic Face Gesture Recognition , 2018.",4
Challenges in Recognizing Spontaneous and Intentionally Expressed Reactions,CVPR_2020,2,"Hamdi Dibeklioglu, Albert Salah, and T. Gevers. Recogni- tion of genuine smiles. IEEE Transactions on Multimedia , 17:279–294, 03 2015.",3
Challenges in Recognizing Spontaneous and Intentionally Expressed Reactions,CVPR_2020,3,"Andrew Ortony and Terrance J. Turner. What’s basic about emotions? Psychology Review , 97:315–331, 1990.",1
Challenges in Recognizing Spontaneous and Intentionally Expressed Reactions,CVPR_2020,4,"Jeffrey F Cohn, Zara Ambadar, and Paul Ekman. Observer- based measurement of facial expression with the facial action coding system. The handbook of emotion elicitation and as- sessment , 1(3):203–221, 2007.",3
Challenges in Recognizing Spontaneous and Intentionally Expressed Reactions,CVPR_2020,5,"Jeffrey Cohn, Lawrence Reed, Tsuyoshi Moriyama, Jing Xiao, Karen Schmidt, and Zara Ambadar. Multimodal coor- dination of facial action, head rotation, and eye motion dur- ing spontaneous smiles. pages 129–138, 01 2004.",6
Depth Sensing Beyond LiDAR Range,CVPR_2020,1,"Christian Beder and Richard Steffen. Determining an Initial Image Pair for Fixing the Scale of a 3D Reconstruction from an Image Sequence. In Joint Pattern Recognition Symposium , pages 657–666. Springer, 2006. 2",1
Depth Sensing Beyond LiDAR Range,CVPR_2020,2,"Richard I Hartley. Theory and Practice of Projective Rec- tiﬁcation. Int. J. of Computer Vision , 35(2):115–127, 1999. 2",1
Depth Sensing Beyond LiDAR Range,CVPR_2020,3,"Xinyu Huang, Jizhou Gao, and Ruigang Yang. Calibrating Pan-Tilt Cameras with Telephoto Lenses. In Proc. Asian Conf. on Computer Vision (ACCV) , pages 127–137. Springer, 2007. 2",3
Depth Sensing Beyond LiDAR Range,CVPR_2020,4,"Sunghoon Im, Hyowon Ha, Gyeongmin Choe, Hae-Gon Jeon, Kyungdon Joo, and In So Kweon. Accurate 3D Reconstruc- tion from Small Motion Clip for Rolling Shutter Cameras. Trans. Pattern Analysis and Machine Intelligence , 41:775– 787, 2019. 2[16] Sunghoon Im, Hyowon Ha, Gyeongmin Choe, Hae-Gon Jeon, Kyungdon Joo, and In So Kweon. High Quality Structure from Small Motion for Rolling Shutter Cameras. In Proc. Int. Conf. on Computer Vision (ICCV) , pages 837–845, 2015. 2",6
Depth Sensing Beyond LiDAR Range,CVPR_2020,5,"Richard Hartley and Andrew Zisserman. Multiple View Geom- etry in Computer Vision . Cambridge university press, 2003. 6",1
Image Demoireing with Learnable Bandpass Filters,CVPR_2020,1,"Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep convolutional network for image super-resolution. In ECCV , 2014 .2",4
Image Demoireing with Learnable Bandpass Filters,CVPR_2020,2,"Tianyu Gao, Yanqing Guo, Xin Zheng, Qianyu Wang, and Xiangyang Luo. Moir ´e pattern removal with multi-scale fea- ture enhancing network. In ICMEW, 2019 .1",5
Image Demoireing with Learnable Bandpass Filters,CVPR_2020,3,"Xi Cheng, Zhenyong Fu, and Jian Yang. Multi-scale dy- namic feature encoding network for image demoir ´eing. In ICCVW, 2019 .1,2",3
Image Demoireing with Learnable Bandpass Filters,CVPR_2020,4,"Chao Dong, Yubin Deng, Chen Change Loy, and Xiaoou Tang. Compression artifacts reduction by a deep convolu- tional network. In ICCV , 2015 .2 3643",4
Image Demoireing with Learnable Bandpass Filters,CVPR_2020,5,"Gabriel Eilertsen, Joel Kronander, Gyorgy Denes, Rafał K Mantiuk, and Jonas Unger. Hdr image reconstruction from a single exposure using deep cnns. TOG, 2017 .2",5
Effective Data Fusion with Generalized Vegetation Index:,CVPR_2020,1,"Barak Oshri, Annie Hu, Peter Adelson, Xiao Chen, Pasca- line Dupas, Jeremy Weinstein, Marshall Burke, David Lo- bell, and Stefano Ermon. Infrastructure quality assessment in africa using satellite imagery and deep learning. In Proceed- ings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining , pages 616–625, 2018. 1",9
Effective Data Fusion with Generalized Vegetation Index:,CVPR_2020,2,"Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classiﬁcation with convolutional neural networks. In Pro- ceedings of the IEEE conference on Computer Vision and Pattern Recognition , pages 1725–1732, 2014. 1",6
Effective Data Fusion with Generalized Vegetation Index:,CVPR_2020,3,"Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for seman- tic image segmentation. arXiv preprint arXiv:1706.05587 , 2017. 2,6",4
Effective Data Fusion with Generalized Vegetation Index:,CVPR_2020,4,"D Tanr ´e, C Deroo, P Duhaut, M Herman, JJ Morcrette, J Per- bos, and PY Deschamps. Technical note description of a computer code to simulate the satellite signal in the solar spectrum: the 5s code. International Journal of Remote Sensing , 11(4):659–668, 1990. 2",7
Effective Data Fusion with Generalized Vegetation Index:,CVPR_2020,5,"A Bannari, D Morin, F Bonn, and AR Huete. A review of vegetation indices. Remote sensing reviews , 13(1-2):95–120, 1995. 2",4
Reducing the feature divergence of RGB and near-infrared images using,CVPR_2020,1,"Ping Luo, Jiamin Ren, Zhanglin Peng, Ruimao Zhang, and Jingyu Li. Differentiable learning-to-normalize via switch- able normalization. In International Conference on Learning Representations , 2019. 4,5",5
Reducing the feature divergence of RGB and near-infrared images using,CVPR_2020,2,"Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In ECCV , 2018. 1,3,5",4
Reducing the feature divergence of RGB and near-infrared images using,CVPR_2020,3,"Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understand- ing of scenes through the ade20k dataset. arXiv preprint arXiv:1608.05442 , 2016. 1",6
Reducing the feature divergence of RGB and near-infrared images using,CVPR_2020,4,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In International Conference on Machine Learn- ing, pages 448–456, 2015. 2,3",1
Reducing the feature divergence of RGB and near-infrared images using,CVPR_2020,5,"Mang Tik Chiu, Xingqian Xu, Yunchao Wei, Zilong Huang, Alexander Schwing, Robert Brunner, Hrant Khachatrian, Hovnatan Karapetyan, Ivan Dozier, Greg Rose, et al. Agriculture-vision: A large aerial image database for agri- cultural pattern analysis. arXiv preprint arXiv:2001.01306 , 2020. 1,5",11
Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud,CVPR_2020,1,"Changmao Cheng, Yanwei Fu, Yu-Gang Jiang, Wei Liu, Wenlian Lu, Jianfeng Feng, and Xiangyang Xue. Dual skip- ping networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4071– 4079, 2018.",7
Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud,CVPR_2020,2,"Liang Du, Jingang Tan, Hongye Yang, Jianfeng Feng, Xi- angyang Xue, Qibao Zheng, Xiaoqing Ye, and Xiaolin Zhang. Ssf-dan: Separated semantic feature based domain adaptation network for semantic segmentation. In The IEEE International Conference on Computer Vision (ICCV) , Octo- ber 2019.",8
Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud,CVPR_2020,3,"Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition , pages 3354–3361. IEEE, 2012.",3
Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud,CVPR_2020,4,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in neural information pro- cessing systems , pages 91–99, 2015.",4
Associate-3Ddet: Perceptual-to-Conceptual Association for 3D Point Cloud,CVPR_2020,5,"Manzar Ashtari. Anatomy and functional role of the inferior longitudinal fasciculus: a search that has just begun. Devel- opmental Medicine & Child Neurology , 54(1):6–7, 2012.",2
Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting,CVPR_2020,1,"Liang Liao, Ruimin Hu, Jing Xiao, and Zhongyuan Wang. Edge-aware context encoder for image inpainting. In 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 3156–3160. IEEE, 2018.",4
Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting,CVPR_2020,2,"Coloma Ballester, Marcelo Bertalmio, Vicent Caselles, Guillermo Sapiro, and Joan Verdera. Filling-in by joint inter- polation of vector ﬁelds and gray levels. IEEE transactions on image processing , 10(8):1200–1211, 2001.",5
Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting,CVPR_2020,3,"Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A randomized correspon- dence algorithm for structural image editing. In ACM Trans- actions on Graphics (ToG) , volume 28, page 24. ACM, 2009.",4
Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting,CVPR_2020,4,"Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles, and Coloma Ballester. Image inpainting. In Proceedings of the 27th annual conference on Computer graphics and in- teractive techniques , pages 417–424. ACM Press/Addison- Wesley Publishing Co., 2000.",4
Contextual Residual Aggregation for Ultra High-Resolution Image Inpainting,CVPR_2020,5,"Peter Burt and Edward Adelson. The laplacian pyramid as a compact image code. IEEE Transactions on communica- tions , 31(4):532–540, 1983.",1
Learning to Structure an Image with Few Colors,CVPR_2020,1,"Zihao Liu, Xiaowei Xu, Tao Liu, Qi Liu, Yanzhi Wang, Yiyu Shi, Wujie Wen, Meiping Huang, Haiyun Yuan, and Jian Zhuang. Machine vision guided 3d medical image compres- sion for efﬁcient transmission and accurate segmentation in the clouds. arXiv preprint arXiv:1904.08487 , 2019.",10
Learning to Structure an Image with Few Colors,CVPR_2020,2,"Bowen Zhang, Limin Wang, Zhe Wang, Yu Qiao, and Hanli Wang. Real-time action recognition with enhanced motion vector cnns. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2718–2726, 2016.",5
Learning to Structure an Image with Few Colors,CVPR_2020,3,"Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine S ¨usstrunk. Slic superpix- els compared to state-of-the-art superpixel methods. IEEE transactions on pattern analysis and machine intelligence , 34(11):2274–2282, 2012.",6
Learning to Structure an Image with Few Colors,CVPR_2020,4,"Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and Luc V Gool. Soft-to-hard vector quantization for end-to-end learn- ing compressible representations. In Advances in Neural In- formation Processing Systems , pages 1141–1151, 2017.",7
Learning to Structure an Image with Few Colors,CVPR_2020,5,"Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Generative adversarial networks for extreme learned image compression. arXiv preprint arXiv:1804.02958 , 2018.",5
FroDO: From Detections to 3D Objects,CVPR_2020,1,"C. B. Choy, D. Xu, J. Gwak, K. Chen, and S. Savarese. 3d- r2n2: A uniﬁed approach for single and multi-view 3d object reconstruction. In European conference on computer vision , pages 628–644. Springer, 2016. 1,2,3,6",2
FroDO: From Detections to 3D Objects,CVPR_2020,2,"M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J. Davison. Codeslam—learning a compact, optimisable representation for dense visual slam. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 2560–2568, 2018. 3",2
FroDO: From Detections to 3D Objects,CVPR_2020,3,"R. Chabra, J. Straub, C. Sweeney, R. Newcombe, and H. Fuchs. Stereodrnet: Dilated residual stereonet. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 11786–11795, 2019. 1",2
FroDO: From Detections to 3D Objects,CVPR_2020,4,"A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li, S. Savarese, M. Savva, S. Song, H. Su, et al. Shapenet: An information-rich 3d model repository. arXiv preprint arXiv:1512.03012 , 2015. 2,4,5",2
FroDO: From Detections to 3D Objects,CVPR_2020,5,"S. Choi, Q.-Y . Zhou, S. Miller, and V . Koltun. A large dataset of object scans. arXiv:1602.02481 , 2016. 2,6,7",2
Learning to Segment 3D Point Clouds in 2D Image Space,CVPR_2020,1,"Hongyun Cai, Vincent W Zheng, and Kevin Chen-Chuan Chang. A comprehensive survey of graph embedding: Problems, techniques, and applications. IEEE Transactions on Knowledge and Data Engi- neering , 30(9):1616–1637, 2018. 3",3
Learning to Segment 3D Point Clouds in 2D Image Space,CVPR_2020,2,"Marek Chrobak and Thomas H Payne. A linear-time algorithm for drawing a planar graph on a grid. Information Processing Letters , 54(4):241–246, 1995. 2",1
Learning to Segment 3D Point Clouds in 2D Image Space,CVPR_2020,3,"Aric A. Hagberg, Daniel A. Schult, and Pieter J. Swart. Exploring network structure, dynamics, and function using networkx. In Gaël Varoquaux, Travis Vaught, and Jarrod Millman, editors, Proceedings of the 7th Python in Science Conference , pages 11 – 15, Pasadena, CA USA, 2008. 7",2
Learning to Segment 3D Point Clouds in 2D Image Space,CVPR_2020,4,"Binh-Son Hua, Minh-Khoi Tran, and Sai-Kit Yeung. Pointwise con- volutional neural networks. In CVPR , pages 984–993, 2018. 1,2, 3",3
Learning to Segment 3D Point Clouds in 2D Image Space,CVPR_2020,5,"Qiangui Huang, Weiyue Wang, and Ulrich Neumann. Recurrent slice networks for 3d segmentation of point clouds. In CVPR , pages 2626–2635, 2018. 1,3[19] Mathieu Jacomy, Tommaso Venturini, Sebastien Heymann, and Math- ieu Bastian. Forceatlas2, a continuous graph layout algorithm for handy network visualization designed for the gephi software. PloS one, 9(6):e98679, 2014. 2,3,5",3
Subpixel Dense Reﬁnement Network for Skeletonization,CVPR_2020,1,"N. Jiang, Y . Zhang, D. Luo, C. Liu, Y . Zhou, and Z. Han. Feature hourglass network for skeleton detection. In 2019 IEEE/CVF Conference on Computer Vision and Pat- tern Recognition Workshops (CVPRW) , pages 1172–1176, 2019.",2
Subpixel Dense Reﬁnement Network for Skeletonization,CVPR_2020,2,"S. Nathan and P. Kansal. Skeletonnet: Shape pixel to skele- ton pixel. In 2019 IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition Workshops (CVPRW) , pages 1181–1185, 2019.",2
Subpixel Dense Reﬁnement Network for Skeletonization,CVPR_2020,3,"W. Shi, J. Caballero, F. Husz ´ar, J. Totz, A. P. Aitken, R. Bishop, D. Rueckert, and Z. Wang. Real-time single im- age and video super-resolution using an efﬁcient sub-pixel convolutional neural network. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 1874–1883, 2016.",2
Subpixel Dense Reﬁnement Network for Skeletonization,CVPR_2020,4,"O. Ronneberger, P. Fischer, and T. Brox. U-net: Convolu- tional networks for biomedical image segmentation. In Med- ical Image Computing and Computer-Assisted Intervention – MICCAI 2015 , pages 234–241, 2015.",2
Subpixel Dense Reﬁnement Network for Skeletonization,CVPR_2020,5,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 770– 778, 2016.",2
RasterNet: Modeling Free-Flow Speed using LiDAR and Overhead Imagery,CVPR_2020,1,Kentucky Division of Geographic Information KyFromAbove. http://kyfromabove-kygeonet. opendata.arcgis.com/ .,2
RasterNet: Modeling Free-Flow Speed using LiDAR and Overhead Imagery,CVPR_2020,2,"Martin Weinmann, Boris Jutzi, and Cl ´ement Mallet. Seman- tic 3d scene interpretation: A framework combining optimal neighborhood size selection with relevant features. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences , 2(3):181, 2014.",3
RasterNet: Modeling Free-Flow Speed using LiDAR and Overhead Imagery,CVPR_2020,3,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Con- ference on Computer Vision and Pattern Recognition , 2016.",4
RasterNet: Modeling Free-Flow Speed using LiDAR and Overhead Imagery,CVPR_2020,4,"Mei Chen, Xu Zhang, and Eric R Green. Analysis of histor- ical travel time data. Technical report, Kentucky Transporta- tion Cabinet, 2015.",3
RasterNet: Modeling Free-Flow Speed using LiDAR and Overhead Imagery,CVPR_2020,5,"S Daneshtalab, H Rastiveis, and B Hosseiny. Cnn-based feature-level fusion of very high resolution aerial imagery and lidar data. International Archives of the Photogramme- try, Remote Sensing & Spatial Information Sciences , 2019.",3
Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems,CVPR_2020,1,"Luo, W., Schwing, A.G., Urtasun, R.: Efﬁcient deep learning for stereo matching. In: IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR). pp. 5695–5703 (2016)",6
Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems,CVPR_2020,2,"Alahari, K., Russell, C., Torr, P.H.S.: Efﬁcient piecewise learning for conditional random ﬁelds. In: Conference on Computer Vision and Pattern Recognition (2010)",6
Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems,CVPR_2020,3,"Bleyer, M., Gelautz, M.: Simple but effective tree structures for dynamic programming-based stereo matching. In: In VISAPP. pp. 415–422 (2008)",4
Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems,CVPR_2020,4,"Butler, D.J., Wulff, J., Stanley, G.B., Black, M.J.: A natu- ralistic open source movie for optical ﬂow evaluation. In: A. Fitzgibbon et al. (Eds.) (ed.) European Conference on Computer Vision (ECCV). pp. 611–625 (2012)",8
Belief Propagation Reloaded: Learning BP-Layers for Labeling Problems,CVPR_2020,5,"Chang, J.R., Chen, Y .S.: Pyramid stereo matching network. In: IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR). pp. 5410–5418 (2018)",4
A Certiﬁably Globally Optimal Solution to,CVPR_2020,1,"Uwe Helmke, Knut H ¨uper, Pei Yean Lee, and John Moore. Essential matrix estimation using Gauss-Newton iterations on a manifold. International Journal of Computer Vision , 74(2):117–136, 2007.",4
A Certiﬁably Globally Optimal Solution to,CVPR_2020,2,"Stephen Boyd and Lieven Vandenberghe. Convex Optimiza- tion. Cambridge University Press, 2004.",1
A Certiﬁably Globally Optimal Solution to,CVPR_2020,3,"Kurt Anstreicher and Henry Wolkowicz. On Lagrangian re- laxation of quadratic matrix constraints. SIAM Journal on Matrix Analysis and Applications , 22(1):41–55, 2000.",1
A Certiﬁably Globally Optimal Solution to,CVPR_2020,4,"Diego Cifuentes, Sameer Agarwal, Pablo A. Parrilo, and Rekha R. Thomas. On the local stability of semideﬁnite re- laxations. arXiv:1710.04287v2 , 2018.",3
A Certiﬁably Globally Optimal Solution to,CVPR_2020,5,"Martin Mevissen and Masakazu Kojima. SDP relaxations for quadratic optimization problems derived from polyno- mial optimization problems. Asia-Paciﬁc Journal of Oper- ational Research , 27(1):15–38, 2010.",1
MSeg: A Composite Dataset for Multi-domain Semantic Segmentation,CVPR_2020,1,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L. Yuille. Semantic image seg- mentation with deep convolutional nets and fully connected CRFs. In ICLR , 2015. 1",5
MSeg: A Composite Dataset for Multi-domain Semantic Segmentation,CVPR_2020,2,"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes dataset for semantic urban scene understanding. In CVPR , 2016. 1,2,3",9
MSeg: A Composite Dataset for Multi-domain Semantic Segmentation,CVPR_2020,3,"Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV , 2014. 1,2,3",3
MSeg: A Composite Dataset for Multi-domain Semantic Segmentation,CVPR_2020,4,"Petra Bevandi ´c, Ivan Kre ˇso, Marin Or ˇsi´c, and Sini ˇsaˇSegvi ´c. Simultaneous semantic segmentation and outlier detection in presence of domain shift. In Pattern Recognition , 2019. 2,7",4
MSeg: A Composite Dataset for Multi-domain Semantic Segmentation,CVPR_2020,5,"Hakan Bilen and Andrea Vedaldi. Universal representations: The missing link between faces, text, planktons, and cat breeds. arXiv:1701.07275 , 2017. 2",1
Domain Adaptation for Image Dehazing,CVPR_2020,1,"Yanyun Qu, Yizi Chen, Jingying Huang, and Yuan Xie. En- hanced pix2pix dehazing network. In IEEE Conference on Computer Vision and Pattern Recognition , 2019. 1,2,6,7,8",4
Domain Adaptation for Image Dehazing,CVPR_2020,2,"Amir Atapour-Abarghouei and Toby P Breckon. Real-time monocular depth estimation using synthetic data with do- main adaptation via image style transfer. In IEEE Confer- ence on Computer Vision and Pattern Recognition , 2018. 3",1
Domain Adaptation for Image Dehazing,CVPR_2020,3,"Dana Berman, Shai Avidan, et al. Non-local image dehaz- ing. In IEEE Conference on Computer Vision and Pattern Recognition , 2016. 2,6,7,8",3
Domain Adaptation for Image Dehazing,CVPR_2020,4,"Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel- level domain adaptation with generative adversarial network- s. In IEEE Conference on Computer Vision and Pattern Recognition , 2017. 3",5
Domain Adaptation for Image Dehazing,CVPR_2020,5,"Bolun Cai, Xiangmin Xu, Kui Jia, Qing Chunmei, and Dacheng Tao. Dehazenet: An end-to-end system for single image haze removal. IEEE Transactions on Image Process- ing, 2016. 1,2,6,7,8",5
VOLDOR: Visual Odometry from Log-logistic Dense Optical ﬂow Residuals,CVPR_2020,1,"P Fern ´andez Alcantarilla. Vision based localization: from humanoid robots to visually impaired people. Electronics (University of Alcala, 2011) , 2011. 7",2
VOLDOR: Visual Odometry from Log-logistic Dense Optical ﬂow Residuals,CVPR_2020,2,"Pablo F Alcantarilla, Jos ´e J Yebes, Javier Almaz ´an, and Luis M Bergasa. On combining visual slam and dense scene ﬂow to increase the robustness of localization and mapping in dynamic environments. In 2012 IEEE Inter- national Conference on Robotics and Automation , pages 1290–1297. IEEE, 2012. 7",4
VOLDOR: Visual Odometry from Log-logistic Dense Optical ﬂow Residuals,CVPR_2020,3,"G Jogesh Babu and Calyampudi R Rao. Goodness-of-ﬁt tests when parameters are estimated. Sankhya , 66(1):63– 74, 2004. 4",1
VOLDOR: Visual Odometry from Log-logistic Dense Optical ﬂow Residuals,CVPR_2020,4,"Jose-Luis Blanco. A tutorial on se (3) transformation pa- rameterizations and on-manifold optimization. University of Malaga, Tech. Rep , 3, 2010. 6",2
VOLDOR: Visual Odometry from Log-logistic Dense Optical ﬂow Residuals,CVPR_2020,5,"Michael Bloesch, Jan Czarnowski, Ronald Clark, Stefan Leutenegger, and Andrew J Davison. Codeslamlearning a compact, optimisable representation for dense visual slam. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2560–2568, 2018. 2",5
VIBE: Video Inference for Human Body Pose and Shape Estimation,CVPR_2020,1,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In European Conference on Computer Vision , 2016. 5",4
VIBE: Video Inference for Human Body Pose and Shape Estimation,CVPR_2020,2,"Liang-Yan Gui, Yu-Xiong Wang, Xiaodan Liang, and Jose M. F. Moura. Adversarial geometry-aware human motion prediction. In European Conference on Computer Vision , 2018. 3",4
VIBE: Video Inference for Human Body Pose and Shape Estimation,CVPR_2020,3,"Partha Ghosh, Mehdi S. M. Sajjadi, Antonio Vergari, Michael Black, and Bernhard Sch ¨olkopf. From variational to deterministic autoencoders. In International Conference on Learning Representations , 2020. 7",2
VIBE: Video Inference for Human Body Pose and Shape Estimation,CVPR_2020,4,"Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bern- hard Scholkopf. Wasserstein auto-encoders. In International Conference on Learning Representations , 2018. 7",4
VIBE: Video Inference for Human Body Pose and Shape Estimation,CVPR_2020,5,"Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural networks. In Advances in neural information processing systems , 2014. 3",3
Feudal Steering: Hierarchical Learning for Steering Angle Prediction,CVPR_2020,1,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. 5",4
Feudal Steering: Hierarchical Learning for Steering Angle Prediction,CVPR_2020,2,"V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602 , 2013. 1",7
Feudal Steering: Hierarchical Learning for Steering Angle Prediction,CVPR_2020,3,"Yuhang Song, Jianyi Wang, Thomas Lukasiewicz, Zhenghua Xu, and Mai Xu. Diversity-driven extensible hierarchical reinforcement learning. In Proceedings of the AAAI Confer- ence on Artiﬁcial Intelligence , volume 33, pages 4992–4999, 2019. 2",5
Feudal Steering: Hierarchical Learning for Steering Angle Prediction,CVPR_2020,4,"Pierre-Luc Bacon, Jean Harb, and Doina Precup. The option- critic architecture. In Thirty-First AAAI Conference on Arti- ﬁcial Intelligence , 2017. 2",3
Feudal Steering: Hierarchical Learning for Steering Angle Prediction,CVPR_2020,5,"Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316 , 2016. 3",11
Cross-Domain Document Object Detection: Benchmark Suite and Method,CVPR_2020,1,"Jing Fang, Xin Tao, Zhi Tang, Ruiheng Qiu, and Ying Liu. Dataset, ground-truth and performance metrics for table de- tection evaluation. In DAS, 2012.",5
Cross-Domain Document Object Detection: Benchmark Suite and Method,CVPR_2020,2,"Azka Gilani, Shah Rukh Qasim, Imran Malik, and Faisal Shafait. Table detection using deep learning. In ICDAR , 2017.",4
Cross-Domain Document Object Detection: Benchmark Suite and Method,CVPR_2020,3,"Max G ¨obel, Tamir Hassan, Ermelinda Oro, and Giorgio Orsi. Icdar 2013 table competition. In ICDAR , 2013.",4
Cross-Domain Document Object Detection: Benchmark Suite and Method,CVPR_2020,4,"Zhenwei He and Lei Zhang. Multi-adversarial faster-rcnn for unrestricted object detection. In ICCV , 2019.",1
Cross-Domain Document Object Detection: Benchmark Suite and Method,CVPR_2020,5,"Mehran Khodabandeh, Arash Vahdat, Mani Ranjbar, and William G Macready. A robust learning approach to domain adaptive object detection. In ICCV , 2019.[18] Seunghyeon Kim, Jaehoon Choi, Taekyung Kim, and Chang- ick Kim. Self-training and adversarial background regular- ization for unsupervised domain adaptive one-stage object detection. In ICCV , 2019.",4
Hierarchical Human Parsing with Typed Part-Relation Reasoning,CVPR_2020,1,"Hao-Shu Fang, Guansong Lu, Xiaolin Fang, Jianwen Xie, Yu-Wing Tai, and Cewu Lu. Weakly and semi supervised human body part parsing via pose-guided knowledge trans- fer. In CVPR , 2018. 1,2,7",6
Hierarchical Human Parsing with Typed Part-Relation Reasoning,CVPR_2020,2,"Lubor Ladicky, Philip HS Torr, and Andrew Zisserman. Hu- man pose estimation using a joint pixel-wise and part-wise formulation. In CVPR , 2013. 2",3
Hierarchical Human Parsing with Typed Part-Relation Reasoning,CVPR_2020,3,"Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE TPAMI , 39(12):2481–2495, 2017. 6,7",3
Hierarchical Human Parsing with Typed Part-Relation Reasoning,CVPR_2020,4,"Yihang Bo and Charless C Fowlkes. Shape-based pedestrian parsing. In CVPR , 2011. 2",1
Hierarchical Human Parsing with Typed Part-Relation Reasoning,CVPR_2020,5,"Hong Chen, Zi Jian Xu, Zi Qiang Liu, and Song Chun Zhu. Composite templates for cloth modeling and sketching. In CVPR , 2006. 2",4
Semantic Drift Compensation for Class-Incremental Learning,CVPR_2020,1,"Z. Li and D. Hoiem. Learning without forgetting. IEEE Trans. on PAMI , 40(12):2935–2947, 2018. 1,2,4,6",2
Semantic Drift Compensation for Class-Incremental Learning,CVPR_2020,2,"R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV , pages 139–154, 2018. 2,5,6",2
Semantic Drift Compensation for Class-Incremental Learning,CVPR_2020,3,"R. Aljundi, P. Chakravarty, and T. Tuytelaars. Expert gate: Lifelong learning with a network of experts. In CVPR , pages 3366–3375, 2017. 2,4,6,8",2
Semantic Drift Compensation for Class-Incremental Learning,CVPR_2020,4,"E. Belouadah and A. Popescu. Il2m: Class incremental learning with dual memory. In Proceedings of the IEEE In- ternational Conference on Computer Vision , pages 583–592, 2019. 2",2
Semantic Drift Compensation for Class-Incremental Learning,CVPR_2020,5,"J. Bromley, I. Guyon, Y . LeCun, E. S ¨ackinger, and R. Shah. Signature veriﬁcation using a ”siamese” time delay neural network. In NIPS , pages 737–744, 1994. 3",2
Detecting Video Speed Manipulation,CVPR_2020,1,"Yuezun Li, Ming-Ching Chang, and Siwei Lyu. In ictu oculi: Exposing ai created fake videos by detecting eye blinking. In 2018 IEEE International Workshop on Information Forensics and Security (WIFS) , pages 1–7, 2018. 1",3
Detecting Video Speed Manipulation,CVPR_2020,2,"The deepfake detection challenge (dfdc), 2019. 5",2
Detecting Video Speed Manipulation,CVPR_2020,3,"Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. Mesonet: a compact facial video forgery detection network. In 2018 IEEE International Workshop on Informa- tion Forensics and Security (WIFS) , pages 1–7, 2018. 1",4
Detecting Video Speed Manipulation,CVPR_2020,4,"Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser Sheikh. Recycle-gan: Unsupervised video retargeting. In The European Conference on Computer Vision (ECCV) , September 2018. 1",4
Detecting Video Speed Manipulation,CVPR_2020,5,"Paolo Bestagini, Marco Fontani, Simone Milani, Mauro Barni, Alessandro Piva, Marco Tagliasacchi, and Stefano Tubaro. An overview on video forensics. In 2012 Pro- ceedings of the 20th European Signal Processing Conference (EUSIPCO) , pages 1229–1233, 2012. 1,2",7
Uniﬁed Dynamic Convolutional Network for Super-Resolution with,CVPR_2020,1,"C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunning- ham, A. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, and W. Shi, “Photo-realistic single image super-resolution using a generative adversarial network,” in Proc. IEEE Conf. Com- puter Vision and Pattern Recognition (CVPR) , pp. 105-114, 2017. 1",2
Uniﬁed Dynamic Convolutional Network for Super-Resolution with,CVPR_2020,2,"J. Gu, H. Lu, W. Zuo, and C. Dong, “Blind super-resolution with iterative kernel correction,” in Proc. IEEE Conf. Com- puter Vision and Pattern Recognition (CVPR) , 2019. 1,2,3, 5,7",2
Uniﬁed Dynamic Convolutional Network for Super-Resolution with,CVPR_2020,3,"K. Zhang, W. Zuo, and L. Zhang, “Learning a single convo- lutional super-resolution network for multiple degradations,” inProc. IEEE Conf. Computer Vision and Pattern Recogni- tion (CVPR) , pp. 3262-3271, 2018. 1,2,3,5,6,7,8",2
Uniﬁed Dynamic Convolutional Network for Super-Resolution with,CVPR_2020,4,"C. Dong, C. C. Loy, K. He, and X. Tang, “Image super- resolution using deep convolutional networks,” IEEE Trans. Pattern Analysis and Machine Intelligence (TPAMI) , vol. 38, pp. 295-307, 2015. 1,2,8",2
Uniﬁed Dynamic Convolutional Network for Super-Resolution with,CVPR_2020,5,"J. Kim, J. K. Lee, and K. M. Lee, “Accurate image super-resolution using very deep convolutional networks,” in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) , pp. 1646-1654, 2016. 1,2,8",2
CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus,CVPR_2020,1,"Daniel Barath, Jiri Matas, and Levente Hajder. Multi-H: Efﬁ- cient recovery of tangent planes in stereo images. In BMVC , 2016. 2",3
CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus,CVPR_2020,2,"Gilles Simon, Antoine Fond, and Marie-Odile Berger. A-contrario horizon-ﬁrst vanishing point detection using second-order grouping laws. In ECCV , 2018. 3,7",3
CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus,CVPR_2020,3,"Eric Brachmann and Carsten Rother. Neural-guided RANSAC: Learning where to sample model hypotheses. In ICCV , 2019. 2,3,4,6,7,8",1
CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus,CVPR_2020,4,"Hossam Isack and Yuri Boykov. Energy-based geometric multi-model ﬁtting. IJCV , 2012. 2,8",1
CONSAC: Robust Multi-Model Fitting by Conditional Sample Consensus,CVPR_2020,5,"Ondrej Chum and Jiri Matas. Matching with PROSAC- progressive sample consensus. In CVPR , 2005. 3",1
Unifying Training and Inference for Panoptic Segmentation,CVPR_2020,1,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. 7",4
Unifying Training and Inference for Panoptic Segmentation,CVPR_2020,2,"Daan de Geus, Panagiotis Meletis, and Gijs Dubbelman. Panoptic segmentation with a joint semantic and instance segmentation network. arXiv preprint arXiv:1809.02110 , 2018. 7,8",3
Unifying Training and Inference for Panoptic Segmentation,CVPR_2020,3,"Naiyu Gao, Yanhu Shan, Yupei Wang, Xin Zhao, Yinan Yu, Ming Yang, and Kaiqi Huang. Ssap: Single-shot instance segmentation with afﬁnity pyramid. In Proceedings of the IEEE International Conference on Computer Vision , pages 642–651, 2019. 7,8",7
Unifying Training and Inference for Panoptic Segmentation,CVPR_2020,4,"Yuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin Yumer, and Raquel Urtasun. Upsnet: A uniﬁed panoptic segmentation network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8818–8826, 2019. 1,2,3,4,6,7,8",7
Unifying Training and Inference for Panoptic Segmentation,CVPR_2020,5,"Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Dollár. Panoptic feature pyramid networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6399–6408, 2019. 1,2,3,6,7,8",4
Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects,CVPR_2020,1,"Jan Kotera and Filip ˇSroubek. Motion estimation and deblur- ring of fast moving objects. In 2018 25th IEEE International Conference on Image Processing (ICIP) , pages 2860–2864, Oct 2018.",1
Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects,CVPR_2020,2,"Stephen Boyd, Neal Parikh, Eric Chu, Borja Peleato, and Jonathan Eckstein. Distributed optimization and statistical learning via the alternating direction method of multipliers. Found. Trends Mach. Learn. , 3(1):1–122, Jan. 2011.",5
Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects,CVPR_2020,3,"Richard Hartley and Andrew Zisserman. Multiple View Ge- ometry in Computer Vision . Cambridge University Press, New York, NY , USA, 2 edition, 2003.",1
Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects,CVPR_2020,4,"Michal Jancosek and Tomas Pajdla. Multi-view reconstruc- tion preserving weakly-supported surfaces. In CVPR 2011 , pages 3121–3128, June 2011.",1
Sub-frame Appearance and 6D Pose Estimation of Fast Moving Objects,CVPR_2020,5,"Meiguang Jin et al. Learning to extract a video sequence from a single motion-blurred image. In IEEE CVPR , pages 6334–6342, June 2018.",2
Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and,CVPR_2020,1,"Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. arXiv preprint arXiv:1802.07687 , 2018. 3,6",1
Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and,CVPR_2020,2,"Chelsea Finn and Sergey Levine. Deep visual foresight for planning robot motion. In ICRA , 2017. 1",1
Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and,CVPR_2020,3,"Ruben Villegas, Jimei Yang, Seunghoon Hong, Xunyu Lin, and Honglak Lee. Decomposing motion and con- tent for natural video sequence prediction. arXiv preprint arXiv:1706.08033 , 2017. 1,3,5",5
Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and,CVPR_2020,4,"Milad Alemohammad, Jasper R Stroud, Bryan T Bosworth, and Mark A Foster. High-speed all-optical haar wavelet transform for real-time image compression. Optics Express , 2017. 2",4
Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and,CVPR_2020,5,"Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine. Stochastic variational video prediction. arXiv preprint arXiv:1710.11252 , 2017. 3, 5,6",5
Frequency Domain Compact 3D Convolutional Neural Networks,CVPR_2020,1,"Emily L Denton, Wojciech Zaremba, Joan Bruna, Yann Le- Cun, and Rob Fergus. Exploiting linear structure within con- volutional networks for efﬁcient evaluation. In NIPS , 2014. 2",5
Frequency Domain Compact 3D Convolutional Neural Networks,CVPR_2020,2,"Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman. Convolutional two-stream network fusion for video action recognition. In CVPR , pages 1933–1941, 2016. 2",3
Frequency Domain Compact 3D Convolutional Neural Networks,CVPR_2020,3,"Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and im- agenet? In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition , pages 6546–6555, 2018. 1, 2,6,7",3
Frequency Domain Compact 3D Convolutional Neural Networks,CVPR_2020,4,"Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for efﬁcient convnets. arXiv preprint arXiv:1608.08710 , 2016. 2,5,6",5
Frequency Domain Compact 3D Convolutional Neural Networks,CVPR_2020,5,"Yuxin Zhang, Huan Wang, Yang Luo, Lu Yu, Haoji Hu, Hangguan Shan, and Tony QS Quek. Three-dimensional convolutional neural network pruning with regularization- based method. In 2019 IEEE International Conference on Image Processing (ICIP) , pages 4270–4274. IEEE, 2019. 3, 5,6 1650",7
Softmax Splatting for Video Frame Interpolation,CVPR_2020,1,"Wenbo Bao, Wei-Sheng Lai, Chao Ma, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. Depth-Aware Video Frame Interpolation. In IEEE Conference on Computer Vi- sion and Pattern Recognition , 2019. 1,2,3,4,6,8",6
Softmax Splatting for Video Frame Interpolation,CVPR_2020,2,"Simon Baker, Daniel Scharstein, J. P. Lewis, Stefan Roth, Michael J. Black, and Richard Szeliski. A Database and Evaluation Methodology for Optical Flow. International Journal of Computer Vision , 92(1):1–31, 2011. 1,4,6,7, 8",3
Softmax Splatting for Video Frame Interpolation,CVPR_2020,3,"Wenbo Bao, Wei-Sheng Lai, Xiaoyun Zhang, Zhiyong Gao, and Ming-Hsuan Yang. MEMC-Net: Motion Estimation and Motion Compensation Driven Neural Network for Video In- terpolation and Enhancement. arXiv/1810.08768 , 2018. 2",5
Softmax Splatting for Video Frame Interpolation,CVPR_2020,4,"Jose Caballero, Christian Ledig, Andrew P. Aitken, Alejan- dro Acosta, Johannes Totz, Zehan Wang, and Wenzhe Shi. Real-Time Video Super-Resolution With Spatio-Temporal Networks and Motion Compensation. In IEEE Conference on Computer Vision and Pattern Recognition , 2017. 2",3
Softmax Splatting for Video Frame Interpolation,CVPR_2020,5,"Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip H¨ausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. FlowNet: Learn- ing Optical Flow With Convolutional Networks. In IEEE International Conference on Computer Vision , 2015. 7",9
Single Image Reﬂection Removal with Physically-Based Training Images,CVPR_2020,1,"Nikolaos Arvanitopoulos, Radhakrishna Achanta, and Sabine Susstrunk. Single image reﬂection suppression. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4498–4506, 2017.",3
Single Image Reﬂection Removal with Physically-Based Training Images,CVPR_2020,2,"William Donnelly. Per-pixel displacement mapping with dis- tance functions. GPU gems , 2(22):3, 2005.",2
Single Image Reﬂection Removal with Physically-Based Training Images,CVPR_2020,3,"Wenzel Jakob. Mitsuba renderer, 2010. http://www.mitsuba- renderer.org.",2
Single Image Reﬂection Removal with Physically-Based Training Images,CVPR_2020,4,"Anat Levin and Yair Weiss. User assisted separation of re- ﬂections from a single image using a sparsity prior. IEEE Transactions on Pattern Analysis and Machine Intelligence , 29(9):1647–1654, 2007.",1
Single Image Reﬂection Removal with Physically-Based Training Images,CVPR_2020,5,"Hao Su, Charles R. Qi, Yangyan Li, and Leonidas J. Guibas. Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views. In The IEEE Inter- national Conference on Computer Vision (ICCV) , December 2015.",2
Learning Visual Emotion Representations from Web Data,CVPR_2020,1,"Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. OpenPose: realtime multi-person 2D pose es- timation using Part Afﬁnity Fields. In arXiv:1812.08008 , 2018. 3",5
Learning Visual Emotion Representations from Web Data,CVPR_2020,2,"Ali Mollahosseini, Behzad Hasani, and Mohammad H Ma- hoor. Affectnet: A database for facial expression, valence, and arousal computing in the wild. arXiv:1708.03985 , 2017. 2,3,6",3
Learning Visual Emotion Representations from Web Data,CVPR_2020,3,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016. 1,5",4
Learning Visual Emotion Representations from Web Data,CVPR_2020,4,"Sanjeev Arora, Yingyu Liang, and Tengyu Ma. A simple but tough-to-beat baseline for sentence embeddings. In Proceed- ings of International Conference on Learning and Represen- tation , 2017. 4",3
Learning Visual Emotion Representations from Web Data,CVPR_2020,5,"Pooyan Balouchian, Marjaneh Safaei, and Hassan Foroosh. LUCFER: A large-scale context-sensitive image dataset for deep learning of visual emotions. In IEEE Winter Conference on Applications of Computer Vision , 2019. 2",3
Ensemble Generative Cleaning with Feedback Loops for Defending Adversarial,CVPR_2020,1,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572 , 2014.",3
Ensemble Generative Cleaning with Feedback Loops for Defending Adversarial,CVPR_2020,2,"Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practi- cal black-box attacks against machine learning. In Proceed- ings of the 2017 ACM on Asia Conference on Computer and Communications Security , pages 506–519. ACM, 2017.",6
Ensemble Generative Cleaning with Feedback Loops for Defending Adversarial,CVPR_2020,3,"Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami. Distillation as a defense to adver- sarial perturbations against deep neural networks. In 2016 IEEE Symposium on Security and Privacy (SP) , pages 582– 597. IEEE, 2016.",5
Ensemble Generative Cleaning with Feedback Loops for Defending Adversarial,CVPR_2020,4,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016.",4
Ensemble Generative Cleaning with Feedback Loops for Defending Adversarial,CVPR_2020,5,"Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. Adversarial example defense: Ensembles of weak defenses are not strong. In 11th{USENIX}Workshop on Offensive Technologies ( {WOOT}17), 2017.",5
A Simple Discriminative Dual Semantic Auto-encoder for Zero-shot,CVPR_2020,1,"Yashas Annadani and Soma Biswas. Preserving semantic relations for zero-shot learning. In CVPR , pages 7603–7612, 2018.",1
A Simple Discriminative Dual Semantic Auto-encoder for Zero-shot,CVPR_2020,2,"Richard H. Bartels and George W Stewart. Solution of the matrix equation ax+ xb= c [f4]. Communications of the ACM , 15(9):820–826, 1972.",2
A Simple Discriminative Dual Semantic Auto-encoder for Zero-shot,CVPR_2020,3,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR , pages 770–778, 2016.",4
A Simple Discriminative Dual Semantic Auto-encoder for Zero-shot,CVPR_2020,4,"Bernardino Romera-Paredes and Philip Torr. An embarrass- ingly simple approach to zero-shot learning. In ICML , pages 2152–2161, 2015.",1
A Simple Discriminative Dual Semantic Auto-encoder for Zero-shot,CVPR_2020,5,"Yongqin Xian, Tobias Lorenz, Bernt Schiele, and Zeynep Akata. Feature generating networks for zero-shot learning. InCVPR , pages 5542–5551, 2018.",4
Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning,CVPR_2020,1,"Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6299–6308, 2017. 5",1
Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning,CVPR_2020,2,"Xiaojun Chang, Yi Yang, Alexander Hauptmann, Eric P Xing, and Yao-Liang Yu. Semantic concept discovery for large-scale zero-shot event detection. In Proceedings of the Twenty-fourth International Joint Conference on Artiﬁcial Intelligence , 2015. 1",5
Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning,CVPR_2020,3,"Zhenfang Chen, Lin Ma, Wenhan Luo, and Kwan-Yee Ken- neth Wong. Weakly-supervised spatio-temporally grounding natural sentence in video. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , pages 1884–1894, 2019. 2",4
Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning,CVPR_2020,4,"Jeffrey Dalton, James Allan, and Pranav Mirajkar. Zero-shot video retrieval using content and concepts. In Proceedings of the 22nd ACM international conference on Information & Knowledge Management , pages 1857–1860. ACM, 2013. 1",3
Fine-grained Video-Text Retrieval with Hierarchical Graph Reasoning,CVPR_2020,5,"Jianfeng Dong, Xirong Li, and Cees GM Snoek. Predict- ing visual features from text for image and video caption retrieval. IEEE Transactions on Multimedia , 20(12):3377– 3388, 2018. 5",3
DSGN: Deep Stereo Geometry Network for 3D Object Detection,CVPR_2020,1,"Mengqi Ji, Juergen Gall, Haitian Zheng, Yebin Liu, and Lu Fang. Surfacenet: An end-to-end 3d neural network for mul- tiview stereopsis. In ICCV , pages 2307–2315, 2017.",5
DSGN: Deep Stereo Geometry Network for 3D Object Detection,CVPR_2020,2,"Xinzhu Ma, Zhihui Wang, Haojie Li, Wanli Ouyang, and Pengbo Zhang. Accurate monocular 3d object detection via color-embedded 3d reconstruction for autonomous driving. 2019.",5
DSGN: Deep Stereo Geometry Network for 3D Object Detection,CVPR_2020,3,"Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical ﬂow, and scene ﬂow estimation. In CVPR , pages 4040–4048, 2016.",7
DSGN: Deep Stereo Geometry Network for 3D Object Detection,CVPR_2020,4,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR , pages 770–778, 2016.",4
DSGN: Deep Stereo Geometry Network for 3D Object Detection,CVPR_2020,5,"Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin Huang. Deepmvs: Learning multi-view stereopsis. In CVPR , pages 2821–2830, 2018.",5
Collaborative Distillation for Ultra-Resolution Universal Style Transfer,CVPR_2020,1,"Song Han, Jeff Pool, John Tran, and William J Dally. Learn- ing both weights and connections for efﬁcient neural net- work. In NeurIPS , 2015. 2,3",4
Collaborative Distillation for Ultra-Resolution Universal Style Transfer,CVPR_2020,2,"Jimmy Ba and Rich Caruana. Do deep nets really need to be deep? In NeurIPS , 2014. 2,3",1
Collaborative Distillation for Ultra-Resolution Universal Style Transfer,CVPR_2020,3,"Cristian Bucilu ˇa, Rich Caruana, and Alexandru Niculescu- Mizil. Model compression. In SIGKDD , 2006. 2,3",3
Collaborative Distillation for Ultra-Resolution Universal Style Transfer,CVPR_2020,4,"Dongdong Chen, Lu Yuan, Jing Liao, Nenghai Yu, and Gang Hua. Stylebank: An explicit representation for neural image style transfer. In CVPR , 2017. 3",5
Collaborative Distillation for Ultra-Resolution Universal Style Transfer,CVPR_2020,5,"Tian Qi Chen and Mark Schmidt. Fast patch-based style transfer of arbitrary style. arXiv preprint arXiv:1612.04337 , 2016. 2,3",1
ENSEI: Efﬁcient Secure Inference via Frequency-Domain,CVPR_2020,1,"Junfeng Fan and Frederik Vercauteren. Somewhat practi- cal fully homomorphic encryption. IACR Cryptology ePrint Archive , 2012:144, 2012.",1
ENSEI: Efﬁcient Secure Inference via Frequency-Domain,CVPR_2020,2,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural net- works. In Advances in neural information processing sys- tems, pages 1097–1105, 2012.",3
ENSEI: Efﬁcient Secure Inference via Frequency-Domain,CVPR_2020,3,"Qing Lu, Weiwen Jiang, Xiaowei Xu, Yiyu Shi, and Jingtong Hu. On neural architecture search for resource-constrained hardware platforms. arXiv preprint arXiv:1911.00105 , 2019.",5
ENSEI: Efﬁcient Secure Inference via Frequency-Domain,CVPR_2020,4,"Nigel P Smart and Frederik Vercauteren. Fully homomorphic encryption with relatively small key and ciphertext sizes. In International Workshop on Public Key Cryptography , pages 420–443. Springer, 2010.",1
ENSEI: Efﬁcient Secure Inference via Frequency-Domain,CVPR_2020,5,"Zhenyu Wu, Zhangyang Wang, Zhaowen Wang, and Hailin Jin. Towards privacy-preserving visual recognition via ad- versarial training: A pilot study. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV) , pages 606– 624, 2018.",4
VoronoiNet,CVPR_2020,1,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- bastian Nowozin, and Andreas Geiger. Occupancy net- works: Learning 3d reconstruction in function space. arXiv:1812.03828 , 2018. 1,2,3,4",5
VoronoiNet,CVPR_2020,2,"Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classiﬁcation and segmentation. In Proc. of Comp. Vision and Pattern Recognition (CVPR) , 2017. 1",4
VoronoiNet,CVPR_2020,3,"Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. CoRR , abs/1706.02413, 2017. 1",4
VoronoiNet,CVPR_2020,4,"Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning con- tinuous signed distance functions for shape representation. Proc. of Comp. Vision and Pattern Recognition (CVPR) , 2019. 1,2,3,4",5
VoronoiNet,CVPR_2020,5,"Narendra Ahuja, Byong An, and Bruce Schachter. Image representation using voronoi tessellation. Computer Vision, Graphics, and Image Processing , 29(3):286 – 295, 1985. 1",3
Learning to Discriminate Information for Online Action Detection,CVPR_2020,1,"R. De Geest, E. Gavves, A. Ghodrati, Z. Li, G. Snoek, and T. Tuytelaars. Online action detection. In Proc. European Con- ference on Computer Vision (ECCV) , pages 269–285, Oct. 2016.",2
Learning to Discriminate Information for Online Action Detection,CVPR_2020,2,"R. De Geest and T. Tuytelaars. Modeling temporal structure with lstm for online action detection. In Proc. IEEE Win- ter Conference on Applications of Computer Vision (WACV) , pages 1549–1557, Mar. 2018.",2
Learning to Discriminate Information for Online Action Detection,CVPR_2020,3,"K. Simonyan and A. Zisserman. Two-stream convolutional networks for action recognition in videos. In Proc. Confer- ence on Neural Information Processing Systems (NeurIPS) , pages 568–576, Dec. 2014.",2
Learning to Discriminate Information for Online Action Detection,CVPR_2020,4,"Y . Cai, H. Li, J.-F. Hu, and W.-S. Zheng. Action knowl- edge transfer for action prediction with partial videos. In Proc. Association for the Advancement of Artiﬁcial Intel- ligence (AAAI) Conference on Artiﬁcial Intelligence , pages 8118–8125, Jan. 2019.",2
Learning to Discriminate Information for Online Action Detection,CVPR_2020,5,"J. Carreira and A. Zisserman. Quo vaids, action recognition? a new model and the kinectics dataset. In Proc. IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR) , pages 4724–4733, Jul. 2017.",2
When2com: Multi-Agent Perception via Communication Graph Grouping,CVPR_2020,1,"Jiechuan Jiang, Chen Dun, and Zongqing Lu. Graph convo- lutional reinforcement learning for multi-agent cooperation. arXiv preprint arXiv:1810.09202 , 2018. 2",3
When2com: Multi-Agent Perception via Communication Graph Grouping,CVPR_2020,2,"Dzmitry Bahdanau, KyungHyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translation. In Proceedings of the International Conference on Learning Representations (ICLR) , 2015. 3",3
When2com: Multi-Agent Perception via Communication Graph Grouping,CVPR_2020,3,"Peter Battaglia, Razvan Pascanu, Matthew Lai, Danilo Jimenez Rezende, et al. Interaction networks for learning about objects, relations and physics. In Ad- vances in Neural Information Processing Systems (NIPS) , 2016. 2",5
When2com: Multi-Agent Perception via Communication Graph Grouping,CVPR_2020,4,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European Conference on Computer Vi- sion (ECCV) , pages 801–818, 2018. 1",5
When2com: Multi-Agent Perception via Communication Graph Grouping,CVPR_2020,5,"Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short- term memory-networks for machine reading. In Confer- ence on Empirical Methods in Natural Language Processing (EMNLP) , 2016. 4",3
Video Instance Segmentation Tracking with a Modiﬁed V AE Architecture,CVPR_2020,1,"Ignas Budvytis, Vijay Badrinarayanan, and Roberto Cipolla. Mot-mixture of trees probabilistic graphical model for video segmentation. In BMVC , volume 1, page 7. Citeseer, 2012. 3",3
Video Instance Segmentation Tracking with a Modiﬁed V AE Architecture,CVPR_2020,2,"Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taix ´e, Daniel Cremers, and Luc Van Gool. One- shot video object segmentation. In CVPR 2017 . IEEE, 2017. 2,3",6
Video Instance Segmentation Tracking with a Modiﬁed V AE Architecture,CVPR_2020,3,"Anton Andriyenko, Konrad Schindler, and Stefan Roth. Discrete-continuous optimization for multi-target tracking. InComputer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on , pages 1926–1933. IEEE, 2012. 1,2",3
Video Instance Segmentation Tracking with a Modiﬁed V AE Architecture,CVPR_2020,4,"S Avinash Ramakanth and R Venkatesh Babu. Seamseg: Video object segmentation using patch seams. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 376–383, 2014. 3",1
Video Instance Segmentation Tracking with a Modiﬁed V AE Architecture,CVPR_2020,5,"Vijay Badrinarayanan, Ignas Budvytis, and Roberto Cipolla. Semi-supervised video segmentation using tree structured graphical models. IEEE transactions on pattern analysis and machine intelligence , 35(11):2751–2764, 2013. 3",3
Learning Longterm Representations for Person Re-Identiﬁcation,CVPR_2020,1,"Fadel Adib, Zach Kabelac, Dina Katabi, and Robert C Miller. 3d tracking via body radio reﬂections. In 11th{USENIX} Symposium on Networked Systems Design and Implementa- tion ({NSDI}14), pages 317–329, 2014. 3",4
Learning Longterm Representations for Person Re-Identiﬁcation,CVPR_2020,2,"Fadel Adib, Hongzi Mao, Zachary Kabelac, Dina Katabi, and Robert C Miller. Smart homes that monitor breathing and heart rate. In Proceedings of the 33rd annual ACM conference on human factors in computing systems , pages 837–846. ACM, 2015. 3",5
Learning Longterm Representations for Person Re-Identiﬁcation,CVPR_2020,3,"Ejaz Ahmed, Michael Jones, and Tim K Marks. An improved deep learning architecture for person re-identiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3908–3916, 2015. 1, 3",3
Learning Longterm Representations for Person Re-Identiﬁcation,CVPR_2020,4,"Petr Beckmann and Andre Spizzichino. The scattering of electromagnetic waves from rough surfaces. Norwood, MA, Artech House, Inc., 1987, 511 p. , 1987. 2, 3",1
Learning Longterm Representations for Person Re-Identiﬁcation,CVPR_2020,5,"Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6299–6308, 2017. 3",1
A Neural Rendering Framework for Free-Viewpoint Relighting,CVPR_2020,1,"Peter Hedman, Julien Philip, True Price, Jan-Michael Frahm, George Drettakis, and Gabriel Brostow. Deep blending for free-viewpoint image-based rendering. In SIGGRAPH Asia 2018 Technical Papers , page 257. ACM, 2018.",6
A Neural Rendering Framework for Free-Viewpoint Relighting,CVPR_2020,2,https://github.com/tunabrain/tungsten.,2
A Neural Rendering Framework for Free-Viewpoint Relighting,CVPR_2020,3,"Kara-Ali Aliev, Dmitry Ulyanov, and Victor Lempit- sky. Neural point-based graphics. arXiv preprint arXiv:1906.08240 , 2019.",3
A Neural Rendering Framework for Free-Viewpoint Relighting,CVPR_2020,4,"Dejan Azinovic, Tzu-Mao Li, Anton Kaplanyan, and Matthias Nießner. Inverse path tracing for joint material and lighting estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2447– 2456, 2019.",4
A Neural Rendering Framework for Free-Viewpoint Relighting,CVPR_2020,5,"Michael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geometric deep learning: going beyond euclidean data. IEEE Signal Processing Mag- azine , 34(4):18–42, 2017.",5
EGO-TOPO: Environment Affordances from Egocentric Video,CVPR_2020,1,"R. Girdhar, D. Ramanan, A. Gupta, J. Sivic, and B. Russell. Actionvlad: Learning spatio-temporal aggregation for action classiﬁcation. In CVPR , 2017. 2,7,8",2
EGO-TOPO: Environment Affordances from Egocentric Video,CVPR_2020,2,"H. Grabner, J. Gall, and L. Van Gool. What makes a chair a chair? In CVPR , 2011. 2[20] J. Guan, Y . Yuan, K. M. Kitani, and N. Rhinehart. Generative hybrid representations for activity forecasting with no-regret learning. arXiv preprint arXiv:1904.06250 , 2019. 1,2,6",2
EGO-TOPO: Environment Affordances from Egocentric Video,CVPR_2020,3,"K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR , 2016. 3",2
EGO-TOPO: Environment Affordances from Egocentric Video,CVPR_2020,4,"D. Damen, H. Doughty, G. Maria Farinella, S. Fidler, A. Furnari, E. Kazakos, D. Moltisanti, J. Munro, T. Perrett, W. Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In ECCV , 2018. 1,2,3,5,6",2
EGO-TOPO: Environment Affordances from Egocentric Video,CVPR_2020,5,"D. Damen, T. Leelasawassuk, and W. Mayol-Cuevas. You- do, i-learn: Egocentric unsupervised discovery of objects and their modes of interaction towards video-based guid- ance. CVIU , 2016. 2",2
"Instance-Aware, Context-Focused, and Memory-Ef ﬁcient",CVPR_2020,1,"Zequn Jie, Yunchao Wei, Xiaojie Jin, Jiashi Feng, and Wei Liu. Deep self-taught learning for weakly supervised object localization. In Proc. CVPR , 2017. 2,12,13",5
"Instance-Aware, Context-Focused, and Memory-Ef ﬁcient",CVPR_2020,2,"Xiaopeng Zhang, Jiashi Feng, Hongkai Xiong, and Qi Tian. Zigzag learning for weakly supervised object detection. In Proc. CVPR , 2018. 2",4
"Instance-Aware, Context-Focused, and Memory-Ef ﬁcient",CVPR_2020,3,"Zhaoyang Zeng, Bei Liu, Jianlong Fu, Hongyang Chao, and Lei Zhang. WSOD2: Learning bottom-up and top-down ob- jectness distillation for weakly-supervised object detection. InProc. ICCV , 2019. 1,2,3,5,6,11,12,13",5
"Instance-Aware, Context-Focused, and Memory-Ef ﬁcient",CVPR_2020,4,"Fanyi Xiao and Yong Jae Lee. Video object detection with an aligned spatial-temporal memory. In Proc. ECCV , 2018. 8,13",1
"Instance-Aware, Context-Focused, and Memory-Ef ﬁcient",CVPR_2020,5,"P. Arbel ´aez, J. Pont-Tuset, J. Barron, F. Marques, and J. Ma- lik. Multiscale combinatorial grouping. In Proc. CVPR , 2014. 12",2
Organ at Risk Segmentation for Head and Neck Cancer using Stratiﬁed,CVPR_2020,1,"OAR’s size distributions. To our best knowledge, this simple yet effective organ stratiﬁcation scheme has not been studied for such a complex segmentation and parsing task like ours, by previous work.",2
Organ at Risk Segmentation for Head and Neck Cancer using Stratiﬁed,CVPR_2020,2,4x Upsample,2
Organ at Risk Segmentation for Head and Neck Cancer using Stratiﬁed,CVPR_2020,3,S&H Segmentation,2
Organ at Risk Segmentation for Head and Neck Cancer using Stratiﬁed,CVPR_2020,4,Truth + k x k x 1,2
Organ at Risk Segmentation for Head and Neck Cancer using Stratiﬁed,CVPR_2020,5,"Stratiﬁed Learning Stratiﬁcation is an effective strategy to decouple a complicated task into easier sub-tasks. Com- puter vision has a long history using this strategy. Several contextual learning models have been used to assist gen- eral object detection [ 14,28] within the conditional ran- dom ﬁeld framework [ 21]. Instance learning, i.e. instance localization, segmentation and categorization, often strati- ﬁes the problem into multiple sub-tasks [ 3,7,13]. Within medical imaging, stratiﬁed statistical learning has also been used to recognize whether a candidate nodule connects to any other major lung anatomies [ 43]. Yet, the use of strati- ﬁed learning for semantic segmentation, particularly in the deep-learning era, is still relatively understudied in medi- cal imaging. Within OAR segmentation, Tong et al. [39] have applied a hierarchical stratiﬁcation, but this used a non-deep fuzzy-connectedness model. We are the ﬁrst to execute stratiﬁed learning for deep OAR segmentation.",2
Something-Else: Compositional Action Recognition with,CVPR_2020,1,"Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Scaling egocentric vision: The epic-kitchens dataset. In ECCV , pages 720–736, 2018. 6",11
Something-Else: Compositional Action Recognition with,CVPR_2020,2,"Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Sub- hashini Venugopalan, Sergio Guadarrama, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional networks for visual recognition and description. In CVPR , 2015. 2",7
Something-Else: Compositional Action Recognition with,CVPR_2020,3,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- agnostic meta-learning for fast adaptation of deep networks. InICML , pages 1126–1135. JMLR. org, 2017. 3,7",3
Something-Else: Compositional Action Recognition with,CVPR_2020,4,"Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Judy Hoffman, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. Inferring and executing programs for visual rea- soning. In ICCV , pages 2989–2998, 2017. 3",7
Something-Else: Compositional Action Recognition with,CVPR_2020,5,"Jacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In CVPR , pages 39–48, 2016. 3",4
LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of,CVPR_2020,1,"Dongdong Chen, Jing Liao, Lu Yuan, Nenghai Yu, and Gang Hua. Coherent online video style transfer. In Proceedings of the IEEE International Conference on Computer Vision , pages 1105–1114, 2017.",5
LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of,CVPR_2020,2,"Jan Knopp, Mukta Prasad, Geert Willems, Radu Timofte, and Luc Van Gool. Hough transform and 3d surf for robust three dimensional classiﬁcation. In European Conference on Computer Vision , pages 589–602. Springer, 2010.",5
LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of,CVPR_2020,3,"Robert Osada, Thomas Funkhouser, Bernard Chazelle, and David Dobkin. Shape distributions. ACM Transactions on Graphics (TOG) , 21(4):807–832, 2002.",4
LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of,CVPR_2020,4,"Shumeet Baluja and Ian Fischer. Adversarial transformation networks: Learning to generate adversarial examples. arXiv preprint arXiv:1703.09387 , 2017.",1
LG-GAN: Label Guided Adversarial Network for Flexible Targeted Attack of,CVPR_2020,5,"Yulong Cao, Chaowei Xiao, Dawei Yang, Jing Fang, Ruigang Yang, Mingyan Liu, and Bo Li. Adversarial ob- jects against lidar-based autonomous driving systems. arXiv preprint arXiv:1907.05418 , 2019.",7
Self-supervised Domain-aware Generative Network for Generalized Zero-shot,CVPR_2020,1,"Weilun Chao, Soravit Changpinyo, Boqing Gong, and Fei Sha. An empirical study and analysis of generalized zero- shot learning for object recognition in the wild. In ECCV , 2016.",4
Self-supervised Domain-aware Generative Network for Generalized Zero-shot,CVPR_2020,2,"Long Chen, Hanwang Zhang, Jun Xiao, Wei Liu, and Shih- Fu Chang. Zero-shot visual recognition using semantics- preserving adversarial embedding networks. In CVPR , 2018.",5
Self-supervised Domain-aware Generative Network for Generalized Zero-shot,CVPR_2020,3,"Kai Li, Martin Renqiang Min, and Yun Fu. Rethinking zero- shot learning: A conditional visual classiﬁcation perspective. InCVPR , 2019.[19] Jiawei Liu, Zheng-Jun Zha, Di Chen, Richang Hong, and Meng Wang. Adaptive transfer network for cross-domain person re-identiﬁcation. In CVPR , 2019.",3
Self-supervised Domain-aware Generative Network for Generalized Zero-shot,CVPR_2020,4,"Pierre Sermanet, Corey Lynch, Yevgen Chebotar, Jasmine Hsu, Eric Jang, Stefan Schaal, Sergey Levine, and Google Brain. Time-contrastive networks: Self-supervised learning from video. In IEEE International Conference on Robotics and Automation , pages 1134–1141, 2018.",8
Self-supervised Domain-aware Generative Network for Generalized Zero-shot,CVPR_2020,5,"Catherine Wah, Steve Branson, Peter Welinder, Pietro Per- ona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011.",5
Towards the Perceptual Quality Enhancement of Low B it-rate Compressed,CVPR_2020,1,"Filippov, A., et al. ""CE3: A combination of tests 3 .1. 2 and 3.1. 4 for intra reference sample interpolation fil ter."" document JVET-L0628, in Proc. of 12th JVET meeting. 2018.",2
Towards the Perceptual Quality Enhancement of Low B it-rate Compressed,CVPR_2020,2,"X. Ma, et al. ""CE3: Tests of cross-component linear model in BMS, "" document JVET-K0190, in Proc. of 13th JVET meeting. 2018.",2
Towards the Perceptual Quality Enhancement of Low B it-rate Compressed,CVPR_2020,3,"Simonyan, Karen, and Andrew Zisserman. ""Very deep convolutional networks for large-scale image recogn ition."" arXiv preprint arXiv:1409.1556 (2014).",3
Towards the Perceptual Quality Enhancement of Low B it-rate Compressed,CVPR_2020,4,"Wang, Xintao, et al. ""Esrgan: Enhanced super-resolu tion generative adversarial networks."" Proceedings of th e European Conference on Computer Vision (ECCV). 2018 .",3
Towards the Perceptual Quality Enhancement of Low B it-rate Compressed,CVPR_2020,5,"Kim, Dong‐Wook, et al. ""Constrained adversarial los s for generative adversarial network‐based faithful image restoration."" ETRI Journal 41.4 (2019): 415-425.",3
PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable,CVPR_2020,1,"from two references partially. The results on the third col- umn recombine the makeup of lip from reference 1 and other part of makeup from reference 2, which are natural and realistic. Also, only transferring the lipstick from ref- erence 1 and remain other parts unchanged can be achieved 5199 Source Reference 1 Results Reference 2 Single-referenceMulti-referenceα = 1 α = 0.8 α = 0.6 α = 0.4 α = 0.2 α = 0 Figure 6. Results of interpolated makeup styles. If only one reference is used, adjusting the shade of makeup can be realized (1st row). If two references are used (1st column and last column), the makeup of the transferred images is gradually changing from reference 1 towards reference 2 from left to right (2nd rows). by assigning x=y2. The new feature of partial makeup makes PSGAN realize the ﬂexible partial makeup transfer. Moreover, we can interpolate the makeup with two ref- erence images by a coefﬁcient α∈[0,1]. We ﬁrst get the makeup tensors of two references y1andy2, and then com- pute the new parameters by weighting them with the coefﬁ- cientα. The resulted feature map Vx′is calculated by Vx′= (αΓ′ y1+(1−α)Γ′ y2)Vx+(αB′ y1+(1−α)B′ y2). (11) Figure 6shows the interpolated makeup transfer results with one and two reference images. By feeding the new makeup tensors into MANet, we yield a smooth transition between two reference makeup styles. Similarly, we can adjust the shade of transfer using only one reference image by assign- ingx=y1. The generated results demonstrate that our PS- GAN can not only control the shade of makeup transfer but also generate a new style of makeup by mixing the makeup tensors of two makeup styles. We can also perform partial and interpolated transfer si- multaneously by leveraging both the face parsing maps and coefﬁcient thanks to the design of spatial-aware makeup matrices. The above experiments have demonstrated that PSGAN broadens the application range of makeup transfer signiﬁcantly. 4.5. Comparison We conduct comparison with general image-to-image translation methos DIA [ 19] and CycleGAN [ 33] as well as state-of-the-art makeup transfer methods BeautyGAN (BGAN) [ 18], PairedCycleGAN (PGAN) [ 2], BeautyGlow (BGlow) [ 3] and LADN [ 11]. Current makeup transfer methods leverage face parsing maps [ 2,3,18] and facial landmarks [ 11] for training and realize different functions as shown in Table 1. Quantitative Comparison. We conduct a user study for quantitative evaluation on Amazon Mechanical Turk that use BGAN, CGAN, DIA, and LADN as baselines. ForMethod shade-controllable partial robust BGAN [ 18] PGAN [ 2] BGlow [ 3] /check LADN [ 11] /check PSGAN /check /check /check Table 1. Analysises of existing methods. “robust” indicates pose and expression robust transfer. Test set PSGAN BGAN DIA CGAN LADN MT 61.5 32.5 3.25 2.5 0.25 M-Wild 83.5 13.5 1.75 1.25 0.0 Table 2. Ratio selected as best (%). a fair comparison, we only compare with methods whose code and pre-train model are released since we cannot guar- antee a perfect re-implementation. We randomly select 20 source images and 20 reference images from both the MT test set and Makeup-Wild (M-Wild) dataset. After using the above methods to perform makeup transfer between these images, we obtain 800 images for each method. Workers are asked to choose the best images generated by ﬁve meth- ods by considering image realism and the similarity with reference makeup styles. Table 2shows the human evalua- tion results. Our PSGAN outperforms other methods by a large margin, especially on the M-Wild test set. Qualitative Comparison. Figure 7shows the qualita- tive comparison of PSGAN with other state-of-the-art meth- ods on frontal faces in neutral expressions. Since the code of BeautyGlow and PairedCycleGAN is not released, we follow the strategy of BeautyGlow which cropped the re- sults from corresponding papers. The result produced by DIA has an unnatural color on hair and background since it performs transfer in the whole image. CycleGAN can only synthesize general makeup style with is not similar to the 5200 Reference Source CycleGAN BeautyGAN BeautyGlow DIA PSGAN LADN Reference Source CycleGAN PairedCycleGAN DIA PSGAN LADN BeautyGlow Figure 7. Qualitative comparison. PSGAN is able to generate realistic images with the same makeup styles as the reference. Reference Source BeautyGAN LADN PSGAN Figure 8. Qualitative comparison on M-Wild test set. reference. Besides, BeautyGlow fails to preserve the color of pupils and does not have the same foundation makeup as reference. We also use the pre-trained model released by the author of LADN, which produces blurry transfer re- sults and unnatural background. Compared to the baselines, our method is able to generate vivid images with the same makeup styles as reference. We also conduct a comparison on the M-Wild test set with the state-of-the-art method (BeautyGAN and LADN) that provide code and pre-trained model, as shown in Fig- ure8. Since the current methods lack an explicit mecha- nism to guide the direction of transfer at the pixel-level and also overﬁt on frontal images, the makeup is applied in the wrong region of the face when dealing with images with different poses and expressions. For example, the lip gloss is transferred to the skin on the ﬁrst row of Figure 8. In the second row, other methods fail to perform transfer on faces with different sizes. However, our AMM module can accu- rately assign the makeup for every pixel through calculating the similarities, which makes our results look better. 4.6. Video Makeup Transfer To transfer makeup for a person in the video is a chal- lenging and meaningful task, which has wide prospects in the applications. However, the pose and expression of a face in the video are continuously changing which brings extra difﬁculties. To examine the effectiveness of our method, ReferenceBefore After Figure 9. Video makeup transfer results of PSGAN. we simply perform makeup transfer on every frame of the video, as shown in Figure 9. By incorporating the design of PSGAN, we receive nice and stable transferred results. 5. Conclusion In order to bring makeup transfer to real-world appli- cations, we propose PSGAN that ﬁrst distills the makeup style into two makeup matrices from the reference and then leverages an Attentive Makeup Morphing (AMM) module to conduct makeup transfer accurately. The experiments demonstrate our approach can achieve state-of-the-art trans- fer results on both frontal facial images and facial images that have various poses and expressions. Also, with the spatial-aware makeup matrices, PSGAN can transfer the makeup partially and adjust the shade of transfer, which greatly broadens the application range of makeup transfer. Moreover, we believe our novel framework can be used in other conditional image synthesis problems that require customizable and precise synthesis. Acknowledgement This work is partially supported by the National Natural Science Foundation of China (Grant 61572493, Grant 61876177), Beijing Natural Science Foun- dation (L182013, 4202034) and Fundamental Research Funds for the Central Universities. This work is also spon- sored by Zhejiang Lab (No.2019KD0AB04). We also thank Jinyu Chen for his help. 5201 References",2
PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable,CVPR_2020,2,"from two references partially. The results on the third col- umn recombine the makeup of lip from reference 1 and other part of makeup from reference 2, which are natural and realistic. Also, only transferring the lipstick from ref- erence 1 and remain other parts unchanged can be achieved 5199 Source Reference 1 Results Reference 2 Single-referenceMulti-referenceα = 1 α = 0.8 α = 0.6 α = 0.4 α = 0.2 α = 0 Figure 6. Results of interpolated makeup styles. If only one reference is used, adjusting the shade of makeup can be realized (1st row). If two references are used (1st column and last column), the makeup of the transferred images is gradually changing from reference 1 towards reference 2 from left to right (2nd rows). by assigning x=y2. The new feature of partial makeup makes PSGAN realize the ﬂexible partial makeup transfer. Moreover, we can interpolate the makeup with two ref- erence images by a coefﬁcient α∈[0,1]. We ﬁrst get the makeup tensors of two references y1andy2, and then com- pute the new parameters by weighting them with the coefﬁ- cientα. The resulted feature map Vx′is calculated by Vx′= (αΓ′ y1+(1−α)Γ′ y2)Vx+(αB′ y1+(1−α)B′ y2). (11) Figure 6shows the interpolated makeup transfer results with one and two reference images. By feeding the new makeup tensors into MANet, we yield a smooth transition between two reference makeup styles. Similarly, we can adjust the shade of transfer using only one reference image by assign- ingx=y1. The generated results demonstrate that our PS- GAN can not only control the shade of makeup transfer but also generate a new style of makeup by mixing the makeup tensors of two makeup styles. We can also perform partial and interpolated transfer si- multaneously by leveraging both the face parsing maps and coefﬁcient thanks to the design of spatial-aware makeup matrices. The above experiments have demonstrated that PSGAN broadens the application range of makeup transfer signiﬁcantly. 4.5. Comparison We conduct comparison with general image-to-image translation methos DIA [ 19] and CycleGAN [ 33] as well as state-of-the-art makeup transfer methods BeautyGAN (BGAN) [ 18], PairedCycleGAN (PGAN) [ 2], BeautyGlow (BGlow) [ 3] and LADN [ 11]. Current makeup transfer methods leverage face parsing maps [ 2,3,18] and facial landmarks [ 11] for training and realize different functions as shown in Table 1. Quantitative Comparison. We conduct a user study for quantitative evaluation on Amazon Mechanical Turk that use BGAN, CGAN, DIA, and LADN as baselines. ForMethod shade-controllable partial robust BGAN [ 18] PGAN [ 2] BGlow [ 3] /check LADN [ 11] /check PSGAN /check /check /check Table 1. Analysises of existing methods. “robust” indicates pose and expression robust transfer. Test set PSGAN BGAN DIA CGAN LADN MT 61.5 32.5 3.25 2.5 0.25 M-Wild 83.5 13.5 1.75 1.25 0.0 Table 2. Ratio selected as best (%). a fair comparison, we only compare with methods whose code and pre-train model are released since we cannot guar- antee a perfect re-implementation. We randomly select 20 source images and 20 reference images from both the MT test set and Makeup-Wild (M-Wild) dataset. After using the above methods to perform makeup transfer between these images, we obtain 800 images for each method. Workers are asked to choose the best images generated by ﬁve meth- ods by considering image realism and the similarity with reference makeup styles. Table 2shows the human evalua- tion results. Our PSGAN outperforms other methods by a large margin, especially on the M-Wild test set. Qualitative Comparison. Figure 7shows the qualita- tive comparison of PSGAN with other state-of-the-art meth- ods on frontal faces in neutral expressions. Since the code of BeautyGlow and PairedCycleGAN is not released, we follow the strategy of BeautyGlow which cropped the re- sults from corresponding papers. The result produced by DIA has an unnatural color on hair and background since it performs transfer in the whole image. CycleGAN can only synthesize general makeup style with is not similar to the 5200 Reference Source CycleGAN BeautyGAN BeautyGlow DIA PSGAN LADN Reference Source CycleGAN PairedCycleGAN DIA PSGAN LADN BeautyGlow Figure 7. Qualitative comparison. PSGAN is able to generate realistic images with the same makeup styles as the reference. Reference Source BeautyGAN LADN PSGAN Figure 8. Qualitative comparison on M-Wild test set. reference. Besides, BeautyGlow fails to preserve the color of pupils and does not have the same foundation makeup as reference. We also use the pre-trained model released by the author of LADN, which produces blurry transfer re- sults and unnatural background. Compared to the baselines, our method is able to generate vivid images with the same makeup styles as reference. We also conduct a comparison on the M-Wild test set with the state-of-the-art method (BeautyGAN and LADN) that provide code and pre-trained model, as shown in Fig- ure8. Since the current methods lack an explicit mecha- nism to guide the direction of transfer at the pixel-level and also overﬁt on frontal images, the makeup is applied in the wrong region of the face when dealing with images with different poses and expressions. For example, the lip gloss is transferred to the skin on the ﬁrst row of Figure 8. In the second row, other methods fail to perform transfer on faces with different sizes. However, our AMM module can accu- rately assign the makeup for every pixel through calculating the similarities, which makes our results look better. 4.6. Video Makeup Transfer To transfer makeup for a person in the video is a chal- lenging and meaningful task, which has wide prospects in the applications. However, the pose and expression of a face in the video are continuously changing which brings extra difﬁculties. To examine the effectiveness of our method, ReferenceBefore After Figure 9. Video makeup transfer results of PSGAN. we simply perform makeup transfer on every frame of the video, as shown in Figure 9. By incorporating the design of PSGAN, we receive nice and stable transferred results. 5. Conclusion In order to bring makeup transfer to real-world appli- cations, we propose PSGAN that ﬁrst distills the makeup style into two makeup matrices from the reference and then leverages an Attentive Makeup Morphing (AMM) module to conduct makeup transfer accurately. The experiments demonstrate our approach can achieve state-of-the-art trans- fer results on both frontal facial images and facial images that have various poses and expressions. Also, with the spatial-aware makeup matrices, PSGAN can transfer the makeup partially and adjust the shade of transfer, which greatly broadens the application range of makeup transfer. Moreover, we believe our novel framework can be used in other conditional image synthesis problems that require customizable and precise synthesis. Acknowledgement This work is partially supported by the National Natural Science Foundation of China (Grant 61572493, Grant 61876177), Beijing Natural Science Foun- dation (L182013, 4202034) and Fundamental Research Funds for the Central Universities. This work is also spon- sored by Zhejiang Lab (No.2019KD0AB04). We also thank Jinyu Chen for his help. 5201 References",2
PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable,CVPR_2020,3,"Taleb Alashkar, Songyao Jiang, Shuyang Wang, and Yun Fu. Examples-rules guided deep neural network for makeup rec- ommendation. In AAAI , 2017. 2",4
PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable,CVPR_2020,4,"Huiwen Chang, Jingwan Lu, Fisher Yu, and Adam Finkel- stein. Pairedcyclegan: Asymmetric style transfer for apply- ing and removing makeup. In CVPR , 2018. 1,2,7",4
PSGAN: Pose and Expression Robust Spatial-Aware GAN for Customizable,CVPR_2020,5,"Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS , 2014. 1",2
Learning Augmentation Network via Inﬂuence Functions,CVPR_2020,1,"Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: learning augmentation policies from data. IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. 1,2,6,7,8 10968",5
Learning Augmentation Network via Inﬂuence Functions,CVPR_2020,2,"Xiaodong Cui, Vaibhava Goel, and Brian Kingsbury. Data augmentation for deep neural network acoustic modeling. IEEE/ACM Transactions on Audio, Speech and Language Processing , 23(9):1469–1477, 2015. 1",3
Learning Augmentation Network via Inﬂuence Functions,CVPR_2020,3,"Xavier Gastaldi. Shake-shake regularization. arXiv preprint arXiv:1705.07485 , 2017. 6,7",2
Learning Augmentation Network via Inﬂuence Functions,CVPR_2020,4,"Henry A Rowley, Shumeet Baluja, and Takeo Kanade. Neu- ral network-based face detection. IEEE Transactions on Pat- tern Analysis and Machine Intelligence (TPAMI) , 20(1):23– 38, 1998. 2",3
Learning Augmentation Network via Inﬂuence Functions,CVPR_2020,5,"Naman Agarwal, Brian Bullins, and Elad Hazan. Second- order stochastic optimization in linear time. stat, 1050:15, 2016. 3",3
Unsupervised object detection via LWIR/RGB translation,CVPR_2020,1,"Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsuper- vised pixel-level domain adaptation with generative adversarial networks. CoRR , abs/1612.05424, 2016. 2",5
Unsupervised object detection via LWIR/RGB translation,CVPR_2020,2,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural im- age synthesis. CoRR , abs/1809.11096, 2018. 2",3
Unsupervised object detection via LWIR/RGB translation,CVPR_2020,3,"Xun Huang, Ming-Yu Liu, Serge J. Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. CoRR , abs/1804.04732, 2018. 2,5",3
Unsupervised object detection via LWIR/RGB translation,CVPR_2020,4,"R. Abbott, J. M. Del Rincon, B. Connor, and N. Robertson. Deep object classiﬁcation in low resolu- tion lwir imagery via transfer learning. In Proceed- ings of the 5th IMA Conference on Mathematics in Defence , 11 2017. 2[2] Rachael Abbott, Neil Robertson, Jesus Martinez del Rincon, and Barry Connor. Multimodal object detec- tion using unsupervised transfer learning and adapta- tion techniques. In Judith Dijk, editor, Artiﬁcial In- telligence and Machine Learning in Defense Applica- tions , volume 11169, pages 49 – 58. International So- ciety for Optics and Photonics, SPIE, 2019. 2,4",2
Unsupervised object detection via LWIR/RGB translation,CVPR_2020,5,"J ¨orgen Ahlberg and Michael Felsberg. Generating vis- ible spectrum images from thermal infrared. pages 1224–122409, 06 2018. 2",1
"Representations, Metrics and Statistics for Shape Analysis of Elastic Graphs",CVPR_2020,1,"David G Kendall. Shape manifolds, procrustean metrics, and complex projective spaces. Bulletin of the London mathemat- ical society , 16(2):81–121, 1984. 1",1
"Representations, Metrics and Statistics for Shape Analysis of Elastic Graphs",CVPR_2020,2,"Joseph B Kruskal. Multidimensional scaling by optimizing goodness of ﬁt to a nonmetric hypothesis. Psychometrika , 29(1):1–27, 1964. 7",1
"Representations, Metrics and Statistics for Shape Analysis of Elastic Graphs",CVPR_2020,3,"Giorgio A Ascoli, Duncan E Donohue, and Maryam Halavi. Neuromorpho. org: a central resource for neuronal mor- phologies. Journal of Neuroscience , 27(35):9247–9251, 2007. 5",3
"Representations, Metrics and Statistics for Shape Analysis of Elastic Graphs",CVPR_2020,4,"Burcu Aydın, G ´abor Pataki, Haonan Wang, Elizabeth Bullitt, James Stephen Marron, et al. A principal component analysis for trees. The Annals of Applied Statistics , 3(4):1597–1615, 2009. 5",6
"Representations, Metrics and Statistics for Shape Analysis of Elastic Graphs",CVPR_2020,5,"Stephen R Aylward and Elizabeth Bullitt. Initialization, noise, singularities, and scale in height ridge traversal for tubular object centerline extraction. IEEE transactions on medical imaging , 21(2):61–75, 2002. 5",1
Learning Dynamic Routing for Semantic Segmentation,CVPR_2020,1,"Lin Song, Yanwei Li, Zeming Li, Gang Yu, Hongbin Sun, Jian Sun, and Nanning Zheng. Learnable tree ﬁlter for structure-preserving feature transform. In NeurIPS , 2019. 1,2,5",7
Learning Dynamic Routing for Semantic Segmentation,CVPR_2020,2,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. TPAMI , 2017. 2",5
Learning Dynamic Routing for Semantic Segmentation,CVPR_2020,3,"Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv:1706.05587 , 2017. 2,4,5,6,8",4
Learning Dynamic Routing for Semantic Segmentation,CVPR_2020,4,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV , 2018. 1,2,8",5
Learning Dynamic Routing for Semantic Segmentation,CVPR_2020,5,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. InMICCAI , 2015. 1,2,4,5,6",3
Training Noise-Robust Deep Neural Networks via Meta-Learning,CVPR_2020,1,"Giorgio Patrini, Alessandro Rozza, Aditya Krishna Menon, Richard Nock, and Lizhen Qu. Making deep neural networks robust to label noise: A loss correction approach. In CVPR , pages 2233–2241, 2017.",5
Training Noise-Robust Deep Neural Networks via Meta-Learning,CVPR_2020,2,"Sainbayar Sukhbaatar and Rob Fergus. Learning from noisy labels with deep neural networks. arXiv preprint arXiv:1406.2080 , 2(3):4, 2014. 43294532",1
Training Noise-Robust Deep Neural Networks via Meta-Learning,CVPR_2020,3,"Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep neural networks. In AAAI , pages 1919–1925, 2017.",3
Training Noise-Robust Deep Neural Networks via Meta-Learning,CVPR_2020,4,"Aritra Ghosh, Naresh Manwani, and PS Sastry. Making risk minimization tolerant to label noise. Neurocomputing , 160:93–107, 2015.",3
Training Noise-Robust Deep Neural Networks via Meta-Learning,CVPR_2020,5,"Tongliang Liu and Dacheng Tao. Classiﬁcation with noisy labels by importance reweighting. IEEE TPAMI , 38(3):447– 461, 2016.",1
Quality and Relevance Metrics for Selection of Multimodal Pretraining Data,CVPR_2020,1,"Mark Davies. English n-grams (based on data from the coca corpus). https://www.ngrams.info, 2011. 4",2
Quality and Relevance Metrics for Selection of Multimodal Pretraining Data,CVPR_2020,2,"P. Goyal, D. Mahajan, A. Gupta, and I. Misra. Scaling and benchmarking self-supervised visual representation learn- ing. In The International Conference on Computer Vision (ICCV) , 2019. 1,2",2
Quality and Relevance Metrics for Selection of Multimodal Pretraining Data,CVPR_2020,3,"Peter Young, Alice Lai, Micah Hodosh, and Julia Hocken- maier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descrip- tions. The Conference of the Association for Computational Linguistics (ACL) , 2:67–78, 2014. 1,5,6",4
Quality and Relevance Metrics for Selection of Multimodal Pretraining Data,CVPR_2020,4,"Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. In The Conference on Empirical Methods in Natural Language Processing (EMNLP) , pages 1532–1543, 2014. 1,3",3
Quality and Relevance Metrics for Selection of Multimodal Pretraining Data,CVPR_2020,5,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In The Conference on Computer Vision and Pattern Recogni- tion (CVPR) , 2009. 2",2
Probing for Artifacts: Detecting Imagenet Model Evasions,CVPR_2020,1,"Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, and Somesh Jha. Towards understanding limitations of pixel discretization against adversarial attacks. In 2019 IEEE Euro- pean Symposium on Security and Privacy (EuroS&P) , pages 480–495. IEEE, 2019.",5
Probing for Artifacts: Detecting Imagenet Model Evasions,CVPR_2020,2,"Trevor Hastie, Robert Tibshirani, and Jerome Friedman. The elements of statistical learning: data mining, inference, and prediction . Springer Science & Business Media, 2009.",3
Probing for Artifacts: Detecting Imagenet Model Evasions,CVPR_2020,3,"Bo Huang, Yi Wang, and Wei Wang. Model-agnostic adver- sarial detection by random perturbations. In Proceedings of the 28th International Joint Conference on Artiﬁcial Intelli- gence , pages 4689–4696. AAAI Press, 2019.",3
Probing for Artifacts: Detecting Imagenet Model Evasions,CVPR_2020,4,"Saumya Jetley, Nicholas Lord, and Philip Torr. With friends like these, who needs adversaries? In Advances in Neural Information Processing Systems , pages 10749–10759, 2018.",3
Probing for Artifacts: Detecting Imagenet Model Evasions,CVPR_2020,5,"Jinkyu Koo, Michael Roth, and Saurabh Bagchi. Hawkeye: Adversarial example detector for deep neural networks. arXiv preprint arXiv:1909.09938 , 2019.",3
Hierarchical Clustering with Hard-batch Triplet Loss for Person,CVPR_2020,1,"Wei Li, Rui Zhao, Tong Xiao, and Xiaogang Wang. Deep- reid: Deep ﬁlter pairing neural network for person re- identiﬁcation. In CVPR , 2014.",4
Hierarchical Clustering with Hard-batch Triplet Loss for Person,CVPR_2020,2,"Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In ICML , 2016.",5
Hierarchical Clustering with Hard-batch Triplet Loss for Person,CVPR_2020,3,"Liangchen Song, Cheng Wang, Lefei Zhang, Bo Du, Qian Zhang, Chang Huang, and Xinggang Wang. Unsuper- vised domain adaptive re-identiﬁcation: Theory and practice. arXiv preprint arXiv:1807.11334 , 2018.",7
Hierarchical Clustering with Hard-batch Triplet Loss for Person,CVPR_2020,4,"Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the importance of initialization and momentum in deep learning. In ICML , 2013.",4
Hierarchical Clustering with Hard-batch Triplet Loss for Person,CVPR_2020,5,"Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing- dong Wang, and Qi Tian. Scalable person re-identiﬁcation: A benchmark. In CVPR , 2015.",6
Hierarchical Image Classi ﬁcation using Entailment Cone Embeddings,CVPR_2020,1,"J. Redmon and A. Farhadi, “Yolo9000: Better, faster, stronger,” in The IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR) , July 2017.",2
Hierarchical Image Classi ﬁcation using Entailment Cone Embeddings,CVPR_2020,2,"B. Barz and J. Denzler, “Hierarchy-based image em- beddings for semantic image retrieval,” arXiv preprint arXiv:1809.bib09924 , 2018.",2
Hierarchical Image Classi ﬁcation using Entailment Cone Embeddings,CVPR_2020,3,"S. Kumar and R. Zheng, “Hierarchical category de- tector for clothing recognition from visual data,” in Proceedings of the IEEE International Conference on Computer Vision , pp. 2306–2312, 2017.",2
Hierarchical Image Classi ﬁcation using Entailment Cone Embeddings,CVPR_2020,4,"M. Le, S. Roller, L. Papaxanthos, D. Kiela, and M. Nickel, “Inferring concept hierarchies from text corpora via hyperbolic embeddings,” arXiv preprint arXiv:1902.00913 , 2019.",2
Hierarchical Image Classi ﬁcation using Entailment Cone Embeddings,CVPR_2020,5,"O. Ganea, G. B ´ecigneul, and T. Hofmann, “Hyper- bolic neural networks,” in Advances in neural infor- mation processing systems , pp. 5345–5355, 2018.",2
Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking,CVPR_2020,1,"M. Danelljan, G. Bhat, F. S. Khan, and M. Felsberg. ATOM: Accurate tracking by overlap maximization. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. , pages 4660–4669, 2019.",2
Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking,CVPR_2020,2,"M. Danelljan, G. H ¨ager, F. S. Khan, and M. Felsberg. Learn- ing spatially regularized correlation ﬁlters for visual track- ing. In Proc. IEEE Int. Conf. Comput. Vis. , pages 4310–4318, 2015.",2
Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking,CVPR_2020,3,"M. Danelljan, A. Robinson, F. S. Khan, and M. Felsberg. Beyond correlation ﬁlters: Learning continuous convolution operators for visual tracking. In Proc. Eur. Conf. Comput. Vis., pages 472–488, 2016.",2
Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking,CVPR_2020,4,"I. Jung, J. Son, M. Baek, and B. Han. Real-time MDNet. In Proc. Eur. Conf. Comput. Vis. , pages 89–104, 2018.[18] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath, D. Ku- maran, and R. Hadsell. Continual learning of context- dependent processing in neural networks. Proc. Natl. Acad. Sci., 114(13):3521–3526, Mar. 2017.",2
Recursive Least-Squares Estimator-Aided Online Learning for Visual Tracking,CVPR_2020,5,"C. Ma, J.B. Huang, X. Yang, and M.H. Yang. Hierarchical convolutional features for visual tracking. In Proc. IEEE Int. Conf. Comput. Vis. , pages 3074–3082, 2015.",2
3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation,CVPR_2020,1,"C. R. Qi, W. Liu, C. Wu, H. Su, and L. J. Guibas. Frustum PointNets for 3D Object Detection from RGB-D Data. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) , 2018. 5",2
3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation,CVPR_2020,2,"T.-Y . Lin, P. Goyal, R. B. Girshick, K. He, and P. Doll ´ar. Fo- cal Loss for Dense Object Detection. In IEEE International Conference on Computer Vision (ICCV) , 2017. 5",2
3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation,CVPR_2020,3,"I. Armeni, Z.-Y . He, J. Gwak, A. R. Zamir, M. Fischer, J. Ma- lik, and S. Savarese. 3D Scene Graph: A Structure for Uni- ﬁed Semantics, 3D Space, and Camera. In IEEE Interna- tional Conference on Computer Vision (ICCV) , 2019. 2",2
3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation,CVPR_2020,4,"I. Armeni, O. Sener, A. R. Zamir, H. Jiang, I. Brilakis, M. Fischer, and S. Savarese. 3D Semantic Parsing of Large- Scale Indoor Spaces. In IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR) , 2016. 5,7,8",2
3D-MPA: Multi Proposal Aggregation for 3D Semantic Instance Segmentation,CVPR_2020,5,"A. Behl, D. Paschalidou, S. Donne, and A. Geiger. Point- FlowNet: Learning Representations for Rigid Motion Esti- mation from Point Clouds. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. 2",2
Adversarial Camouﬂage: Hiding Physical-World Attacks with Natural Styles,CVPR_2020,1,"Hsueh-Ti Derek Liu, Michael Tao, Chun-Liang Li, Derek Nowrouzezahrai, and Alec Jacobson. Beyond pixel norm- balls: Parametric adversaries using an analytically differen- tiable renderer. In ICLR , 2018. 3",5
Adversarial Camouﬂage: Hiding Physical-World Attacks with Natural Styles,CVPR_2020,2,"Xingjun Ma, Yuhao Niu, Lin Gu, Yisen Wang, Yitian Zhao, James Bailey, and Feng Lu. Understanding adversarial at- tacks on deep learning based medical image analysis sys- tems. In arXiv:1907.10456 , 2019. 1",7
Adversarial Camouﬂage: Hiding Physical-World Attacks with Natural Styles,CVPR_2020,3,"Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition. In CCS, 2016. 1, 3,4",4
Adversarial Camouﬂage: Hiding Physical-World Attacks with Natural Styles,CVPR_2020,4,"Yisen Wang, Xuejiao Deng, Songbai Pu, and Zhiheng Huang. Residual convolutional ctc networks for automatic speech recognition. arXiv preprint arXiv:1702.07793 , 2017. 1",4
Adversarial Camouﬂage: Hiding Physical-World Attacks with Natural Styles,CVPR_2020,5,"Yisen Wang, Xingjun Ma, James Bailey, Jinfeng Yi, Bowen Zhou, and Quanquan Gu. On the convergence and robustness of adversarial training. In ICML , 2019. 1",6
Minimal Solutions to Relative Pose Estimation From Two Views Sharing a,CVPR_2020,1,"Sylvain Bougnoux. From projective to euclidean space under any practical situation, a criticism of self-calibration. In The IEEE International Conference on Computer Vision (ICCV) , 1998. 1,6,7,8",2
Minimal Solutions to Relative Pose Estimation From Two Views Sharing a,CVPR_2020,2,"Banglei Guan, Qifeng Yu, and Friedrich Fraundorfer. Mini- mal solutions for the rotational alignment of imu-camera sys- tems using homography constraints. Computer vision and image understanding , 2018. 2",3
Minimal Solutions to Relative Pose Estimation From Two Views Sharing a,CVPR_2020,3,"Zuzana Kukelova, Martin Bujnak, and Tomas Pajdla. Auto- matic generator of minimal problem solvers. In The Euro- pean Conference on Computer Vision (ECCV) , 2008. 3",3
Minimal Solutions to Relative Pose Estimation From Two Views Sharing a,CVPR_2020,4,"Richard Hartley. Estimation of relative camera positions for uncalibrated cameras. In The European Conference on Com- puter Vision (ECCV) , 1992. 1",2
Minimal Solutions to Relative Pose Estimation From Two Views Sharing a,CVPR_2020,5,"David Nist ´er. An efﬁcient solution to the ﬁve-point relative pose problem. IEEE transactions on pattern analysis and machine intelligence , 2004. 1,5",2
Diagram Image Retrieval,CVPR_2020,1,"M. Oquab, L. Bottou, I. Laptev, and J. Sivic. Learning and transferring mid-level image representations using con- volutional neural networks. In Computer Vision and Pat- tern Recognition (CVPR), IEEE Conference on , pages 1717– 1724, 2014. 3",2
Diagram Image Retrieval,CVPR_2020,2,"S. Antani, R. Kasturi, and R. Jain. A survey on the use of pattern recognition methods for abstraction, indexing and re- trieval of images and video. Pattern recognition , 35(4):945– 965, 2002. 2",2
Diagram Image Retrieval,CVPR_2020,3,"G. Csurka, J.-M. Renders, and G. Jacquet. Xrce’s partici- pation at patent image classiﬁcation and image-based patent retrieval tasks of the clef-ip 2011. In CLEF (Notebook Pa- pers/Labs/Workshop) , volume 2, 2011. 2",2
Diagram Image Retrieval,CVPR_2020,4,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. Imagenet: A large-scale hierarchical image database. In2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. 1",2
Diagram Image Retrieval,CVPR_2020,5,"R. Hadsell, S. Chopra, and Y . LeCun. Dimensionality reduc- tion by learning an invariant mapping. In 2006 IEEE Com- puter Society Conference on Computer Vision and Pattern Recognition (CVPR’06) , volume 2, pages 1735–1742. IEEE, 2006. 6",2
Instance Guided Proposal Network for Person Search,CVPR_2020,1,"Piotr Doll ´ar, Ron Appel, Serge Belongie, and Pietro Per- ona. Fast feature pyramids for object detection. IEEE Transactions on Pattern Analysis and Machine Intelligence , 36(8):1532–1545, 2014. 2",4
Instance Guided Proposal Network for Person Search,CVPR_2020,2,"Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision , pages 2961–2969, 2017. 5",4
Instance Guided Proposal Network for Person Search,CVPR_2020,3,"Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net- works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7132–7141, 2018. 5",3
Instance Guided Proposal Network for Person Search,CVPR_2020,4,"Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3588–3597, 2018. 2",5
Instance Guided Proposal Network for Person Search,CVPR_2020,5,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 770–778, 2016. 3",4
AutoTrack: Towards High-Performance Visual Tracking for UA V with,CVPR_2020,1,"Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, and Qi Tian. The unmanned aerial vehicle benchmark: object detection and tracking. In ECCV , pages 370–386, 2018. 5, 6,8",9
AutoTrack: Towards High-Performance Visual Tracking for UA V with,CVPR_2020,2,"Bonatti Rogerio, Ho Cherie, Wang Wenshan, Choudhury Sanjiban, and Scherer Sebastian. Towards a robust aerial cinematography platform: Localizing and tracking moving targets in unstructured environments. In IROS , pages 229– 236, 2019. 1",5
AutoTrack: Towards High-Performance Visual Tracking for UA V with,CVPR_2020,3,"Li Rui, Pang Minjian, Zhao Cong, Zhou Guyue, and Lu Fang. Monocular long-term target following on uavs. In CVPR Workshops , pages 29–37, 2016. 1",5
AutoTrack: Towards High-Performance Visual Tracking for UA V with,CVPR_2020,4,"Changhong Fu, Adrian Carrio, Miguel A Olivares-Mendez, Ramon Suarez-Fernandez, and Pascual Campoy. Ro- bust real-time vision-based aircraft tracking from unmanned aerial vehicles. In ICRA , pages 5441–5446, 2014. 1",5
AutoTrack: Towards High-Performance Visual Tracking for UA V with,CVPR_2020,5,"M ¨ucahit Karaduman, Ahmet C ¸ ınar, and Haluk Eren. Uav trafﬁc patrolling via road detection and tracking in anony- mous aerial video frames. Journal of Intelligent & Robotic Systems , 95(2):675–690, 2019. 1",3
Conditional Gaussian Distribution Learning for Open Set Recognition,CVPR_2020,1,"Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision , pages 2961–2969, 2017. 1",4
Conditional Gaussian Distribution Learning for Open Set Recognition,CVPR_2020,2,"Abhijit Bendale and Terrance E Boult. Towards open set deep networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 1563–1572, 2016. 3,7,8",1
Conditional Gaussian Distribution Learning for Open Set Recognition,CVPR_2020,3,"Larry M Manevitz and Malik Yousef. One-class svms for document classiﬁcation. Journal of machine Learning re- search , 2(Dec):139–154, 2001. 3",1
Conditional Gaussian Distribution Learning for Open Set Recognition,CVPR_2020,4,"Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. In International Conference on Learning Rep- resentations , 2014. 2,3",1
Conditional Gaussian Distribution Learning for Open Set Recognition,CVPR_2020,5,"Poojan Oza and Vishal M Patel. C2ae: Class conditioned auto-encoder for open-set recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, 2019. 3,5,7,8",1
High-dimensional Convolutional Networks for Geometric Pattern Recognition,CVPR_2020,1,"Peter J Huber. Robust statistics . Springer, 2011. 1,2",1
High-dimensional Convolutional Networks for Geometric Pattern Recognition,CVPR_2020,2,"Martin Ankerl. Robin hood hashing. https://github. com/martinus/robin-hood-hashing , 2019. 3",2
High-dimensional Convolutional Networks for Geometric Pattern Recognition,CVPR_2020,3,"Yasuhiro Aoki, Hunter Goforth, Rangaprasad Arun Srivat- san, and Simon Lucey. PointNetLK: Robust & efﬁcient point cloud registration using pointnet. In CVPR , 2019. 2",4
High-dimensional Convolutional Networks for Geometric Pattern Recognition,CVPR_2020,4,"Eric Brachmann, Alexander Krull, Sebastian Nowozin, Jamie Shotton, Frank Michel, Stefan Gumhold, and Carsten Rother. DSAC - differentiable RANSAC for camera local- ization. In CVPR , 2017. 1,2",7
High-dimensional Convolutional Networks for Geometric Pattern Recognition,CVPR_2020,5,"Eric Brachmann and Carsten Rother. Neural-guided RANSAC: Learning where to sample model hypotheses. In ICCV , 2019. 2",1
CPARR: Category-based Proposal Analysis for Referring Relationships,CVPR_2020,1,"Chen Gao, Yuliang Zou, and Jia-Bin Huang. ican: Instance- centric attention network for human-object interaction detec- tion. arXiv preprint arXiv:1808.10437 , 2018. 2",3
CPARR: Category-based Proposal Analysis for Referring Relationships,CVPR_2020,2,"Ranjay Krishna, Ines Chami, Michael Bernstein, and Li Fei- Fei. Referring relationships. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6867–6876, 2018. 1,2,5,6",4
CPARR: Category-based Proposal Analysis for Referring Relationships,CVPR_2020,3,"Yu-Wei Chao, Yunfan Liu, Xieyang Liu, Huayi Zeng, and Jia Deng. Learning to detect human-object interactions. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV) , pages 381–389. IEEE, 2018. 2",5
CPARR: Category-based Proposal Analysis for Referring Relationships,CVPR_2020,4,"Kan Chen, Jiyang Gao, and Ram Nevatia. Knowledge aided consistency for weakly supervised phrase grounding. In CVPR , 2018. 2",3
CPARR: Category-based Proposal Analysis for Referring Relationships,CVPR_2020,5,"Kan Chen, Rama Kovvuri, Jiyang Gao, and Ram Nevatia. Msrc: Multimodal spatial regression with semantic context for phrase grounding. International Journal of Multimedia Information Retrieval , 7(1):17–28, 2018. 2",4
Incremental Few-Shot Object Detection,CVPR_2020,1,"Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chel- lappa, and Ajay Divakaran. Zero-shot object detection. In ECCV , 2018. 3",5
Incremental Few-Shot Object Detection,CVPR_2020,2,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural net- works. In NeurIPS , 2012. 1",3
Incremental Few-Shot Object Detection,CVPR_2020,3,"Andreas Opelt, Axel Pinz, and Andrew Zisserman. Incre- mental learning of object detectors using a visual shape al- phabet. In CVPR , 2006. 3",3
Incremental Few-Shot Object Detection,CVPR_2020,4,"Sylvestre-Alvise Rebufﬁ, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classiﬁer and representation learning. In CVPR , 2017. 1",4
Incremental Few-Shot Object Detection,CVPR_2020,5,"Saif Alabachi, Gita Sukthankar, and Rahul Sukthankar. Cus- tomizing object detectors for indoor robots. In ICRA , 2019. 1",3
Orthogonal Convolutional Neural Networks,CVPR_2020,1,"M. Arjovsky, A. Shah, and Y . Bengio. Unitary evolution recurrent neural networks. In Proceedings of the Inter- national Conference on Machine Learning (ICML) , pages 1120–1128, 2016. 2,3",2
Orthogonal Convolutional Neural Networks,CVPR_2020,2,"J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. 3",2
Orthogonal Convolutional Neural Networks,CVPR_2020,3,"R. Balestriero and R. Baraniuk. Mad max: Afﬁne spline in- sights into deep learning. arXiv preprint arXiv:1805.06576 , 2018. 1,2",2
Orthogonal Convolutional Neural Networks,CVPR_2020,4,"R. Balestriero et al. A spline theory of deep networks. In Pro- ceedings of the International Conference on Machine Learn- ing (ICML) , pages 383–392, 2018. 1,2",2
Orthogonal Convolutional Neural Networks,CVPR_2020,5,"N. Bansal, X. Chen, and Z. Wang. Can we gain more from orthogonality regularizations in training deep cnns? In Ad- vances in Neural Information Processing Systems (NeurIPS) , pages 4266–4276, 2018. 1,2,3,5,6,8",2
Toward a Universal Model for Shape from Texture,CVPR_2020,1,"Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014. 7",2
Toward a Universal Model for Shape from Texture,CVPR_2020,2,"Project page: Toward a univeral model for shape from tex- ture,http://vision.seas.harvard.edu/sft/ .7",2
Toward a Universal Model for Shape from Texture,CVPR_2020,3,"Jonathan T. Barron and Jitendra Malik. Shape, illumination, and reﬂectance from shading. TPAMI , 2015. 5,6",2
Toward a Universal Model for Shape from Texture,CVPR_2020,4,"Urs Bergmann, Nikolay Jetchev, and Roland V ollgraf. Learn- ing texture manifolds with the periodic spatial GAN. In Pro- ceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 469–477. JMLR. org, 2017. 2, 4,5",3
Toward a Universal Model for Shape from Texture,CVPR_2020,5,"Maureen Clerc and St ´ephane Mallat. The texture gra- dient equation for recovering shape from texture. IEEE Transactions on Pattern Analysis and Machine Intelligence , 24(4):536–549, 2002. 2,7,8",1
SuperGlue: Learning Feature Matching with Graph Neural Networks,CVPR_2020,1,"Eliane Maria Loiola, Nair Maria Maia de Abreu, Paulo Os- waldo Boaventura-Netto, Peter Hahn, and Tania Querido. A survey for the quadratic assignment problem. European jour- nal of operational research , 176(2):657–690, 2007. 2",5
SuperGlue: Learning Feature Matching with Graph Neural Networks,CVPR_2020,2,"Phototourism Challenge, CVPR 2019 Image Matching Workshop. https://image-matching-workshop. github.io . Accessed November 8, 2019. 7",2
SuperGlue: Learning Feature Matching with Graph Neural Networks,CVPR_2020,3,"Relja Arandjelovi ´c and Andrew Zisserman. Three things ev- eryone should know to improve object retrieval. In CVPR , 2012. 6",1
SuperGlue: Learning Feature Matching with Graph Neural Networks,CVPR_2020,4,"Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Al- varo Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Ma- linowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational inductive biases, deep learn- ing, and graph networks. arXiv:1806.01261 , 2018. 2,3",11
SuperGlue: Learning Feature Matching with Graph Neural Networks,CVPR_2020,5,"Alexander C Berg, Tamara L Berg, and Jitendra Malik. Shape matching and object recognition using low distortion correspondences. In CVPR , 2005. 2",3
Multi-path Learning for Object Pose Estimation Across Domains,CVPR_2020,1,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European Conference on Computer Vi- sion (ECCV) , pages 801–818, 2018.",5
Multi-path Learning for Object Pose Estimation Across Domains,CVPR_2020,2,"Nikolaus Hansen, Youhei Akimoto, and Petr Baudis. CMA-ES/pycma on Github. Zenodo, DOI:10.5281/zenodo.2559634, Feb. 2019. 6",3
Multi-path Learning for Object Pose Estimation Across Domains,CVPR_2020,3,"Stefan Hinterstoisser, Cedric Cagniart, Slobodan Ilic, Peter Sturm, Nassir Navab, Pascal Fua, and Vincent Lepetit. Gra-dient response maps for real-time detection of textureless ob- jects. IEEE Transactions on Pattern Analysis and Machine Intelligence , 34(5):876–888, 2012. 2",7
Multi-path Learning for Object Pose Estimation Across Domains,CVPR_2020,4,"Vassileios Balntas, Andreas Doumanoglou, Caner Sahin, Juil Sock, Rigas Kouskouridas, and Tae-Kyun Kim. Pose Guided RGB-D Feature Learning for 3D Object Pose Estimation. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3856–3864, 2017. 2,3,4",6
Multi-path Learning for Object Pose Estimation Across Domains,CVPR_2020,5,"G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Software Tools , 2000. 5",2
Image Search with Text Feedback by Visiolinguistic Attention Learning,CVPR_2020,1,"Marin Ferecatu and Donald Geman. Interactive search for image categories by mental matching. In IEEE International Conference on Computer Vision , 2007. 1",1
Image Search with Text Feedback by Visiolinguistic Attention Learning,CVPR_2020,2,"Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation , 1997. 5",1
Image Search with Text Feedback by Visiolinguistic Attention Learning,CVPR_2020,3,"Adriana Kovashka, Devi Parikh, and Kristen Grauman. Whittlesearch: Image search with relative attribute feedback. InIEEE Conference on Computer Vision and Pattern Recog- nition , 2012. 1,2",3
Image Search with Text Feedback by Visiolinguistic Attention Learning,CVPR_2020,4,"Bryan Plummer, Hadi Kiapour, Shuai Zheng, and Robinson Piramuthu. Give me a hint! navigating image databases us- ing human-in-the-loop feedback. In IEEE Winter Conference on Applications of Computer Vision , 2019. 1",4
Image Search with Text Feedback by Visiolinguistic Attention Learning,CVPR_2020,5,"Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face recognition and clus- tering. In IEEE Conference on Computer Vision and Pattern Recognition , 2015. 5",3
Boundary-aware 3D Building Reconstruction from a Single Overhead Image,CVPR_2020,1,"Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Si- mon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building rome in a day. Communications of the ACM , 54(10):105–112, 2011. 1",7
Boundary-aware 3D Building Reconstruction from a Single Overhead Image,CVPR_2020,2,"Ke Wang, Craig Stutts, Enrique Dunn, and Jan-Michael Frahm. Efﬁcient joint stereo estimation and land usage clas- siﬁcation for multiview satellite data. In 2016 IEEE Win- ter Conference on Applications of Computer Vision (WACV) , pages 1–9. IEEE, 2016. 2",4
Boundary-aware 3D Building Reconstruction from a Single Overhead Image,CVPR_2020,3,2019 IEEE GRSS Data Fusion Contest. http: //www.grss-ieee.org/community/ technical-committees/data-fusion . Accessed on: 2019-03-15. 6,2
Boundary-aware 3D Building Reconstruction from a Single Overhead Image,CVPR_2020,4,"2d semantic labeling contest - potsdam. http: //www2.isprs.org/commissions/comm3/wg4/ 2d-sem-label-potsdam.html . Last modiﬁed: 2019-10-23, Accessed on: 2019-10-23. 6",2
Boundary-aware 3D Building Reconstruction from a Single Overhead Image,CVPR_2020,5,"2d semantic labeling contest - vaihingen. http: //www2.isprs.org/commissions/comm3/wg4/ 2d-sem-label-vaihingen.html . Last modiﬁed: 2019-10-23, Accessed on: 2019-10-23. 6",2
MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks,CVPR_2020,1,"Mart ´ın Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasserstein generative adversarial networks. In ICML , 2017.",3
MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks,CVPR_2020,2,"Mart ´ın Arjovsky and L ´eon Bottou. Towards principled meth- ods for training generative adversarial networks. CoRR , 2017.",1
MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks,CVPR_2020,3,"Emily L. Denton, Soumith Chintala, Arthur Szlam, and Rob Fergus. Deep generative image models using a laplacian pyramid of adversarial networks. In NIPS , 2015.",2
MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks,CVPR_2020,4,"Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. CoRR , 2016.",3
MSG-GAN: Multi-Scale Gradients for Generative Adversarial Networks,CVPR_2020,5,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high ﬁdelity natural image synthe- sis. In International Conference on Learning Representa- tions , 2019.",3
Dynamic Fluid Surface Reconstruction Using Deep Neural Network,CVPR_2020,1,"Adrian Constantin and Joachim Escher. Wave breaking for nonlinear nonlocal shallow water equations. Acta Mathemat- ica, 181(2):229–243, 1998. 2,5",1
Dynamic Fluid Surface Reconstruction Using Deep Neural Network,CVPR_2020,2,"David Eigen and Rob Fergus. Predicting depth, surface nor- mals and semantic labels with a common multi-scale convo- lutional architecture. In International Conference on Com- puter Vision , pages 2650–2658, 2015. 2,4",1
Dynamic Fluid Surface Reconstruction Using Deep Neural Network,CVPR_2020,3,"Ricardo Ferreira, Joao P Costeira, and Joao A Santos. Stereo reconstruction of a submerged scene. In Iberian Conference on Pattern Recognition and Image Analysis , pages 102–109, 2005. 1,2[15] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research , 32(11):1231– 1237, 2013. 2",3
Dynamic Fluid Surface Reconstruction Using Deep Neural Network,CVPR_2020,4,"Huaizu Jiang and Erik Learned-Miller. Face detection with the faster R-CNN. In International Conference on Automatic Face & Gesture Recognition , pages 650–657, 2017. 2",1
Dynamic Fluid Surface Reconstruction Using Deep Neural Network,CVPR_2020,5,"Jos Stam. Stable ﬂuids. In Siggraph , volume 99, pages 121– 128, 1999. 2",2
Constraint-Aware Importance Estimation for,CVPR_2020,1,"Ting-Wu Chin, Cha Zhang, and Diana Marculescu. Layer- compensated pruning for resource-constrained convolutional neural networks. arXiv preprint arXiv:1810.00518 , 2018.",3
Constraint-Aware Importance Estimation for,CVPR_2020,2,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in Neural Information Pro- cessing Systems (NeurIPS) , pages 91–99, 2015.",4
Constraint-Aware Importance Estimation for,CVPR_2020,3,"Zhuangwei Zhuang, Mingkui Tan, Bohan Zhuang, Jing Liu, Yong Guo, Qingyao Wu, Junzhou Huang, and Jinhui Zhu. Discrimination-aware channel pruning for deep neural net- works. In Advances in Neural Information Processing Sys- tems (NeurIPS) , pages 875–886, 2018.",8
Constraint-Aware Importance Estimation for,CVPR_2020,4,"Ariel Gordon, Elad Eban, Oﬁr Nachum, Bo Chen, Hao Wu, Tien-Ju Yang, and Edward Choi. Morphnet: Fast & simple resource-constrained structure learning of deep networks. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) , pages 1586–1595, 2018.",7
Constraint-Aware Importance Estimation for,CVPR_2020,5,"Yang He, Ping Liu, Ziwei Wang, Zhilan Hu, and Yi Yang. Filter pruning via geometric median for deep convolutional neural networks acceleration. In IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 4340– 4349, 2019.",5
Knowledge as Priors: Cross-Modal Knowledge Generalization,CVPR_2020,1,"Yuxiao Chen, Long Zhao, Xi Peng, Jianbo Yuan, and Dim- itris N. Metaxas. Construct dynamic graphs for hand gesture recognition via spatial-temporal attention. In BMVC , 2019.",5
Knowledge as Priors: Cross-Modal Knowledge Generalization,CVPR_2020,2,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML , pages 448–456, 2015.[18] Herve Jegou, Matthijs Douze, and Cordelia Schmid. Ham- ming embedding and weak geometric consistency for large scale image search. In ECCV , pages 304–317, 2008.",1
Knowledge as Priors: Cross-Modal Knowledge Generalization,CVPR_2020,3,"Gyeongsik Moon, Ju Yong Chang, and Kyoung Mu Lee. V2V-PoseNet: V oxel-to-voxel prediction network for accu- rate 3D hand and human pose estimation from a single depth map. In CVPR , pages 5079–5088, 2018.",3
Knowledge as Priors: Cross-Modal Knowledge Generalization,CVPR_2020,4,"Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Y . Ng. Multimodal deep learn- ing. In ICML , pages 689–696, 2011.",6
Knowledge as Priors: Cross-Modal Knowledge Generalization,CVPR_2020,5,"Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In CVPR , 2020.",3
The “Vertigo Effect” on Your Smartphone:,CVPR_2020,1,"Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang. Coherent semantic attention for image inpainting. The IEEE Inter- national Conference on Computer Vision (ICCV) , October 2019.",4
The “Vertigo Effect” on Your Smartphone:,CVPR_2020,2,"Ziwei Liu, Raymond A. Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala. Video frame synthesis using deep voxel ﬂow. The IEEE International Conference on Computer Vi- sion (ICCV) , Oct 2017.",2
The “Vertigo Effect” on Your Smartphone:,CVPR_2020,3,"Abhishek Badki, Orazio Gallo, Jan Kautz, and Pradeep Sen . Computational zoom: a framework for post–capture im- age composition. ACM Transactions on Graphics (TOG) , 36(4):46, 2017.",4
The “Vertigo Effect” on Your Smartphone:,CVPR_2020,4,"Jonathan T. Barron, Andrew Adams, YiChang Shih, and Car- los Hern´ andez. Fast bilateral–space stereo for synthetic defo- cus. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2015.",2
The “Vertigo Effect” on Your Smartphone:,CVPR_2020,5,"Marcelo Bertalmio, Guillermo Sapiro, Vincent Caselles , and Coloma Ballester. Image inpainting. Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’00) , pages 417–424, 2000.",4
ResDepth: Learned Residual Stereo Reconstruction,CVPR_2020,1,"K. Bittner, P. d’Angelo, M. K ¨orner, and P. Reinartz. DSM-to- LoD2: Spaceborne stereo digital surface model reﬁnement. Remote Sensing , 10(12), 2018.",2
ResDepth: Learned Residual Stereo Reconstruction,CVPR_2020,2,"M. Bleyer, C. Rhemann, and C. Rother. PatchMatch stereo- stereo matching with slanted support windows. BMVC , 2011.",2
ResDepth: Learned Residual Stereo Reconstruction,CVPR_2020,3,"J.R. Chang and Y .S. Chen. Pyramid stereo matching net- work. CVPR , 2018.",2
ResDepth: Learned Residual Stereo Reconstruction,CVPR_2020,4,"K. He, J. Sun, and X. Tang. Guided image ﬁltering. ECCV , 2010.",2
ResDepth: Learned Residual Stereo Reconstruction,CVPR_2020,5,"H. Hirschm ¨uller. Accurate and efﬁcient stereo processing by semi-global matching and mutual information. CVPR , 2005.",2
4D Association Graph for Realtime Multi-person Motion Capture,CVPR_2020,1,"Vasileios Belagiannis, Sikandar Amin, Mykhaylo Andriluka, Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pic- torial structures revisited: Multiple human pose estimation. TPAMI , 2016.",6
4D Association Graph for Realtime Multi-person Motion Capture,CVPR_2020,2,"Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir- shick. Mask r-cnn. In ICCV , 2017.",4
4D Association Graph for Realtime Multi-person Motion Capture,CVPR_2020,3,"Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Srinath Sridhar, Gerard Pons-Moll, and Christian Theobalt. Single-shot multi-person 3d pose estimation from monocular rgb. In 3DV, 2018. 1332",7
4D Association Graph for Realtime Multi-person Motion Capture,CVPR_2020,4,"George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros Gidaris, Jonathan Tompson, and Kevin Murphy. Person- lab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model. In ECCV , 2018.",6
4D Association Graph for Realtime Multi-person Motion Capture,CVPR_2020,5,"Robin J. Wilson. Introduction to graph theory, fourth edition, 1996.",2
Photosequencing of Motion Blur using Short and Long Exposures,CVPR_2020,1,"Christian Ledig, Lucas Theis, Ferenc Husz ´ar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. In IEEE Conference on Computer Vision and Pattern Recognition , 2017. 4",11
Photosequencing of Motion Blur using Short and Long Exposures,CVPR_2020,2,"Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo Sapiro, Wolfgang Heidrich, and Oliver Wang. Deep video deblurring for hand-held cameras. In IEEE Conference on Computer Vision and Pattern Recognition , 2017. 4",6
Photosequencing of Motion Blur using Short and Long Exposures,CVPR_2020,3,"Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji- aya Jia. Scale-recurrent network for deep image deblurring. InIEEE Conference on Computer Vision and Pattern Recog- nition , 2018. 2,7,8",5
Photosequencing of Motion Blur using Short and Long Exposures,CVPR_2020,4,"Moshe Ben-Ezra and Shree K Nayar. Motion deblurring us- ing hybrid imaging. In IEEE Conference on Computer Vision and Pattern Recognition , 2003. 2",1
Photosequencing of Motion Blur using Short and Long Exposures,CVPR_2020,5,"Ayan Chakrabarti. A neural approach to blind motion deblur- ring. In European Conference on Computer Vision , 2016. 2",2
Neural Contours: Learning to Draw Lines from 3D Shapes,CVPR_2020,1,"Pierre B ´enard and Aaron Hertzmann. Line drawings from 3D models: A tutorial. Foundations and Trends in Computer Grapics and Vision , 11(1-2), 2019. 1,2",1
Neural Contours: Learning to Draw Lines from 3D Shapes,CVPR_2020,2,"Angel X Chang et al. Shapenet: An information-rich 3d model repository. arXiv:1512.03012, 2015. 5",1
Neural Contours: Learning to Draw Lines from 3D Shapes,CVPR_2020,3,"Forrester Cole, Aleksey Golovinskiy, Alex Limpaecher, Heather Stoddart Barros, Adam Finkelstein, Thomas Funkhouser, and Szymon Rusinkiewicz. Where do people draw lines? ACM Trans. Graph. , 27(3), 2008. 2,4,5,6,8",7
Neural Contours: Learning to Draw Lines from 3D Shapes,CVPR_2020,4,"Doug DeCarlo. Depicting 3d shape using lines. In Proc. SPIE Human Vision and Electronic Imaging XVII , 2012. 2",2
Neural Contours: Learning to Draw Lines from 3D Shapes,CVPR_2020,5,"Doug DeCarlo, Adam Finkelstein, Szymon Rusinkiewicz, and Anthony Santella. Suggestive contours for conveying shape. ACM Trans. Graph. , 22(3), 2003. 2,3",4
Google Landmarks Dataset v2,CVPR_2020,1,"Y . Avrithis, Y . Kalantidis, G. Tolias, and E. Spyrou. Retrieving Landmark and Non-landmark Images from Community Photo Collections. In Proc. ACM MM , 2010.",2
Google Landmarks Dataset v2,CVPR_2020,2,"G. Grifﬁn, A. Holub, and P. Perona. Caltech-256 Object Category Dataset. Technical Report 7694, California Institute of Technology, 2007.",2
Google Landmarks Dataset v2,CVPR_2020,3,"A. Krizhevsky, I. Sutskever, and G. Hinton. Imagenet Classi- ﬁcation with Deep Convolutional Neural Networks. In Proc. NIPS , 2012.",2
Google Landmarks Dataset v2,CVPR_2020,4,"M. Teichmann, A. Araujo, M. Zhu, and J. Sim. Detect-to- Retrieve: Efﬁcient Regional Aggregation for Image Search. InProc. CVPR , 2019.",2
Google Landmarks Dataset v2,CVPR_2020,5,"C. Szegedy, S. Ioffe, V . Vanhoucke, and A. Alemi. Inception- v4, Inception-ResNet and the Impact of Residual Connections on Learning. In Proc. AAAI , 2017.",2
Large-Scale Object Detection in the Wild,CVPR_2020,1,"Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international confer- ence on computer vision(ICCV) , 2017.",7
Large-Scale Object Detection in the Wild,CVPR_2020,2,"Kaiming He, Ross Girshick, and Piotr Doll ´ar. Rethinking im- agenet pre-training. arXiv preprint arXiv:1811.08883 , 2018.",3
Large-Scale Object Detection in the Wild,CVPR_2020,3,"Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Con- ference on Computer Vision (ECCV) , pages 181–196, 2018.",8
Large-Scale Object Detection in the Wild,CVPR_2020,4,"Takuya Akiba, Tommi Kerola, Yusuke Niitani, Toru Ogawa, Shotaro Sano, and Shuji Suzuki. Pfdet: 2nd place solution to open images challenge 2018 object detection track. arXiv preprint arXiv:1809.00778 , 2018.",6
Large-Scale Object Detection in the Wild,CVPR_2020,5,"Matthew R Boutell, Jiebo Luo, Xipeng Shen, and Christo- pher M Brown. Learning multi-label scene classiﬁcation. Pattern recognition , 37(9):1757–1771, 2004.",4
Front2Back: Single View 3D Shape Reconstruction via Front to Back Prediction,CVPR_2020,1,"Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano- lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], 2015. 1,6,7",2
Front2Back: Single View 3D Shape Reconstruction via Front to Back Prediction,CVPR_2020,2,"Dror Aiger, Niloy J Mitra, and Daniel Cohen-Or. 4-points congruent sets for robust surface registration. ACM Transac- tions on Graphics (TOG) , 27(3), 2008. 4",3
Front2Back: Single View 3D Shape Reconstruction via Front to Back Prediction,CVPR_2020,3,"Matthew Berger, Andrea Tagliasacchi, Lee M. Seversky, Pierre Alliez, Joshua A. Levine, Andrei Sharf, and Cl ´audio T. Silva. State of the art in surface reconstruction from point clouds. In Eurographics , 2014. 3",3
Front2Back: Single View 3D Shape Reconstruction via Front to Back Prediction,CVPR_2020,4,"Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano- lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], 2015. 1,6,7",2
Front2Back: Single View 3D Shape Reconstruction via Front to Back Prediction,CVPR_2020,5,"Huiwen Chang, Jingwan Lu, Fisher Yu, and Adam Finkel- stein. Pairedcyclegan: Asymmetric style transfer for apply- ing and removing makeup. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) , 2018. 3",4
Structure-Preserving Super Resolution with Gradient Guidance,CVPR_2020,1,"David Berthelot, Thomas Schumm, and Luke Metz. Be- gan: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717 , 2017. 4",3
Structure-Preserving Super Resolution with Gradient Guidance,CVPR_2020,2,"David R. Martin, Charless C. Fowlkes, Doron Tal, and Jiten- dra Malik. A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics. In ICCV , pages 416–425, 2001. 5",2
Structure-Preserving Super Resolution with Gradient Guidance,CVPR_2020,3,"Mohammad Saeed Rad, Behzad Bozorgtabar, Urs-Viktor Marti, Max Basler, Hazim Kemal Ekenel, and Jean-Philippe Thiran. Srobb: Targeted perceptual loss for single image super-resolution. arXiv preprint arXiv:1908.07222 , 2019. 2",6
Structure-Preserving Super Resolution with Gradient Guidance,CVPR_2020,4,"Jian Sun, Zongben Xu, and Heung-Yeung Shum. Gradient proﬁle prior and its applications in image super-resolution and enhancement. TIP, 20(6):1529–1542, 2010. 2",3
Structure-Preserving Super Resolution with Gradient Guidance,CVPR_2020,5,"Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie-Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. InBMVC , 2012. 5",4
Vec2Face: Unveil Human Faces from their Blackbox Features in,CVPR_2020,1,"Thomas Brunner, Frederik Diehl, and Alois Knoll. Copy and paste: A simple but effective initialization method for black- box adversarial attacks. ArXiv , abs/1906.06086, 2019.",3
Vec2Face: Unveil Human Faces from their Blackbox Features in,CVPR_2020,2,"Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR , pages 4690–4699, 2019.",4
Vec2Face: Unveil Human Faces from their Blackbox Features in,CVPR_2020,3,"Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing robust adversarial examples. In ICML , 2018.",4
Vec2Face: Unveil Human Faces from their Blackbox Features in,CVPR_2020,4,"Shuyu Cheng, Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Improving black-box adversarial attacks with a transfer-based prior. ArXiv , abs/1906.06919, 2019.",5
Vec2Face: Unveil Human Faces from their Blackbox Features in,CVPR_2020,5,"Forrester Cole, David Belanger, Dilip Krishnan, Aaron Sarna, Inbar Mosseri, and William T. Freeman. Synthesiz- ing normalized faces from facial identity features. In CVPR , 2017.",6
Adversarial Fooling Beyond “Flipping the Label”,CVPR_2020,1,"A. Ganeshan, B. S. Vivek, and R. V . Babu. FDA: Feature disruptive attack. In The IEEE International Conference on Computer Vision (ICCV) , October 2019.",2
Adversarial Fooling Beyond “Flipping the Label”,CVPR_2020,2,"A. Kurakin, I. Goodfellow, and S. Bengio. Adversarial exam- ples in the physical world. arXiv preprint arXiv:1607.02533 , 2016.",2
Adversarial Fooling Beyond “Flipping the Label”,CVPR_2020,3,"A. Madry, A. Makelov, L. Schmidt, T. Dimitris, and A. Vladu. Towards deep learning models resistant to ad- versarial attacks. In International Conference on Learning Representations (ICLR) , 2018.",2
Adversarial Fooling Beyond “Flipping the Label”,CVPR_2020,4,"K. R. Mopuri, A. Ganeshan, and R. V . Babu. Generalizable data-free objective for crafting universal adversarial pertur- bations. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , 2018.",2
Adversarial Fooling Beyond “Flipping the Label”,CVPR_2020,5,"B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi ´c, P. Laskov, G. Giacinto, and F. Roli. Evasion attacks against machine learning at test time. In Joint European Confer- ence on Machine Learning and Knowledge Discovery in Databases , pages 387–402, 2013.",2
Focus on defocus: bridging the synthetic to real domain gap for depth estimation,CVPR_2020,1,"Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy S. J. Ren, and Xiaogang Wang. Learning monocular depth by distill- ing cross-domain stereo networks. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XI , pages 506–523, 2018.",4
Focus on defocus: bridging the synthetic to real domain gap for depth estimation,CVPR_2020,2,"Xiaoyang Guo, Hongsheng Li, Shuai Yi, Jimmy S. J. Ren, and Xiaogang Wang. Learning monocular depth by distill- ing cross-domain stereo networks. In Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, September 8-14, 2018, Proceedings, Part XI , pages 506–523, 2018.",4
Focus on defocus: bridging the synthetic to real domain gap for depth estimation,CVPR_2020,3,"Shir Gur and Lior Wolf. Single image depth estimation trained via depth from defocus cues. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019 , pages 7683–7692, 2019.",1
Focus on defocus: bridging the synthetic to real domain gap for depth estimation,CVPR_2020,4,"C. Hazirbas, S. G. Soyer, M. C. Staab, L. Leal-Taix ´e, and D. Cremers. Deep depth from focus. In Asian Conference on Computer Vision (ACCV) , December 2018.",2
Focus on defocus: bridging the synthetic to real domain gap for depth estimation,CVPR_2020,5,"Reza Mahjourian, Martin Wicke, and Anelia Angelova. Un- supervised learning of depth and ego-motion from monocu- lar video using 3d geometric constraints. In 2018 IEEE Con- ference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018 , pages 5667–5675, 2018.",3
Enhancing Cross-Task Black-Box Transferability of Adversarial Examples,CVPR_2020,1,Github repository for our code. https://github.com/ erbloo/dr_cvpr20 .2,2
Enhancing Cross-Task Black-Box Transferability of Adversarial Examples,CVPR_2020,2,Github repository for our evaluation data. https:// github.com/erbloo/dr_images_cvpr20 .6,2
Enhancing Cross-Task Black-Box Transferability of Adversarial Examples,CVPR_2020,3,Google Cloud Vision. Link .2,2
Enhancing Cross-Task Black-Box Transferability of Adversarial Examples,CVPR_2020,4,"ICDAR2017 Robust reading challenge on COCO-Text. Link .7,8",2
Enhancing Cross-Task Black-Box Transferability of Adversarial Examples,CVPR_2020,5,ImageNet Challenge 2017. Link .8,2
Online Deep Clustering for Unsupervised Representation Learning,CVPR_2020,1,"Christian Ledig, Lucas Theis, Ferenc Husz ´ar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo- realistic single image super-resolution using a generative adversarial network. In CVPR , pages 4681–4690, 2017. 2",11
Online Deep Clustering for Unsupervised Representation Learning,CVPR_2020,2,"Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Learning representations for automatic colorization. In ECCV , pages 577–593, 2016. 2",3
Online Deep Clustering for Unsupervised Representation Learning,CVPR_2020,3,"Xiaolong Wang and Abhinav Gupta. Unsupervised learning of visual representations using videos. In ICCV , pages 2794– 2802, 2015. 2",1
Online Deep Clustering for Unsupervised Representation Learning,CVPR_2020,4,"Ziwei Liu, Raymond A Yeh, Xiaoou Tang, Yiming Liu, and Aseem Agarwala. Video frame synthesis using deep voxel ﬂow. In ICCV , pages 4463–4471, 2017. 2",5
Online Deep Clustering for Unsupervised Representation Learning,CVPR_2020,5,"Aravindh Mahendran, James Thewlis, and Andrea Vedaldi. Cross pixel optical-ﬂow similarity for self-supervised learn- ing. In ACCV , pages 99–116, 2018. 2",3
DLWL: Improving Detection for Lowshot classes with Weakly Labelled data,CVPR_2020,1,"Xinlei Chen, Abhinav Shrivastava, and Abhinav Gupta. Neil: Extracting visual knowledge from web data. In Proceedings of the IEEE International Conference on Computer Vision , pages 1409–1416, 2013. 2",3
DLWL: Improving Detection for Lowshot classes with Weakly Labelled data,CVPR_2020,2,"Xuanyi Dong, Liang Zheng, Fan Ma, Yi Yang, and Deyu Meng. Few-example object detection with model communi- cation. IEEE transactions on pattern analysis and machine intelligence , 41(7):1641–1654, 2018. 2",5
DLWL: Improving Detection for Lowshot classes with Weakly Labelled data,CVPR_2020,3,"Aditya Arun, CV Jawahar, and M Pawan Kumar. Dissimi- larity coefﬁcient based weakly supervised object detection. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 9432–9441, 2019. 1,2,8",3
DLWL: Improving Detection for Lowshot classes with Weakly Labelled data,CVPR_2020,4,"Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei. What’s the point: Semantic segmentation with point supervision. In European conference on computer vision , pages 549–565. Springer, 2016. 4",4
DLWL: Improving Detection for Lowshot classes with Weakly Labelled data,CVPR_2020,5,"Hakan Bilen and Andrea Vedaldi. Weakly supervised deep detection networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2846– 2854, 2016. 1,8",1
CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture,CVPR_2020,1,"Fan Feng, Rosa HM Chan, Xuesong Shi, Yimin Zhang, and Qi She. Challenges in task incremental learning for assistive robotics. IEEE Access , 2019.",5
CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture,CVPR_2020,2,"Stefano Fusi, Patrick J Drew, and Larry F Abbott. Cascade models of synaptically stored memories. Neuron , 45(4):599– 611, 2005.",3
CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture,CVPR_2020,3,"Ronald Kemker and Christopher Kanan. Fearnet: Brain- inspired model for incremental learning. arXiv preprint arXiv:1711.10563 , 2017.",1
CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture,CVPR_2020,4,"German I Parisi, Ronald Kemker, Jose L Part, Christopher Kanan, and Stefan Wermter. Continual lifelong learning with neural networks: A review. Neural Networks , 2019. 9",5
CatNet: Class Incremental 3D ConvNets for Lifelong Egocentric Gesture,CVPR_2020,5,"Patrice Y Simard, David Steinkraus, John C Platt, et al. Best practices for convolutional neural networks applied to visual document analysis. In Icdar , volume 3, 2003.",4
Deep Parametric Shape Predictions using Distance Fields,CVPR_2020,1,"Gunilla Borgefors. Distance transformations in arbitrary di- mensions. Computer vision, graphics, and image processing , 27(3):321–345, 1984. 2",2
Deep Parametric Shape Predictions using Distance Fields,CVPR_2020,2,"Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repos- itory. Technical Report arXiv:1512.03012 [cs.GR], Stanford University — Princeton University — Toyota Technological Institute at Chicago, 2015. 2,8",2
Deep Parametric Shape Predictions using Distance Fields,CVPR_2020,3,"Samaneh Azadi, Matthew Fisher, Vladimir Kim, Zhaowen Wang, Eli Shechtman, and Trevor Darrell. Multi-content gan for few-shot font style transfer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , volume 11, page 13, 2018. 2,7",6
Deep Parametric Shape Predictions using Distance Fields,CVPR_2020,4,"Elena Balashova, Amit Bermano, Vladimir G. Kim, Stephen DiVerdi, Aaron Hertzmann, and Thomas Funkhouser. Learn- ing a stroke-based representation for fonts. CGF , 2018. 2",3
Deep Parametric Shape Predictions using Distance Fields,CVPR_2020,5,"Mikhail Bessmeltsev and Justin Solomon. Vectorization of line drawings via polyvector ﬁelds. ACM Transactions on Graphics (TOG) , 2019. 8",1
The 4th AI City Challenge,CVPR_2020,1,"Keval Doshi and Yasin Yilmaz. Fast unsupervised anomaly detection in trafﬁc videos. In Proc. CVPR Workshops , Seat- tle, WA, USA, 2020.",1
The 4th AI City Challenge,CVPR_2020,2,"Shuting He, Hao Luo, Weihua Chen, Miao Zhang, Yuqi Zhang, Fan Wang, Hao Li, and Wei Jiang. Multi-domain learning and identity mining for vehicle re-identiﬁcation. In Proc. CVPR Workshops , Seattle, WA, USA, 2020.",8
The 4th AI City Challenge,CVPR_2020,3,"Sangrok Lee, Eunsoo Park, Hongsuk Yi, and Sang Hun Lee. StRDAN: Synthetic-to-real domain adaptation network for vehicle re-identiﬁcation. In Proc. CVPR Workshops , Seattle, WA, USA, 2020.",4
The 4th AI City Challenge,CVPR_2020,4,"Ming-Ching Chang, Chen-Kuo Chiang, Chun-Ming Tsai, Yun kai Chang, Hsuan-Lun Chiang, Yu-An Wang, Shih-Ya Chang, Yun-Lun Li, Min-Shuin Tsai, and Hung-Yu Tseng. AI City Challenge 2020 – Computer vision for smart trans- portation applications. In Proc. CVPR Workshops , Seattle, WA, USA, 2020.",10
The 4th AI City Challenge,CVPR_2020,5,"Tsai-Shien Chen, Man-Yu Lee, Chih-Ting Liu, and Shao-Yi Chien. Viewpoint-aware channel-wise attentive network for vehicle re-identiﬁcation. In Proc. CVPR Workshops , Seattle, WA, USA, 2020.",4
D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features,CVPR_2020,1,"H. Deng, T. Birdal, and S. Ilic. Ppf-foldnet: Unsupervised learning of rotation invariant 3d local descriptors. In ECCV , pages 602–618, 2018. 2,6",2
D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features,CVPR_2020,2,"S. Salti, F. Tombari, and L. Di Stefano. Shot: Unique sig- natures of histograms for surface and texture description. In ECCV , 2010. 6",2
D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features,CVPR_2020,3,"K. M. Yi, E. Trulls, V . Lepetit, and P. Fua. Lift: Learned invariant feature transform. In ECCV , 2016. 2",2
D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features,CVPR_2020,4,"C. Choy, J. Gwak, and S. Savarese. 4d spatio-temporal con- vnets: Minkowski convolutional neural networks. In CVPR , 2019. 2",2
D3Feat: Joint Learning of Dense Detection and Description of 3D Local Features,CVPR_2020,5,"C. Choy, J. Park, and V . Koltun. Fully convolutional geomet- ric features. In ICCV , 2019. 2,5,6,7",2
Upgrading Optical Flow to 3D Scene Flow through Optical Expansion,CVPR_2020,1,"Berthold KP Horn, Yajun Fang, and Ichiro Masaki. Time to contact relative to a planar surface. In IEEE Intelligent Vehicles Symposium . IEEE, 2007. 2,6",3
Upgrading Optical Flow to 3D Scene Flow through Optical Expansion,CVPR_2020,2,"Hugh Christopher Longuet-Higgins and Kvetoslav Prazdny. The interpretation of a moving retinal image. Proceedings of the Royal Society of London. Series B. Biological Sciences , 208(1173):385–397, 1980. 7",1
Upgrading Optical Flow to 3D Scene Flow through Optical Expansion,CVPR_2020,3,"Ted Camus, David Coombs, Martin Herman, and Tsai-Hong Hong. Real-time single-workstation obstacle avoidance us- ing only wide-ﬁeld ﬂow divergence. In Proceedings of 13th International Conference on Pattern Recognition , volume 3, pages 323–330. IEEE, 1996. 2,4",4
Upgrading Optical Flow to 3D Scene Flow through Optical Expansion,CVPR_2020,4,"Simon Baker, Daniel Scharstein, JP Lewis, Stefan Roth, Michael J Black, and Richard Szeliski. A database and eval- uation methodology for optical ﬂow. IJCV , 2011. 4",6
Upgrading Optical Flow to 3D Scene Flow through Optical Expansion,CVPR_2020,5,"Daniel Barath. Recovering afﬁne features from orientation- and scale-invariant ones. In ACCV , 2018. 2,4",2
Learning Furniture Compatibility with Graph Neural Networks,CVPR_2020,1,"Xintong Han, Zuxuan Wu, Yu-Gang Jiang, and Larry S Davis. Learning fashion compatibility with bidirectional lstms. In Proceedings of the 25th ACM international con- ference on Multimedia , pages 1078–1086. ACM, 2017.",4
Learning Furniture Compatibility with Graph Neural Networks,CVPR_2020,2,"Amrita Saha, Megha Nawhal, Mitesh M Khapra, and Vikas C Raykar. Learning disentangled multimodal representations for the fashion domain. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV) , pages 557–566. IEEE, 2018.",4
Learning Furniture Compatibility with Graph Neural Networks,CVPR_2020,3,"Ruining He, Charles Packer, and Julian McAuley. Learning compatibility across categories for heterogeneous item rec- ommendation. In 2016 IEEE 16th International Conference on Data Mining (ICDM) , pages 937–942. IEEE, 2016.",3
Learning Furniture Compatibility with Graph Neural Networks,CVPR_2020,4,"Tse-Yu Pan, Yi-Zhu Dai, Wan-Lun Tsai, and Min-Chun Hu. Deep model style: Cross-class style compatibility for 3d fur- niture within a scene. In 2017 IEEE International Confer- ence on Big Data (Big Data) , pages 4307–4313. IEEE, 2017.",4
Learning Furniture Compatibility with Graph Neural Networks,CVPR_2020,5,"Antonio Rubio, LongLong Yu, Edgar Simo-Serra, and Francesc Moreno-Noguer. Multi-modal joint embedding for fashion product retrieval. In 2017 IEEE International Con- ference on Image Processing (ICIP) , pages 400–404. IEEE, 2017.",4
3D Photography using Context-aware Layered Depth Inpainting,CVPR_2020,1,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR , 2015. 6",1
3D Photography using Context-aware Layered Depth Inpainting,CVPR_2020,2,"Yael Pritch, Eitam Kav-Venaki, and Shmuel Peleg. Shift- map image editing. In CVPR , pages 151–158. IEEE, 2009. 2",3
3D Photography using Context-aware Layered Depth Inpainting,CVPR_2020,3,"Connelly Barnes, Eli Shechtman, Adam Finkelstein, and Dan B Goldman. Patchmatch: A randomized correspon- dence algorithm for structural image editing. In ACM Trans- actions on Graphics , volume 28, page 24, 2009. 2",4
3D Photography using Context-aware Layered Depth Inpainting,CVPR_2020,4,"Chris Buehler, Michael Bosse, Leonard McMillan, Steven Gortler, and Michael Cohen. Unstructured lumigraph ren- dering. In Proceedings of the 28th annual conference on Computer graphics and interactive techniques , 2001. 2",5
3D Photography using Context-aware Layered Depth Inpainting,CVPR_2020,5,"Weifeng Chen, Zhao Fu, Dawei Yang, and Jia Deng. Single- image depth perception in the wild. In NeurIPS , 2016. 3",4
Instance-aware Image Colorization,CVPR_2020,1,"Sergio Guadarrama, Ryan Dahl, David Bieber, Mohammad Norouzi, Jonathon Shlens, and Kevin Murphy. Pixcolor: Pixel recursive colorization. In BMVC , 2017. 1",6
Instance-aware Image Colorization,CVPR_2020,2,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. IJCV , 115(3):211–252, 2015. 1,2,3,5",11
Instance-aware Image Colorization,CVPR_2020,3,"Carl V ondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking emerges by col- orizing videos. In ECCV , 2018. 3",5
Instance-aware Image Colorization,CVPR_2020,4,"Tomihisa Welsh, Michael Ashikhmin, and Klaus Mueller. Transferring color to greyscale images. ACM TOG (Proc. SIGGRAPH) , 21(3):277–280, 2002. 1,3",3
Instance-aware Image Colorization,CVPR_2020,5,"L. Yatziv and G. Sapiro. Fast image and video colorization using chrominance blending. TIP, 15(5):1120–1129, 2006. 1,2",2
Modeling the Background for Incremental Learning in Semantic Segmentation,CVPR_2020,1,"Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE T-PAMI , 39(12):2481–2495, 2017. 2",3
Modeling the Background for Incremental Learning in Semantic Segmentation,CVPR_2020,2,"Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In ECCV , 2018. 2",5
Modeling the Background for Incremental Learning in Semantic Segmentation,CVPR_2020,3,"Francisco M Castro, Manuel J Mar ´ın-Jim ´enez, Nicol ´as Guil, Cordelia Schmid, and Karteek Alahari. End-to-end incre- mental learning. In ECCV , 2018. 1,2,3,4",5
Modeling the Background for Incremental Learning in Semantic Segmentation,CVPR_2020,4,"Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan- than, and Philip HS Torr. Riemannian walk for incremen- tal learning: Understanding forgetting and intransigence. In ECCV , 2018. 1,2,5,6",4
Modeling the Background for Incremental Learning in Semantic Segmentation,CVPR_2020,5,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE T-PAMI , 40(4):834–848, 2017. 1,2",5
Symbol Spotting on Digital Architectural Floor Plans Using a Deep,CVPR_2020,1,"Sergio Escalera, Alicia Forn ´es, Oriol Pujol, Josep Llad ´os, and Petia Radeva. Circular blurred shape model for mul- ticlass symbol recognition. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) , 41(2):497–506, 2011. 2",5
Symbol Spotting on Digital Architectural Floor Plans Using a Deep,CVPR_2020,2,"Shreya Goyal, Vishesh Mistry, Chiranjoy Chattopadhyay, and Gaurav Bhatnagar. BRIDGE: Building Plan Reposi- tory for Image Description Generation, and Evaluation. InProceedings of the International Conference on Document Analysis and Recognition (ICDAR) , pages 1071–1076. IEEE, 2019. 4",4
Symbol Spotting on Digital Architectural Floor Plans Using a Deep,CVPR_2020,3,"Herv ´e Locteau, S ´ebastien Adam, Eric Trupin, Jacques Labiche, and Pierre H ´eroux. Symbol spotting using full vis- ibility graph representation. In Proceedings of the Workshop on Graphics Recognition , pages 49–50, 2007. 3,6",5
Symbol Spotting on Digital Architectural Floor Plans Using a Deep,CVPR_2020,4,"Muhammad Muzzamil Luqman, Jean-Yves Ramel, Josep Llad ´os, and Thierry Brouard. Subgraph spotting through ex- plicit graph embedding: An application to content spotting in graphic document images. In Proceedings of the Inter- national Conference on Document Analysis and Recognition (ICDAR’11) , pages 870–874. IEEE, 2011. 3",4
Symbol Spotting on Digital Architectural Floor Plans Using a Deep,CVPR_2020,5,"Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. SSD: Single shot multibox detector. In Proceedings of the European Conference on Computer Vision (ECCV’16) , pages 21–37. Springer, 2016. 4",7
Universal Source-Free Domain Adaptation,CVPR_2020,1,"Mahsa Baktashmotlagh, Masoud Faraki, Tom Drummond, and Mathieu Salzmann. Learning factorized representations for open-set domain adaptation. In ICLR , 2019. 1",4
Universal Source-Free Domain Adaptation,CVPR_2020,2,"Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. InNeurIPS , 2007. 1",4
Universal Source-Free Domain Adaptation,CVPR_2020,3,"Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In CVPR , 2017. 2,3",5
Universal Source-Free Domain Adaptation,CVPR_2020,4,"P. P. Busto and J. Gall. Open set domain adaptation. In ICCV , 2017. 2",2
Universal Source-Free Domain Adaptation,CVPR_2020,5,"Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Michael I Jordan. Partial transfer learning with selective adversarial networks. In CVPR , 2018. 1,2,5",4
PnPNet: End-to-End Perception and Prediction with Tracking in the Loop,CVPR_2020,1,"Keni Bernardin, Alexander Elbs, and Rainer Stiefelhagen. Multiple object tracking performance metrics and evaluation in a smart room environment. In Sixth IEEE International Workshop on Visual Surveillance, in conjunction with ECCV , 2006. 6",3
PnPNet: End-to-End Perception and Prediction with Tracking in the Loop,CVPR_2020,2,"Hasith Karunasekera, Han Wang, and Handuo Zhang. Mul- tiple object tracking with attention to appearance, structure, motion and size. IEEE Access , 7:104423–104434, 2019. 2",3
PnPNet: End-to-End Perception and Prediction with Tracking in the Loop,CVPR_2020,3,"Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. So- cial lstm: Human trajectory prediction in crowded spaces. In CVPR , 2016. 2",6
PnPNet: End-to-End Perception and Prediction with Tracking in the Loop,CVPR_2020,4,"Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauf- feurnet: Learning to drive by imitating the best and synthe- sizing the worst. arXiv preprint arXiv:1812.03079 , 2018. 1, 2",3
PnPNet: End-to-End Perception and Prediction with Tracking in the Loop,CVPR_2020,5,"Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional siamese networks for object tracking. In ECCV , 2016. 5",5
MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask,CVPR_2020,1,"Junhwa Hur and Stefan Roth. Iterative residual reﬁnement for joint optical ﬂow and occlusion estimation. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 5754–5763, 2019. 1,3,5,11[14] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox. Flownet 2.0: Evolu- tion of optical ﬂow estimation with deep networks. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition , pages 2462–2470, 2017. 2,5,6",1
MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask,CVPR_2020,2,"Pengpeng Liu, Michael Lyu, Irwin King, and Jia Xu. Self- low: Self-supervised learning of optical ﬂow. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4571–4580, 2019. 1,3,6,11",4
MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask,CVPR_2020,3,"Christian Bailer, Kiran Varanasi, and Didier Stricker. Cnn- based patch matching for optical ﬂow with thresholded hinge embedding loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3250– 3259, 2017. 2",3
MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask,CVPR_2020,4,"Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, De- qing Sun, Sylvain Paris, and Hanspeter Pﬁster. Blind video temporal consistency. ACM Transactions on Graph- ics (TOG) , 34(6):196, 2015. 1",6
MaskFlownet: Asymmetric Feature Matching with Learnable Occlusion Mask,CVPR_2020,5,"Thomas Brox, Andr ´es Bruhn, Nils Papenberg, and Joachim Weickert. High accuracy optical ﬂow estimation based on a theory for warping. In European conference on computer vision , pages 25–36. Springer, 2004. 2",4
iTASK - Intelligent Trafﬁc Analysis Software Kit,CVPR_2020,1,"E. Hoffer and N. Ailon. Deep metric learning using triplet network. In SIMBAD , 2014.",2
iTASK - Intelligent Trafﬁc Analysis Software Kit,CVPR_2020,2,"P. Huang, R. Huang, J. Huang, R. Yangchen, Z. He, X. Li, and J. Chen. Deep feature fusion with multiple granularity for vehicle re-identiﬁcation. In IEEE Conference on Com- puter Vision and Pattern Recognition Workshops , June 2019.",2
iTASK - Intelligent Trafﬁc Analysis Software Kit,CVPR_2020,3,"H. Jung, M.-K. Choi, J. Jung, J.-H. Lee, S. Kwon, and W. Young Jung. Resnet-based vehicle classiﬁcation and lo- calization in trafﬁc surveillance systems. In IEEE Confer- ence on Computer Vision and Pattern Recognition Work- shops , July 2017.[14] M. Kampelm ¨uhler, M. G. M ¨uller, and C. Feichtenhofer. Camera-based vehicle velocity estimation from monocular video. Computer Vision Winter Workshop , 2018.",2
iTASK - Intelligent Trafﬁc Analysis Software Kit,CVPR_2020,4,"P. Khorramshahi, N. Peri, A. Kumar, A. Shah, and R. Chel- lappa. Attention driven vehicle re-identiﬁcation and unsu- pervised anomaly detection for trafﬁc understanding. In IEEE Conference on Computer Vision and Pattern Recog- nition Workshops , June 2019.",2
iTASK - Intelligent Trafﬁc Analysis Software Kit,CVPR_2020,5,"K. Marotirao Biradar, A. Gupta, M. Mandal, and S. Ku- mar Vipparthi. Challenges in time-stamp aware anomaly de- tection in trafﬁc videos. In IEEE Conference on Computer Vision and Pattern Recognition Workshops , June 2019.",2
A Transductive Approach for Video Object Segmentation,CVPR_2020,1,"where a smaller σ= 8is used when the temporal references are sampled locally and densely, and a larger σ= 21 is employed when the reference frames are distant. We ﬁnd this simple motion model to be effective for ﬁnding long- term dependencies. 6952 Methods Architecture Optical Proposal Tracking Re-ID DyeNet [ 29] ResNet 101 ✓ ✓ ✓ ✓ CNN-MRF [ 3] Deeplab ✓ ✗ ✗ ✗ PReMVOS [ 30] Deeplab-V3+ ✓ ✓ ✓ ✓ FEELVOS [ 41] Deeplab-V3+ ✗ ✗ ✗ ✗ STM [ 33] 2×ResNet-50 ✗ ✗ ✗ ✗ TVOS (ours) ResNet-50 ✗ ✗ ✗ ✗ Table 1: A brief overview of leading VOS methods with dependent modules for other related vision tasks. 3.4. Learning the appearance embedding We learn the appearance embedding in a data-driven fashion using a 2D convolutional neural network. The em- bedding aims to capture both short-term and long-term vari- ations due to motion, scale and deformations. The embed- ding is learned from the training data in which each frame from the video is annotated with the segmented object and the object identity. Given a target pixel xiand we consider all pixels in the prior frames as references. Denote fiandfjthe feature embeddings for pixel xiand a reference pixel xj. Then the predicted label ˆyiofxiis given by ˆyi=/summationdisplay jexp(fT ifj)/summationtext kexp(fT ifk)·yj, (7) where the reference indexes j,k span the temporal his- tory before the current frame. We show detailed ablations on how sampling the historical frames affects the learning quality. We optimize the embedding via a standard cross-entropy loss on all pixels in the target frame, L=−/summationdisplay ilogP(ˆyi=yi|xi). (8) 3.5. Implementation Details We use a ResNet-50 to train the embedding model. The convolution stride of the third and the fourth residual blocks is set to 1 to maintain a high-resolution output. We add one additional 1×1convolutional layer to project the feature to a ﬁnal embedding of 256 dimensions. The embedding model produces a feature with a total stride of 8. During training, we take the pretrained weights from the ImageNet model zoo, and ﬁnetune the model on the Davis 2017 [ 35] training set for 240 epochs and Youtube- VOS [ 46] for 30 epochs. We apply the standard augmen- tations of random ﬂipping and random cropping of size 256×256 on the input images. We use a SGD solver with an initial learning rate of 0.02and a cosine annealing scheduler. The optimization takes 16 hours on 4 Tesla P100 GPUs, with a batch size of 16, each containing 10 snippets from a video sequence. Without Motion With Motion Figure 4: The effect of our simple motion model. Distant frames have weaker spatial priors on the location of objects, thus reducing the drifting problem. During tracking, we extract features at the original im- age resolution of 480p. The results of each video frame are predicted sequentially online. 4. Results In this section, we ﬁrst describe our experimental set- tings and datasets. Then we show detailed ablations on how the transductive approach takes advantage of unlabeled structures in the temporal sequence to signiﬁcantly improve the performance. Results are conducted on various datasets to compare with the state-of-the-art. Finally, we discuss temporal stability and the relationship to optical ﬂow. Our method is abbreviated as TVOS in the result tables. 4.1. Experimental Setup Datasets. We evaluate our method on the Davis 2017 [ 35], and Youtube-VOS [ 46] datasets. Our model is trained on the respective training set and evaluated on the validation set. For Davis 2017, we also train our model on the combined train-val set, and submit the results on the testing set on the evaluation server. Davis 2017 contains 150 video sequences and it involves multiple objects with drastic deformations, heavy and pro- longed occlusions, and very fast motions. High-deﬁnition annotations are available for all frames in the training se- quences. Youtube-VOS is the largest dataset for this task to-date, containing 4453 training sequences and 474 valida- tion sequences. It captures a comprehensive collection of 94daily object categories. However, the frame rate of the video is much lower than videos in Davis (5 fps compared to 24 fps). Evaluation metrics. We use the standard evaluation metric of mean intersection over union (mIoU), averaged across objects and summed over all frames. The mIoU is evaluated on both the full objects (J measure) and only on the object boundaries (F measure). The global metric (G measure) is the average of the J and F measures. Youtube- 6953 1 reference 9 references9 references +spatialRange 40 +spatial 10 10 10 1030 30 30 3037 37 37 3758 58 58 5880 80 80 80Figure 5: Using dense long-range dependencies improves the tracking performance. The spatial term smooths the object boundaries, while long-term dependencies up to 40 frames help to re-detect objects. train / tracking1 frame3 frames9 framesuniform samplesparse samplesparse + motion 1 frame 55.8 60.4 63.4 63.8 64.0 64.3 3 frames 56.0 61.4 65.4 65.5 66.1 66.7 9 frames 60.7 63.4 68.6 68.6 69.0 69.9 uniform sample 55.8 60.2 64.4 65.0 65.1 65.3 sparse sample 59.9 62.9 66.2 67.2 68.5 68.6 Supervised 47.5 52.2 53.8 54.0 54.5 54.8 InstDisc[ 44]42.4 47.3 51.3 51.3 52.1 52.2 MoCo[ 18] 43.5 48.7 53.0 53.2 53.8 54.0 Table 2: Ablation study on the range of temporal depen- dencies and the simple motion component. The mean J measure on the Davis 2017 validation set is reported. See text for details. VOS also includes a separate measure of seen objects and unseen objects to measure the generalization ability. In Sec- tion4.4, we provide a discussion of temporal stability. 4.2. Ablation Study Dense local and global dependencies. While most prior works focus on optimizing single frame models, the key idea of this paper is to build dense long-term mod- els over the spatio-temporal volume. In Table 2, we sum- marize the effect of such long-term potentials which cap- ture both local and global dependency. Each row is an ap- pearance embedding model trained with different reference frame sampling strategies. Each column corresponds to a tracking sampling strategy. We study the following set- tings: one reference frame preceding the target frame, 3consecutive frames preceding the target frame, 9 consec- utive frames preceding the target frame, uniform sampling of 9 frames in the preceding 40 frames, and our sparse sam- pling of 9 frames in the preceding 40 frames as in Figure 3. We ﬁnd that tracking over a longer term generally improves the performance, and denser sampling near the target frame is helpful. For learning the appearance embedding, training with 9 consecutive frames produces the best results, while longer ranges do not always lead to improvements. This may be due to very long ranges covering almost the entire video reduces the variations in the dataset, which leads to worse generalization for training. In Figure 5, we show some qualitative examples for long range tracking. Using 9 consecutive frames yields more sta- ble predictions than using only the previous frame. Adding the spatial term smooths the object boundaries. A long range of 40 frames enables the model to re-detect objects after heavy occlusions. Transferred representations. In the last rows of Ta- ble2, we also test the tracking performance for models pretrained on ImageNet but without further training on the DA VIS dataset. The transferred ImageNet model obtains a meanJmeasure of 54.8%, which is actually better than some prior methods [ 47,10] trained with additional Davis data. Also, even an unsupervised pretrained model on im- ages obtains performance competitive to network modula- tion [ 47] using our transductive inference algorithm. Two recent unsupervised pretrained models on ImageNet are in- vestigated [ 44,18]. Since no domain-speciﬁc training is involved for the appearance embedding, the evaluation of transferred representations clearly validates the effective- 6954 Methods FT J F J &F Speed OnA VOS [ 42] ✓ 61.0 66.1 63.6 0.08 DyeNet [ 29] ✓ 67.3 71.0 69.1 0.43 CNN-MRF [ 3] ✓ 67.2 74.2 70.7 0.03 PReMVOS [ 30] ✓ 73.9 81.7 77.8 0.03 Modulation [ 47] ✗ 52.5 57.1 54.8 3.57 FA VOS [ 10] ✗ 54.6 61.8 58.2 0.83 VideoMatch [ 22] ✗ 56.5 68.2 62.4 2.86 RGMP [ 45] ✗ 64.8 68.8 66.7 3.57 FEELVOS [ 41] ✗ 65.9 72.3 69.1 1.96 STM [ 33] ✗ 69.2 74.0 71.6 6.25 STM [ 33]+Pretrain ✗ 81.7 79.2 84.3 6.25 TVOS ✗ 69.9 74.7 72.3 37 Table 3: Quantitative evaluation on the Davis 2017 valida- tion set. FT denotes methods that perform online training. ness of dense long-term modeling. The simple motion prior. As a weak spatial prior for modeling the dependency between distant frames, our sim- ple motion model reduces noise from the model predictions and leads to about 1%improvement. Figure 4displays two concrete examples. More complicated motion models, such as a linear motion model [ 1], may be even more effective. 4.3. Quantitative Results In Table 1, we ﬁrst give a brief overview of the cur- rent leading methods, including those that use ﬁrst-frame ﬁnetuning (CNN-MRF [ 3], DyeNet [ 29], PReMVOS [ 30]) and those that do not (FEELVOS [ 41], STM [ 33] and our TVOS). For DyeNet and PReMVOS, their sub-modules are learned on dedicated datasets such as optical ﬂow on Flying Chairs, object proposal on MSCOCO, and object segmen- tation on PASCAL VOC. Since Davis is much smaller than the large-scale datasets, it remains unknown how much of the gains can be attributed to knowledge transfer or to the methods themselves. Therefore, the mentioned methods are not directly comparable with our method. FEELVOS, STM and ours are much simpler, as they do not rely on additional modules for this problem. STM additionally requires heavy pretraining on large-scale image datasets. It is also important to note that for PreMVOS, DyeNet, CNN-MRF, they are not able to run tracking in an online fashion . They use information from future frames to sta- bilize prediction for the target frame. Also, instead of us- ing the ﬁrst frame from the given video for training, they use the ﬁrst frames from the entire test-dev set for train- ing. Propagation-based methods are able to track objects sequentially online. DA VIS 2017. We summarize our results on the Davis 2017 validation set in Table 3, and on the Davis 2017 test- dev set in Table 4. On the validation set, our method per- forms slightly better than STM [ 33] under the same amountMethods FT J F J &F Speed OnA VOS [ 42] ✓ 53.4 59.6 56.5 0.08 DyeNet [ 29] ✓ 65.8 70.5 68.2 0.43 CNN-MRF [ 3] ✓ 64.5 70.5 67.5 0.02 PReMVOS [ 30] ✓ 67.5 75.7 71.6 0.02 RGMP [ 45] ✗ 51.4 54.4 52.9 2.38 FEELVOS [ 41] ✗ 51.2 57.5 54.4 1.96 TVOS ✗ 58.8 67.4 63.1 37 Table 4: Quantitative evaluation on the Davis 2017 test-dev set. FT denotes methods that perform online training. Methods OverallSeen Unseen J F J F RGMP [ 45] 53.8 59.5 - 45.2 - OnA VOS [ 42] 55.2 60.1 62.7 46.6 51.4 RVOS [ 40] 56.8 63.6 67.2 45.5 51.0 OSVOS [ 7] 58.8 59.8 60.5 54.2 60.7 S2S [ 46] 64.4 71.0 70.0 55.5 61.2 PreMVOS [ 30] 66.9 71.4 75.9 56.5 63.7 STM [ 33]+Pretrain 79.4 79.7 84.2 72.8 80.9 TVOS 67.8 67.1 69.4 63.0 71.6 TVOS (from DA VIS) 67.4 66.7 69.8 62.5 70.6 Table 5: Quantitative evaluation on the Youtube-VOS vali- dation set. of training data, while surpassing other propagation-based methods which do not need ﬁne-tuning, by 4%for mean J and3%for mean J&F. In comparison to ﬁnetuning based methods, our TVOS also outperforms DyeNet and CNN- MRF by2%while being signiﬁcantly simpler and faster. We train our model on the combined training and vali- dation set for evaluating on the test-dev set. We ﬁnd that there is a large gap of distribution between the Davis 2017 test-dev and validation sets. Heavy and prolonged occlu- sions among objects belonging to the same category are more frequent in the test-dev, which favors methods with re-identiﬁcation modules. As a result, we are 4−5%lower than DyeNet and CNN-MRF on the test-dev set. FEELVOS is even more negatively affected, performing 8%lower than ours in terms of mean J&F. STM [ 33] does not provide an evaluation on the test set. Youtube-VOS. We summarize the results on youtube- VOS validation set in Table 5. Ours surpasses all prior works except STM [ 33], which relies on heavy pretraining on a variety of segmentation datasets such as saliency detec- tion and instance segmentation. Without pretraining, STM obtains a comparable result of 68.1%. We also test the gen- eralization ability of our model trained on DA VIS train-val and test on Youtube-VOS val. The transferred model shows great generalization ability with an overall score of 67.4% Speed analysis. During tracking, we cache the appear- ance embeddings for a history up to 40 frames. Inference 6955 30 55 65 75 30 55 65 75PReMVOS IoUOurs IoU Time TimeFigure 6: Per-frame IoU over time for PreMVOS and our method on a example video sequences from the DA VIS validation set. PreMVOS switches object identities frequently, while our predictions are temporally smooth. The color of each IoU curve matches its corresponding object segment. Overlaid Image Pair FlowNet2 Ours Ours + Smoothness Constraint Figure 7: An example of optical ﬂow computed from our model compared to FlowNet2. per frame thus only involves a feed-forward pass of the tar- get frame through the base network, and an additional dot product of target embeddings to prior embeddings. Compu- tation is also constant of any number of objects. This makes our algorithm extremely fast, with a runtime of 37frames per second on a single Titan Xp GPU. Figure 1compares current algorithms on their trade-off between speed and per- formance. Ours is an order of magnitude faster than prior methods, while achieving the results comparable to state- of-the-art methods. 4.4. Discussions Temporal stability. Temporal stability is often a desir- able property in video object segmentation, as sharp incon- sistencies may be disruptive to downstream video analy- sis. However, temporal stability is typically not included as an evaluation criterion. Here, we give qualitative exam- ples showing the difference in temporal stability between our model and the state-of-the-art PreMVOS [ 30]. In Figure 6, we show examples of per-frame evaluation along video sequences. Although the state-of-the-art inte- grates various temporal smoothing modules, such as opti-cal ﬂow, merging and tracking, we observe the detection- based method to be prone to noise. For example, objects are lost suddenly, or being tagged with a different identity. Our method, on the other hand, makes temporally consis- tent predictions. Does our model learn optical ﬂow? Our method learns a soft mechanism for associating pixels in the target frame with pixels in the history frames. This is similar to opti- cal ﬂow where hard correspondences are computed between pixels. We examine how much our learned model aligns with optical ﬂow. We take two adjacent two frames and calculate the op- tical ﬂow from our model as ∆di=/summationtext jsij∆dij, where sijis the normalized similarity, and ∆dijis the displace- ment between i,j. Figure 7shows an example visualiza- tion of the ﬂow. Compared to the optical ﬂow computed by FlowNet2 [ 23], our ﬂow makes sense on objects that would be segmented, but is much more noisy on the background. We have further added a spatial smoothness constraint on the computed optical ﬂow for jointly learning the embed- dings, as widely used for optical ﬂow estimation [ 15,24]. We observe that the constraint smooths the optical ﬂow on the background, but fails to regularize the model for track- ing. Adding the term consistently hurts the performance of video object segmentation. 5. Conclusion We present a simple approach to semi-supervised video object segmentation. Our main insight is that much more unlabeled structure in the spatio-temporal volume can be exploited for video object segmentation. Our model ﬁnds such structure via transductive inference. The approach is learned end-to-end, without the need of additional modules, additional datasets, or dedicated architectural designs. Our vanilla ResNet50 model achieves competitive performance with a compelling speed of 37 frames per second. We hope our model can serve as a solid baseline for future research. 6956 References",1
A Transductive Approach for Video Object Segmentation,CVPR_2020,2,"S. W. Oh, J.-Y . Lee, N. Xu, and S. J. Kim. Video ob- ject segmentation using space-time memory networks. arXiv preprint arXiv:1904.00607 , 2019. 3,5,7",2
A Transductive Approach for Video Object Segmentation,CVPR_2020,3,"K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick. Momen- tum contrast for unsupervised visual representation learning. arXiv preprint arXiv:1911.05722 , 2019. 6",2
A Transductive Approach for Video Object Segmentation,CVPR_2020,4,"Z. Wu, Y . Xiong, S. X. Yu, and D. Lin. Unsupervised feature learning via non-parametric instance discrimination. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3733–3742, 2018. 6",2
A Transductive Approach for Video Object Segmentation,CVPR_2020,5,"J. Luiten, P. V oigtlaender, and B. Leibe. Premvos: Proposal- generation, reﬁnement and merging for video object segmen- tation. arXiv preprint arXiv:1807.09190 , 2018. 1,2,3,5,7, 8",2
"TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and",CVPR_2020,1,"Tuanfeng Y . Wang, Duygu Ceylan, Jovan Popovic, and Niloy J. Mitra. Learning a shared shape space for multimodal 7374 garment design. ACM Trans. Graph. , 37(6):203:1–203:13, 2018. 2,3,4",2
"TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and",CVPR_2020,2,https://www.marvelousdesigner.com/. 6,2
"TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and",CVPR_2020,3,"Michal Aharon, Michael Elad, and Alfred Bruckstein. K- svd: An algorithm for designing overcomplete dictionaries for sparse representation. IEEE Transactions on signal pro- cessing , 54(11):4311–4322, 2006. 6",3
"TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and",CVPR_2020,4,"Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, and Gerard Pons-Moll. Learning to re- construct people in clothing from a single RGB camera. InIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. 3",5
"TailorNet: Predicting Clothing in 3D as a Function of Human Pose, Shape and",CVPR_2020,5,"Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-Moll. Video based reconstruction of 3D people models. In IEEE Conf. on Computer Vision and Pattern Recognition , 2018. 3",5
Label Distribution Learning on Auxiliary Label Space Graphs,CVPR_2020,1,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016.",4
Label Distribution Learning on Auxiliary Label Space Graphs,CVPR_2020,2,"Masajiro Iwasaki and Daisuke Miyazaki. Optimization of indexing based on k-nearest neighbor graph for prox- imity search in high-dimensional data. arXiv preprint arXiv:1810.07355 , 2018.",1
Label Distribution Learning on Auxiliary Label Space Graphs,CVPR_2020,3,"Yuncheng Li, Jianchao Yang, Yale Song, Liangliang Cao, Jiebo Luo, and Li-Jia Li. Learning from noisy labels with distillation. In Proceedings of the IEEE International Con- ference on Computer Vision , pages 1910–1918, 2017.",6
Label Distribution Learning on Auxiliary Label Space Graphs,CVPR_2020,4,"Caifeng Shan, Shaogang Gong, and Peter W McOwan. Fa- cial expression recognition based on local binary patterns: A comprehensive study. Image and vision Computing , 27(6):803–816, 2009.",3
Label Distribution Learning on Auxiliary Label Space Graphs,CVPR_2020,5,"Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Abhi- nav Gupta, and Serge Belongie. Learning from noisy large- scale datasets with minimal supervision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 839–847, 2017.",6
Visual 3D Reconstruction and Dynamic Simulation of Fruit Trees for Robotic,CVPR_2020,1,"rest of the tree, as in [ 32,27] and the references therein. Fur- thermore, an interesting feature of these approaches is that they can be parameterized using the stiffness and damping coefﬁcients, which allowed to model various structures in",3
Visual 3D Reconstruction and Dynamic Simulation of Fruit Trees for Robotic,CVPR_2020,2,". As an alternative, trees have also been modeled as series of rods interconnected, as presented in [ 10]. Finally, it is important to note that the majority of meth- ods for tree modeling discussed here are not originally quantitatively compared to the real counterparts. This ob- servation is coherent with the fact that most of them are computer graphics applications whose aim is to create vir- tual environments that visually render real looking trees. However, when the aim is to manipulate the tree, the ac- curacy of the model obtained is an important feature to be evaluated. 3. Materials and Methods The proposed approach comprises two stages: the three dimensional modeling of the tree and its dynamical simula- tion. For the ﬁrst stage, we implemented two methods from the state of the art: the space colonization algorithm [ 30] and the Laplacian based contraction [ 6]. We consider that both are purely visual since the sensor employed to get the real data is a camera system, as will be described in the fol- lowing Section. These two algorithms were chosen mainly for the following three reasons. First, they are well known methods to skeletonize and reconstruct branching structures [28,1,22]. Additionally, they have proven effectiveness in various scenarios and datasets [ 8]. Finally, we consider that they provide a complementary way to model the tree struc- ture in the case of a tree with foliage, as will be discussed in Section 4.1. The dynamic simulation was required to be ﬂexible in the sense of allowing joints with elastic stiffness with 3 degrees of freedom. Furthermore, the computation of the movement required a robust and efﬁcient method so later it can be used for a robot to estimate it movement or actu- ate on the tree. While known physics engines (e.g., bullet, open dynamics engine, PhysX) provide these two charac- teristics, they seemed inadequate to explicitly model and tune the elastic stiffness in each degree of freedom. For these reasons, we implemented the model described in [ 27], which is a computer graphics application for botanical tree animation that suits better for the purposes of this work. Figure 1summarizes the pipeline we followed to obtain the geometric structure of a fruit tree and estimate its dy- namics. 3.1. Development Datasets and Hardware We used three types of datasets to evaluate the quality of the 3D reconstruction and the feasibility of the dynamic modeling for real trees. The development-artiﬁcial dataset consisted in 4 freely available skeletons (with their respec- tive point clouds) of different shaped and sized trees, pro- vided in [ 9]. The skeletons were considered as ground truth for all of them. The development-real dataset comprised a set of 14 point clouds from real grape vines, acquired in ﬁeld with a semi-autonomous platform equipped with dual stereo cameras and a gantry system (Fig. 2). We basically used the Iterative Closest Point (ICP) algorithm and rigid transformations to align the point clouds produced by thetop and bottom cameras. As this paper mainly focus on the visual 3D reconstruction and dynamic simulation of tree fruits, the design of the image acquisition system and the platform are targeted for another independent publication. The third dataset corresponded to laboratory tests with a dormant vine, which was mainly used to evaluate the prac- tical applicability of the movement estimation in real sce- narios. 3.2. Image-based approach - Space colonization al- gorithm The Space Colonization Algorithm (SCA) is an itera- tive method to build tree structures based on the concept of competition for space. It takes as input Nattraction points (which can be the leaves or buds of the tree) and the tree root or other pivot points acting as prior nodes (joints). Then, the algorithm builds the branching structure of the tree by attaching new links based on their distance and relative po- sition to the nearest attraction point(s). A deeper description of the algorithm, as well as some interesting modiﬁcations can be found in the original work [ 30]. The natural interpretation of the attraction points from a practical point of view is to consider them as leaves. How- ever, we employed other possibility: detect the buds of the branches and use them as attraction points. In this work, the detection procedure relied on a Faster-RCNN, which is a very popular deep neural architecture for object recogni- tion [ 29]. The consecutive incoming images from top and bottom stereo cameras are sent through the object detector trained to detect dormant buds in 2D images. Using the optimized transformation of the registered point cloud pro- duced by the ICP algorithm (see Sec. 3.1), the dormant buds were ﬁltered and merged. In this case, it was neces- sary to ﬁnd appropriate values for the growing parameters that force the branches to reach the buds. Furthermore, in some cases (specially for the vines) one additional pivot was given as input for the tree to obtain a structure similar to the real. In this case, the algorithm starts trying to reach only the pivot for later start growing using the other attraction points. 3.3. Point cloud based approach - Laplacian-based contraction Among the diverse algorithms for tree skeletonization, we employed the Laplacian-based contraction (LBC) [ 6]. It is a geometric approach that collapses the input point cloud maintaining its global shape. The main idea is to obtain a new point cloud with minimal volume using a repetitive Laplacian smoothing process. At each iteration i, the point cloudPis contracted by ﬁnding the set of contracted points Figure 1: Workﬂow of the proposed approach to digitize and simulate the movement of fruit trees using A) the space colo- nization and B) a skeletonization algorithms. Figure 2: A semi-autonomous platform for image acquisi- tion. The imaging system consists of two stereo cameras attached to a gantry. The stereo cameras acquire seven dif- ferent top and bottom views of the vine at a regular interval along the gantry. P′that solve the linear system: /bracketleftbigg Wi LLi Wi H/bracketrightbigg P′i=/bracketleftbigg0 Wi HPi/bracketrightbigg (1) whereWL,WHare weighting factors to control the con- traction and attraction constraints, and Lis the Laplacian operator with cotangent weights. For the next step, the new set of points found in the current iteration, P′i, is assigned toPi+1while the weights and the Laplacian operator are updated to account for the ”new shape” of Pi+1. This pro- cess repeats until a convergence test is passed. When carefully designing the weighting factors, this overall approach provides efﬁciency and stability when con- tracting point clouds. As the resulting set of points is a thin structure that approximates the skeleton of the original point cloud, it is further necessary to connect the structure ensur-ing to faithfully represent the topology of the original struc- ture (a tree in our case). 3.3.1 Topological connection Various methods have been proposed to convert a point cloud to a skeleton [ 13,31]. However, we used a custom method to connect the points obtained from the LBC that aims to resemble the characteristic topological structure of a tree. We developed this method is speciﬁc to account for the hierarchical pattern that the dynamic simulation requires. This is, starting from the root, the subsequent links require a series of parent-child connections that preserves the tree shape to properly propagate the movement when a force is applied to any branch. Firstly, the points are organized in ascending order ac- cording to their zcoordinates, obtaining a point cloud P={pi}. Denoting a node in the interconnected graph asgi=/braceleftbig pi,αi k/bracerightbig , whereαi kis its parent node with index k, we use the procedure summarized in Algorithm 1. The main idea is to join the children nodes with their par- ents based on its Euclidean distance. If a node has multiple parents during the connection, the one with less distance adopts that node as child. However, there may be cases when this distance test is not conclusive (lines 12 and 17). In this circumstance we use the observation that the con- nection direction is inﬂuenced by the spatial distribution of the points along the branches or the trunk. The neighbors oriented search (lines 13 and 18) is used to implement this idea as follows: we ﬁrst calculate the directional vectors from both disputing parents to the current node. Two imag- inary cylinders oriented in these directions, with ﬁxed ra- diusρand length Lcyldepending on the dimensions of the tree are then built, as shown in the third row of Fig 3c. The points lying inside each cylinder are counted and the one with majority is chosen. Finally, the parent associated to that cylinder keeps the child. The overall procedure, is summarized in Fig. 3as fol- Algorithm 1 Topological tree connection 1:MakeG′(gi) ={pi,∅} ∀pi∈P 2:while nodes without a parent exist do 3: Increase index i 4: Find nodes gj:|pi−pj|< r 5: whilegjis not found doincreaser 6: For allgj:αj=∅, connect pjtopiand make gj=/braceleftBig pj,αj i/bracerightBig . 7: if∃g′ j=gj:αj k/ne}ationslash=∅then 8: forallg′ jdo: 9: dcurr=|pj−pi|,dprev=|pj−αi k| 10: ifdcurr≤dprev then 11: ifdcurr dprev< dthres then 12: Change the parent αj k=αj i 13: else 14: Do neighbor oriented search 15: else 16: ifdprev dcurr< dthres then 17: Keep the previous parent αj k 18: else 19: Do neighbor oriented search 20: MakeG=G′ 21: RemovegifromG′. lows: the ﬁrst row shows a simple case when a multi parent node is found. The parent is assigned based on the shorter distance to the disputing node. Similarly, the second row shows a multi-parent node in a branching point, where the dispute is solved based on the distance. The third row shows the more complex case where the distance test is not con- clusive and the neighbor oriented search is performed. As the orange cylinder has more points inside the original par- ent is ﬁnally chosen. This process is repeated until all the points have a parent assigned. It is noteworthy that the pre- sented method requires three design parameters: the radius to make the initial search r, the ratio to compare the dis- tances between disputing parents dthres and the radius of the oriented cylinders ρ. While its length can be also a pa- rameter we choose to make it function of the tree height to avoid an over-parametrization of the method. 3.4. Dynamic modeling The connected structure from the SC and LBC algo- rithms are just skeletal representations of the tree. To con- vert them into 3D structures capable to move and interact with other objects, we assume that each link has a cylindri- cal shape, whose length is known, but the radius changes depending of the tree structure. This change was modeled making the radius a linear function of the number of descen- dants of each joint (this accounts for the complete lineage, Figure 3: Topological connection applied to three different scenarios, increasing in complexity. The red dot depicts the current node gi, the blue lines are the current connections based on the distance, and the yellow lines show the con- ﬂicting connection with previous parents. not only the current children). The inertial properties of each link were then included by estimating the mass using the volume of each cylinder and the density of the wood for the tree under study. The movement of the tree was calcu- lated by using the four steps originally proposed in [ 27] and described as follows: •External force computation: For each rigid body (cylinder), external forces and torques due to gravity, wind ﬁelds or other sources are calculated. •Composite body update: The inertial characteristics of a link of the tree are at some extent affected by the others. Therefore, the total external forces and torques, mass, and inertia tensor (in world reference frame) of each rigid body are calculated in base of the branching structure of its children and its own. These calculations are backpropagated from the outer links to the root in order to update all the tree structure. •Analytic spring evolution: The joint space conﬁgura- tion is then calculated by solving the equation: ˜I¨θ+βK˙θ+Kθ=˜τ (2) whereθis the joint position, ˜Iis the local composite inertial tensor, Kis a diagonal 3×3matrix whose elements correspond to the rotational stiffness of the joint,βKaccounts for the damping coefﬁcients and ˜τ is the local composite external torque. •Rigid body state update: The state vector of each link [mi,xi,vi,ai,Ii,Ri,ωi,αi]is expressed in maximal coordinates and includes its composite mass, position, velocity, acceleration, composite inertia, rotation ma- trix, angular velocity and angular acceleration, respec- tively. Once the solution of Eqn.( 2) is found for all the links, their state vectors are updated, starting from the root. While for a virtual environment simulation, tuning K to obtain different spring behaviors of the joints would be enough, the modeling real fruit trees requires a more ana- lytical approach. For that reason, we used the deﬁnition of Kdescribed in [ 21]. In this work, each of the elements of Kis related with the area moment of inertia of the link, and structural constants (available for various materials) such as the Young modulus and the Poisson’s ratio. Thus, we have an a prior knowledge of K, which can be used as an initial condition when performing identiﬁcation tests in speciﬁc trees, for example. 4. Evaluation metrics and results The validation of the tree geometric and dynamic models were twofold. First, we used the two development datasets to evaluate the geometric modeling. In these datasets, we mainly evaluated the topological and morphological cor- rectness of the obtained model with respect to the input ground truth and the point clouds. To assess the topological correctness, we quantiﬁed the matching between branching and termination points (BP and TP), among the ground truth and our model. The BP and TP ground truth was manually obtained by visual in- spection of the development-synthetic skeletons. In the gen- erated model, a BP is a joint forming a vertex greater or equal to3o. On the other hand, a TP is a joint with no chil- dren. To ﬁnd the BP and TP in our model that matches the ground truth, we employed the Euclidean distance. Since the trees in the dataset had different sizes, we evaluated two thresholds to deﬁne the matching: a ﬁxed value ∆d1and another depending on the tree height ∆d2. Then, accuracy and recall metrics were calculated as, Accuracy =MP TPmodel,Recall=MP TPGT whereMP is the number of matched points, TPmodel and TPGTare the number of points in the model and the GT, respectively. The morphological correctness of the model was eval- uated using the Hausdorff distance, which allowed us to measure the geometric difference between two point clouds or meshes [ 37]. To generate the point cloud of the model, we uniformly sampled a set of points around the struc- ture. Then, the Cloud Compare software was employed Figure 4: Experimental setup to evaluate the dynamic model. The contact points are encoded according to its loca- tion and the axis on which the force was applied. The right column shows how the sensor was employed to measure the force magnitude for each axis. to obtain the Hausdorff distance. Both topological and morphological correctness were used in complement for the development-synthetic dataset. However, we only as- sessed the morphological correctness in the development- real dataset since it only consisted in point clouds, which made it impractical to accurately ﬁnd the ground truth BP and TP. To evaluate the dynamic modeling, we used laboratory tests on a dormant vine. We placed several ﬁducial tags in the tree structure to track its movement using cameras in a stereo pair, as shown in Fig 4. Subsequently, various forces were applied along each axis x,yandz, and the displace- ment of the markers saved. The contact point locations for each case were marked in the real structure and then iden- tiﬁed in the virtual model. Additionally, the magnitude of the applied force in each trial was measured using a force gauge with 49N capacity and ±0.4%, 1 digit accuracy. We then calculated the mean squared error (MSE) between the measured ﬁnal tag positions and those predicted with the dynamic model, as described by Eqn. 3. MSEj=1 NN/summationdisplay i=1(xi,j−ˆ xi,j)2(3) whereNis the number of markers, xi,jandˆ xi,jare the measured and estimated positions of the tag ifor the trial j, respectively. 4.1. Space colonization algorithm Results of the evaluation of the topological correctness in the development synthetic dataset are summarized in Table 1. It can be noted that the SCA fails to accurately match the BP and TP with the ground truth, specially for ∆d1. This outcome is at some extent expected since in the model, the tree growth with no other constraints than the positions of the buds, acting as attraction points. Additionally, it has Table 1: Summary of topological correctness performance for the SCA. Tree 1 Tree 2 Tree 3 Tree 4 ∆d1∆d2∆d1∆d2∆d1∆d2∆d1∆d2 Bifurcation PointsAccuracy 0.231 0.231 0.015 0.162 0.034 0.101 0.002 0.038 Recall 0.286 0.286 0.091 1.000 0.333 1.000 0.052 0.974 Termination PointsAccuracy 0.429 0.429 0.114 0.343 0.140 0.207 0.016 0.045 Recall 0.273 0.273 0.333 1.000 0.680 1.000 0.328 0.916 ∆d1= 0.25m ∆d2=5% of tree height to be noted that recall is consistently better than accuracy, which indicates that the model was able to match the BP and TP of the ground truth, but it also provided a lot of false detections. Figure 5shows the boxplots for the Hausdorff distance for the models built using the SCA for both development datasets. Additionally, right columns of Figure 5shows ex- amples of two point clouds built based in the SCA model for the synthetic and real datasets. They are colorized ac- cording to the Hausdorff distance to the ground truth data. In general, it can be seen that various parts of the generated structures are similar to the ground truth under this met- ric (blue-colored points), but the distance grows near the branching points, which agrees with the results of Table 1, where the accuracy in the BP and TP matching was low. Despite of the low performance of the SCA in these tests, the results still show potential considering that trees of both datasets are dormant, with their branching structure in sight. However, the SCA can be useful in trees with leaves, where such structure is hidden beneath the canopy. In this case, the overall system can work in two iterative stages: ﬁrst, detecting the leaves positions (with any object detection al- gorithm) to use them along with the SCA to compose an initial “guess” of the branching structure. The second step would require to move the tree to obtain a better perspective of its branching conﬁguration, providing thus a feedback to reﬁne the generated model. 4.2. Laplacian-based contraction In this case, the accuracy of the BP and TP matching certainly improved, as Table 2shows. However, the termi- nation points recall is specially comparable (and in some cases worst) to the values obtained with the SCA. This out- come implies that the TP of the tree model agreed better to the ground truth, but it also produced extra spurious TP, producing a decrease in the recall. Making a close insight of the model, these spurious TP were generated by noisy points throughout the branches. To improve this issue we have to explore ways to improve the topological connection or smooth the skeleton produced by the LBC. The morphological correctness of the model for the LBC algorithm outperformed the SCA, as Fig. 6shows. It can be seen that the Hausdorff distances are considerable smaller (a) (b) Figure 5: Results for the space colonization algorithm. The left ﬁgures show the boxplots for the Housdorff distances between our model and the ground truth for the synthetic and real datasets. The right ﬁgures show examples of the input point clouds (black points) along its generated models, colorized according to their Hausdorff distances to the input point cloud, measured in meters. Table 2: Summary of topological correctness performance for the LBC algorithm. Tree 1 Tree 2 Tree 3 Tree 4 ∆d1∆d2∆d1∆d2∆d1∆d2∆d1∆d2 Bifurcation PointsAccuracy 0.293 0.293 0.333 0.458 0.417 0.500 0.063 0.556 Recall 0.571 0.571 0.727 1.000 0.833 1.000 0.078 0.687 Termination PointsAccuracy 0.455 0.455 0.154 0.923 0.120 0.960 0.014 0.986 Recall 0.455 0.455 0.167 1.000 0.120 0.960 0.008 0.605 ∆d1= 0.25m ∆d2=5% of tree height than the ones obtained previously, which indicates that LBC algorithm combined with our topological connection pro- vides an overall reliable representation of the tree morphol- ogy. This result can be also veriﬁed in the right columns of Fig6, where two point clouds colorized according to their Hausdorff distance to the ground truth are depicted. 4.3. Tree dynamics To generate the tree geometry, we employed the LBC algorithm for the laboratory vine. As the aim in these tests was to validate only the dynamics, the input point cloud was manually enhanced to remove any artifact or spurious point (a) (b) Figure 6: Results for the Laplace based contraction and topological connection method. The left ﬁgures show the box-plots for the Hausdorff distances between our model and the ground truth for the synthetic and real datasets. The right ﬁgures show examples of the input point clouds (black points) along its generated models, colorized according to their Hausdorff distances to the input point cloud, measured in meters. that may affect the geometric model. Additionally, several prior tests were conducted to adjust the value of the Kpa- rameters that correctly account for the observed movement of the tree. For these tests, we obtained 5 values of Kthat provided the best results in terms of the ﬁnal tree shape and the motion of the marked points. The requirement of this manual adjustment for different cases reveals two impor- tant points of the dynamic model employed. First, with a suitable value of K, the model can properly account for the tree movement in presence of external forces. Accordingly, it is required an identiﬁcation/learning approach to estimate Ksuch that it generalizes to movement in all directions. Furthermore, the linear mapping of the cylinders radius as function of the number of descendants is not entirely accu- rate. Instead, the radius of certain segments of the branches can be estimated from the point cloud. The results of the dynamic model validation tests are summarized in Table 3. Given the tree dimensions in height, length and depth are 0.8m, 1.65m and 20.26m, respectively; we consider that the model correctly accounted for the tree motion, given the remark about the spring stiffness Kex- plained above.Table 3: Mean squared error of predicted positions for the ﬁducial markers attached to the tree. Axis of motion x y z Contact Point Id CP xL CP xR CP yR CP yL CP zR CP zL Trial 1 0.015 0.051 0.069 0.036 0.034 0.030 Trial 2 0.020 0.043 0.073 0.050 0.038 0.044 5. Conclusions This work presented the geometric and dynamic model- ing of fruit trees using computer graphic methods mainly designed to create virtual environments. In contrast to common applications in this ﬁeld, the digitization of real trees comprises a series of additional challenges that we addressed. First, it is not enough for the model to have a structure similar to a generic tree, it needs to have - approximately- the same branching disposition, shape and dimensions of the real one. We used point clouds from syn- thetic and real trees (the latter acquired in ﬁeld) to generate a model using two known methods such as the space col- onization and the Laplacian based contraction algorithms. They were subsequently assessed quantitatively in order to test the suitability of each one to account for such charac- teristics. Results from topological and morphological cor- rectness test shown that the Laplacian based contraction to- gether with our topological connection algorithm performed better in dormant trees. However, the topological correct- ness tests with the synthetic dataset showed that although it can properly resemble the BP and TP, it is prone to produce small spurious termination points. Another challenge we addressed when digitizing real trees consisted in obtaining a motion model that accounts for the springy behaviour of the branches. To this aim, we used a dynamic model for rigid bodies articulated by stiff joints. Furthermore, its parametrization allowed a spring- damper interpretation to simulate diverse behaviours for dif- ferent trees. Considering the input is a good geometrical model (using either SCA, LBC or other algorithm), it is ca- pable to accurately predict the motion of the branches in presence of external forces, given the proper value of the parameters. The ongoing work is focusing in the correct identiﬁca- tion of the Kparameter. This is specially critical since the overall accuracy of the motion prediction relies on it. Fi- nally, with a complete geometric and dynamic model of a fruit tree, the ﬁnal objective is to use it in practice for a robot to intelligently interact with the canopy in activities including automated harvesting or pruning. References",2
Visual 3D Reconstruction and Dynamic Simulation of Fruit Trees for Robotic,CVPR_2020,3,"rest of the tree, as in [ 32,27] and the references therein. Fur- thermore, an interesting feature of these approaches is that they can be parameterized using the stiffness and damping coefﬁcients, which allowed to model various structures in",3
Visual 3D Reconstruction and Dynamic Simulation of Fruit Trees for Robotic,CVPR_2020,4,"rest of the tree, as in [ 32,27] and the references therein. Fur- thermore, an interesting feature of these approaches is that they can be parameterized using the stiffness and damping coefﬁcients, which allowed to model various structures in",3
Visual 3D Reconstruction and Dynamic Simulation of Fruit Trees for Robotic,CVPR_2020,5,". As an alternative, trees have also been modeled as series of rods interconnected, as presented in [ 10]. Finally, it is important to note that the majority of meth- ods for tree modeling discussed here are not originally quantitatively compared to the real counterparts. This ob- servation is coherent with the fact that most of them are computer graphics applications whose aim is to create vir- tual environments that visually render real looking trees. However, when the aim is to manipulate the tree, the ac- curacy of the model obtained is an important feature to be evaluated. 3. Materials and Methods The proposed approach comprises two stages: the three dimensional modeling of the tree and its dynamical simula- tion. For the ﬁrst stage, we implemented two methods from the state of the art: the space colonization algorithm [ 30] and the Laplacian based contraction [ 6]. We consider that both are purely visual since the sensor employed to get the real data is a camera system, as will be described in the fol- lowing Section. These two algorithms were chosen mainly for the following three reasons. First, they are well known methods to skeletonize and reconstruct branching structures [28,1,22]. Additionally, they have proven effectiveness in various scenarios and datasets [ 8]. Finally, we consider that they provide a complementary way to model the tree struc- ture in the case of a tree with foliage, as will be discussed in Section 4.1. The dynamic simulation was required to be ﬂexible in the sense of allowing joints with elastic stiffness with 3 degrees of freedom. Furthermore, the computation of the movement required a robust and efﬁcient method so later it can be used for a robot to estimate it movement or actu- ate on the tree. While known physics engines (e.g., bullet, open dynamics engine, PhysX) provide these two charac- teristics, they seemed inadequate to explicitly model and tune the elastic stiffness in each degree of freedom. For these reasons, we implemented the model described in [ 27], which is a computer graphics application for botanical tree animation that suits better for the purposes of this work. Figure 1summarizes the pipeline we followed to obtain the geometric structure of a fruit tree and estimate its dy- namics. 3.1. Development Datasets and Hardware We used three types of datasets to evaluate the quality of the 3D reconstruction and the feasibility of the dynamic modeling for real trees. The development-artiﬁcial dataset consisted in 4 freely available skeletons (with their respec- tive point clouds) of different shaped and sized trees, pro- vided in [ 9]. The skeletons were considered as ground truth for all of them. The development-real dataset comprised a set of 14 point clouds from real grape vines, acquired in ﬁeld with a semi-autonomous platform equipped with dual stereo cameras and a gantry system (Fig. 2). We basically used the Iterative Closest Point (ICP) algorithm and rigid transformations to align the point clouds produced by thetop and bottom cameras. As this paper mainly focus on the visual 3D reconstruction and dynamic simulation of tree fruits, the design of the image acquisition system and the platform are targeted for another independent publication. The third dataset corresponded to laboratory tests with a dormant vine, which was mainly used to evaluate the prac- tical applicability of the movement estimation in real sce- narios. 3.2. Image-based approach - Space colonization al- gorithm The Space Colonization Algorithm (SCA) is an itera- tive method to build tree structures based on the concept of competition for space. It takes as input Nattraction points (which can be the leaves or buds of the tree) and the tree root or other pivot points acting as prior nodes (joints). Then, the algorithm builds the branching structure of the tree by attaching new links based on their distance and relative po- sition to the nearest attraction point(s). A deeper description of the algorithm, as well as some interesting modiﬁcations can be found in the original work [ 30]. The natural interpretation of the attraction points from a practical point of view is to consider them as leaves. How- ever, we employed other possibility: detect the buds of the branches and use them as attraction points. In this work, the detection procedure relied on a Faster-RCNN, which is a very popular deep neural architecture for object recogni- tion [ 29]. The consecutive incoming images from top and bottom stereo cameras are sent through the object detector trained to detect dormant buds in 2D images. Using the optimized transformation of the registered point cloud pro- duced by the ICP algorithm (see Sec. 3.1), the dormant buds were ﬁltered and merged. In this case, it was neces- sary to ﬁnd appropriate values for the growing parameters that force the branches to reach the buds. Furthermore, in some cases (specially for the vines) one additional pivot was given as input for the tree to obtain a structure similar to the real. In this case, the algorithm starts trying to reach only the pivot for later start growing using the other attraction points. 3.3. Point cloud based approach - Laplacian-based contraction Among the diverse algorithms for tree skeletonization, we employed the Laplacian-based contraction (LBC) [ 6]. It is a geometric approach that collapses the input point cloud maintaining its global shape. The main idea is to obtain a new point cloud with minimal volume using a repetitive Laplacian smoothing process. At each iteration i, the point cloudPis contracted by ﬁnding the set of contracted points Figure 1: Workﬂow of the proposed approach to digitize and simulate the movement of fruit trees using A) the space colo- nization and B) a skeletonization algorithms. Figure 2: A semi-autonomous platform for image acquisi- tion. The imaging system consists of two stereo cameras attached to a gantry. The stereo cameras acquire seven dif- ferent top and bottom views of the vine at a regular interval along the gantry. P′that solve the linear system: /bracketleftbigg Wi LLi Wi H/bracketrightbigg P′i=/bracketleftbigg0 Wi HPi/bracketrightbigg (1) whereWL,WHare weighting factors to control the con- traction and attraction constraints, and Lis the Laplacian operator with cotangent weights. For the next step, the new set of points found in the current iteration, P′i, is assigned toPi+1while the weights and the Laplacian operator are updated to account for the ”new shape” of Pi+1. This pro- cess repeats until a convergence test is passed. When carefully designing the weighting factors, this overall approach provides efﬁciency and stability when con- tracting point clouds. As the resulting set of points is a thin structure that approximates the skeleton of the original point cloud, it is further necessary to connect the structure ensur-ing to faithfully represent the topology of the original struc- ture (a tree in our case). 3.3.1 Topological connection Various methods have been proposed to convert a point cloud to a skeleton [ 13,31]. However, we used a custom method to connect the points obtained from the LBC that aims to resemble the characteristic topological structure of a tree. We developed this method is speciﬁc to account for the hierarchical pattern that the dynamic simulation requires. This is, starting from the root, the subsequent links require a series of parent-child connections that preserves the tree shape to properly propagate the movement when a force is applied to any branch. Firstly, the points are organized in ascending order ac- cording to their zcoordinates, obtaining a point cloud P={pi}. Denoting a node in the interconnected graph asgi=/braceleftbig pi,αi k/bracerightbig , whereαi kis its parent node with index k, we use the procedure summarized in Algorithm 1. The main idea is to join the children nodes with their par- ents based on its Euclidean distance. If a node has multiple parents during the connection, the one with less distance adopts that node as child. However, there may be cases when this distance test is not conclusive (lines 12 and 17). In this circumstance we use the observation that the con- nection direction is inﬂuenced by the spatial distribution of the points along the branches or the trunk. The neighbors oriented search (lines 13 and 18) is used to implement this idea as follows: we ﬁrst calculate the directional vectors from both disputing parents to the current node. Two imag- inary cylinders oriented in these directions, with ﬁxed ra- diusρand length Lcyldepending on the dimensions of the tree are then built, as shown in the third row of Fig 3c. The points lying inside each cylinder are counted and the one with majority is chosen. Finally, the parent associated to that cylinder keeps the child. The overall procedure, is summarized in Fig. 3as fol- Algorithm 1 Topological tree connection 1:MakeG′(gi) ={pi,∅} ∀pi∈P 2:while nodes without a parent exist do 3: Increase index i 4: Find nodes gj:|pi−pj|< r 5: whilegjis not found doincreaser 6: For allgj:αj=∅, connect pjtopiand make gj=/braceleftBig pj,αj i/bracerightBig . 7: if∃g′ j=gj:αj k/ne}ationslash=∅then 8: forallg′ jdo: 9: dcurr=|pj−pi|,dprev=|pj−αi k| 10: ifdcurr≤dprev then 11: ifdcurr dprev< dthres then 12: Change the parent αj k=αj i 13: else 14: Do neighbor oriented search 15: else 16: ifdprev dcurr< dthres then 17: Keep the previous parent αj k 18: else 19: Do neighbor oriented search 20: MakeG=G′ 21: RemovegifromG′. lows: the ﬁrst row shows a simple case when a multi parent node is found. The parent is assigned based on the shorter distance to the disputing node. Similarly, the second row shows a multi-parent node in a branching point, where the dispute is solved based on the distance. The third row shows the more complex case where the distance test is not con- clusive and the neighbor oriented search is performed. As the orange cylinder has more points inside the original par- ent is ﬁnally chosen. This process is repeated until all the points have a parent assigned. It is noteworthy that the pre- sented method requires three design parameters: the radius to make the initial search r, the ratio to compare the dis- tances between disputing parents dthres and the radius of the oriented cylinders ρ. While its length can be also a pa- rameter we choose to make it function of the tree height to avoid an over-parametrization of the method. 3.4. Dynamic modeling The connected structure from the SC and LBC algo- rithms are just skeletal representations of the tree. To con- vert them into 3D structures capable to move and interact with other objects, we assume that each link has a cylindri- cal shape, whose length is known, but the radius changes depending of the tree structure. This change was modeled making the radius a linear function of the number of descen- dants of each joint (this accounts for the complete lineage, Figure 3: Topological connection applied to three different scenarios, increasing in complexity. The red dot depicts the current node gi, the blue lines are the current connections based on the distance, and the yellow lines show the con- ﬂicting connection with previous parents. not only the current children). The inertial properties of each link were then included by estimating the mass using the volume of each cylinder and the density of the wood for the tree under study. The movement of the tree was calcu- lated by using the four steps originally proposed in [ 27] and described as follows: •External force computation: For each rigid body (cylinder), external forces and torques due to gravity, wind ﬁelds or other sources are calculated. •Composite body update: The inertial characteristics of a link of the tree are at some extent affected by the others. Therefore, the total external forces and torques, mass, and inertia tensor (in world reference frame) of each rigid body are calculated in base of the branching structure of its children and its own. These calculations are backpropagated from the outer links to the root in order to update all the tree structure. •Analytic spring evolution: The joint space conﬁgura- tion is then calculated by solving the equation: ˜I¨θ+βK˙θ+Kθ=˜τ (2) whereθis the joint position, ˜Iis the local composite inertial tensor, Kis a diagonal 3×3matrix whose elements correspond to the rotational stiffness of the joint,βKaccounts for the damping coefﬁcients and ˜τ is the local composite external torque. •Rigid body state update: The state vector of each link [mi,xi,vi,ai,Ii,Ri,ωi,αi]is expressed in maximal coordinates and includes its composite mass, position, velocity, acceleration, composite inertia, rotation ma- trix, angular velocity and angular acceleration, respec- tively. Once the solution of Eqn.( 2) is found for all the links, their state vectors are updated, starting from the root. While for a virtual environment simulation, tuning K to obtain different spring behaviors of the joints would be enough, the modeling real fruit trees requires a more ana- lytical approach. For that reason, we used the deﬁnition of Kdescribed in [ 21]. In this work, each of the elements of Kis related with the area moment of inertia of the link, and structural constants (available for various materials) such as the Young modulus and the Poisson’s ratio. Thus, we have an a prior knowledge of K, which can be used as an initial condition when performing identiﬁcation tests in speciﬁc trees, for example. 4. Evaluation metrics and results The validation of the tree geometric and dynamic models were twofold. First, we used the two development datasets to evaluate the geometric modeling. In these datasets, we mainly evaluated the topological and morphological cor- rectness of the obtained model with respect to the input ground truth and the point clouds. To assess the topological correctness, we quantiﬁed the matching between branching and termination points (BP and TP), among the ground truth and our model. The BP and TP ground truth was manually obtained by visual in- spection of the development-synthetic skeletons. In the gen- erated model, a BP is a joint forming a vertex greater or equal to3o. On the other hand, a TP is a joint with no chil- dren. To ﬁnd the BP and TP in our model that matches the ground truth, we employed the Euclidean distance. Since the trees in the dataset had different sizes, we evaluated two thresholds to deﬁne the matching: a ﬁxed value ∆d1and another depending on the tree height ∆d2. Then, accuracy and recall metrics were calculated as, Accuracy =MP TPmodel,Recall=MP TPGT whereMP is the number of matched points, TPmodel and TPGTare the number of points in the model and the GT, respectively. The morphological correctness of the model was eval- uated using the Hausdorff distance, which allowed us to measure the geometric difference between two point clouds or meshes [ 37]. To generate the point cloud of the model, we uniformly sampled a set of points around the struc- ture. Then, the Cloud Compare software was employed Figure 4: Experimental setup to evaluate the dynamic model. The contact points are encoded according to its loca- tion and the axis on which the force was applied. The right column shows how the sensor was employed to measure the force magnitude for each axis. to obtain the Hausdorff distance. Both topological and morphological correctness were used in complement for the development-synthetic dataset. However, we only as- sessed the morphological correctness in the development- real dataset since it only consisted in point clouds, which made it impractical to accurately ﬁnd the ground truth BP and TP. To evaluate the dynamic modeling, we used laboratory tests on a dormant vine. We placed several ﬁducial tags in the tree structure to track its movement using cameras in a stereo pair, as shown in Fig 4. Subsequently, various forces were applied along each axis x,yandz, and the displace- ment of the markers saved. The contact point locations for each case were marked in the real structure and then iden- tiﬁed in the virtual model. Additionally, the magnitude of the applied force in each trial was measured using a force gauge with 49N capacity and ±0.4%, 1 digit accuracy. We then calculated the mean squared error (MSE) between the measured ﬁnal tag positions and those predicted with the dynamic model, as described by Eqn. 3. MSEj=1 NN/summationdisplay i=1(xi,j−ˆ xi,j)2(3) whereNis the number of markers, xi,jandˆ xi,jare the measured and estimated positions of the tag ifor the trial j, respectively. 4.1. Space colonization algorithm Results of the evaluation of the topological correctness in the development synthetic dataset are summarized in Table 1. It can be noted that the SCA fails to accurately match the BP and TP with the ground truth, specially for ∆d1. This outcome is at some extent expected since in the model, the tree growth with no other constraints than the positions of the buds, acting as attraction points. Additionally, it has Table 1: Summary of topological correctness performance for the SCA. Tree 1 Tree 2 Tree 3 Tree 4 ∆d1∆d2∆d1∆d2∆d1∆d2∆d1∆d2 Bifurcation PointsAccuracy 0.231 0.231 0.015 0.162 0.034 0.101 0.002 0.038 Recall 0.286 0.286 0.091 1.000 0.333 1.000 0.052 0.974 Termination PointsAccuracy 0.429 0.429 0.114 0.343 0.140 0.207 0.016 0.045 Recall 0.273 0.273 0.333 1.000 0.680 1.000 0.328 0.916 ∆d1= 0.25m ∆d2=5% of tree height to be noted that recall is consistently better than accuracy, which indicates that the model was able to match the BP and TP of the ground truth, but it also provided a lot of false detections. Figure 5shows the boxplots for the Hausdorff distance for the models built using the SCA for both development datasets. Additionally, right columns of Figure 5shows ex- amples of two point clouds built based in the SCA model for the synthetic and real datasets. They are colorized ac- cording to the Hausdorff distance to the ground truth data. In general, it can be seen that various parts of the generated structures are similar to the ground truth under this met- ric (blue-colored points), but the distance grows near the branching points, which agrees with the results of Table 1, where the accuracy in the BP and TP matching was low. Despite of the low performance of the SCA in these tests, the results still show potential considering that trees of both datasets are dormant, with their branching structure in sight. However, the SCA can be useful in trees with leaves, where such structure is hidden beneath the canopy. In this case, the overall system can work in two iterative stages: ﬁrst, detecting the leaves positions (with any object detection al- gorithm) to use them along with the SCA to compose an initial “guess” of the branching structure. The second step would require to move the tree to obtain a better perspective of its branching conﬁguration, providing thus a feedback to reﬁne the generated model. 4.2. Laplacian-based contraction In this case, the accuracy of the BP and TP matching certainly improved, as Table 2shows. However, the termi- nation points recall is specially comparable (and in some cases worst) to the values obtained with the SCA. This out- come implies that the TP of the tree model agreed better to the ground truth, but it also produced extra spurious TP, producing a decrease in the recall. Making a close insight of the model, these spurious TP were generated by noisy points throughout the branches. To improve this issue we have to explore ways to improve the topological connection or smooth the skeleton produced by the LBC. The morphological correctness of the model for the LBC algorithm outperformed the SCA, as Fig. 6shows. It can be seen that the Hausdorff distances are considerable smaller (a) (b) Figure 5: Results for the space colonization algorithm. The left ﬁgures show the boxplots for the Housdorff distances between our model and the ground truth for the synthetic and real datasets. The right ﬁgures show examples of the input point clouds (black points) along its generated models, colorized according to their Hausdorff distances to the input point cloud, measured in meters. Table 2: Summary of topological correctness performance for the LBC algorithm. Tree 1 Tree 2 Tree 3 Tree 4 ∆d1∆d2∆d1∆d2∆d1∆d2∆d1∆d2 Bifurcation PointsAccuracy 0.293 0.293 0.333 0.458 0.417 0.500 0.063 0.556 Recall 0.571 0.571 0.727 1.000 0.833 1.000 0.078 0.687 Termination PointsAccuracy 0.455 0.455 0.154 0.923 0.120 0.960 0.014 0.986 Recall 0.455 0.455 0.167 1.000 0.120 0.960 0.008 0.605 ∆d1= 0.25m ∆d2=5% of tree height than the ones obtained previously, which indicates that LBC algorithm combined with our topological connection pro- vides an overall reliable representation of the tree morphol- ogy. This result can be also veriﬁed in the right columns of Fig6, where two point clouds colorized according to their Hausdorff distance to the ground truth are depicted. 4.3. Tree dynamics To generate the tree geometry, we employed the LBC algorithm for the laboratory vine. As the aim in these tests was to validate only the dynamics, the input point cloud was manually enhanced to remove any artifact or spurious point (a) (b) Figure 6: Results for the Laplace based contraction and topological connection method. The left ﬁgures show the box-plots for the Hausdorff distances between our model and the ground truth for the synthetic and real datasets. The right ﬁgures show examples of the input point clouds (black points) along its generated models, colorized according to their Hausdorff distances to the input point cloud, measured in meters. that may affect the geometric model. Additionally, several prior tests were conducted to adjust the value of the Kpa- rameters that correctly account for the observed movement of the tree. For these tests, we obtained 5 values of Kthat provided the best results in terms of the ﬁnal tree shape and the motion of the marked points. The requirement of this manual adjustment for different cases reveals two impor- tant points of the dynamic model employed. First, with a suitable value of K, the model can properly account for the tree movement in presence of external forces. Accordingly, it is required an identiﬁcation/learning approach to estimate Ksuch that it generalizes to movement in all directions. Furthermore, the linear mapping of the cylinders radius as function of the number of descendants is not entirely accu- rate. Instead, the radius of certain segments of the branches can be estimated from the point cloud. The results of the dynamic model validation tests are summarized in Table 3. Given the tree dimensions in height, length and depth are 0.8m, 1.65m and 20.26m, respectively; we consider that the model correctly accounted for the tree motion, given the remark about the spring stiffness Kex- plained above.Table 3: Mean squared error of predicted positions for the ﬁducial markers attached to the tree. Axis of motion x y z Contact Point Id CP xL CP xR CP yR CP yL CP zR CP zL Trial 1 0.015 0.051 0.069 0.036 0.034 0.030 Trial 2 0.020 0.043 0.073 0.050 0.038 0.044 5. Conclusions This work presented the geometric and dynamic model- ing of fruit trees using computer graphic methods mainly designed to create virtual environments. In contrast to common applications in this ﬁeld, the digitization of real trees comprises a series of additional challenges that we addressed. First, it is not enough for the model to have a structure similar to a generic tree, it needs to have - approximately- the same branching disposition, shape and dimensions of the real one. We used point clouds from syn- thetic and real trees (the latter acquired in ﬁeld) to generate a model using two known methods such as the space col- onization and the Laplacian based contraction algorithms. They were subsequently assessed quantitatively in order to test the suitability of each one to account for such charac- teristics. Results from topological and morphological cor- rectness test shown that the Laplacian based contraction to- gether with our topological connection algorithm performed better in dormant trees. However, the topological correct- ness tests with the synthetic dataset showed that although it can properly resemble the BP and TP, it is prone to produce small spurious termination points. Another challenge we addressed when digitizing real trees consisted in obtaining a motion model that accounts for the springy behaviour of the branches. To this aim, we used a dynamic model for rigid bodies articulated by stiff joints. Furthermore, its parametrization allowed a spring- damper interpretation to simulate diverse behaviours for dif- ferent trees. Considering the input is a good geometrical model (using either SCA, LBC or other algorithm), it is ca- pable to accurately predict the motion of the branches in presence of external forces, given the proper value of the parameters. The ongoing work is focusing in the correct identiﬁca- tion of the Kparameter. This is specially critical since the overall accuracy of the motion prediction relies on it. Fi- nally, with a complete geometric and dynamic model of a fruit tree, the ﬁnal objective is to use it in practice for a robot to intelligently interact with the canopy in activities including automated harvesting or pruning. References",2
FusAtNet: Dual Attention based SpectroSpatial Multimodal Fusion Network for,CVPR_2020,1,"Behnood Rasti, Pedram Ghamisi, Javier Plaza, and Antonio Plaza. Fusion of hyperspectral and lidar data using sparse and low-rank component analysis. IEEE Transactions on Geoscience and Remote Sensing , 55(11):6354–6365, 2017. 2",4
FusAtNet: Dual Attention based SpectroSpatial Multimodal Fusion Network for,CVPR_2020,2,"Junshi Xia, Zuheng Ming, and Akira Iwasaki. Multiple sources data fusion via deep forest. In IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Sympo- sium , pages 1722–1725. IEEE, 2018. 2",3
FusAtNet: Dual Attention based SpectroSpatial Multimodal Fusion Network for,CVPR_2020,3,"Luis G ´omez-Chova, Devis Tuia, Gabriele Moser, and Gustau Camps-Valls. Multimodal classiﬁcation of remote sensing images: A review and future directions. Proceedings of the IEEE , 103(9):1560–1584, 2015. 2[15] Dushyant Rao, Mark De Deuge, Navid Nourani-Vatani, Ste- fan B Williams, and Oscar Pizarro. Multimodal learning and inference from visual and remotely sensed data. The Inter- national Journal of Robotics Research , 36(1):24–43, 2017. 2",4
FusAtNet: Dual Attention based SpectroSpatial Multimodal Fusion Network for,CVPR_2020,4,"Wei Hu, Yangyu Huang, Li Wei, Fan Zhang, and Hengchao Li. Deep convolutional neural networks for hyperspectral image classiﬁcation. Journal of Sensors , 2015, 2015. 2",5
FusAtNet: Dual Attention based SpectroSpatial Multimodal Fusion Network for,CVPR_2020,5,"Lichao Mou and Xiao Xiang Zhu. Learning to pay atten- tion on spectral domain: A spectral attention module-based convolutional network for hyperspectral image classiﬁca- tion. IEEE Transactions on Geoscience and Remote Sensing , 2019. 2,3,4",1
Learning Instance Occlusion for Panoptic Segmentation,CVPR_2020,1,"Hayko Riemenschneider, Sabine Sternig, Michael Donoser, Peter M Roth, and Horst Bischof. Hough regions for joining instance localization and segmentation. In ECCV . 2012. 1",5
Learning Instance Occlusion for Panoptic Segmentation,CVPR_2020,2,"Yi-Ting Chen, Xiaokai Liu, and Ming-Hsuan Yang. Multi- instance object segmentation with occlusion handling. In CVPR , 2015. 4",3
Learning Instance Occlusion for Panoptic Segmentation,CVPR_2020,3,"Daan de Geus, Panagiotis Meletis, and Gijs Dubbelman. Panoptic segmentation with a joint semantic and instance segmentation network. arXiv preprint arXiv:1809.02110 , 2018. 6",3
Learning Instance Occlusion for Panoptic Segmentation,CVPR_2020,4,"Markus Enzweiler, Angela Eigenstetter, Bernt Schiele, and Dariu M Gavrila. Multi-cue pedestrian classiﬁcation with partial occlusion handling. In CVPR , 2010. 4",4
Learning Instance Occlusion for Panoptic Segmentation,CVPR_2020,5,"Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV , 2017. 5",7
Learning to Optimize on SPD Manifolds,CVPR_2020,1,"Pengfei Fang, Jieming Zhou, Soumava Kumar Roy, Lars Petersson, and Mehrtash Harandi. Bilinear attention net- works for person retrieval. In Proceedings of the IEEE In- ternational Conference on Computer Vision (ICCV) , October 2019.",5
Learning to Optimize on SPD Manifolds,CVPR_2020,2,"Maurice Fr ´echet. Les ´el´ements al ´eatoires de nature quel- conque dans un espace distanci ´e. In Annales de l’institut Henri Poincar ´e, volume 10, pages 215–310, 1948.",2
Learning to Optimize on SPD Manifolds,CVPR_2020,3,"Zhiwu Huang, Ruiping Wang, Shiguang Shan, Xianqiu Li, and Xilin Chen. Log-euclidean metric learning on symmet- ric positive deﬁnite manifold with application to image set classiﬁcation. In Proceedings of the International Confer- ence on Machine Learning (ICML) , pages 720–729, 2015.",5
Learning to Optimize on SPD Manifolds,CVPR_2020,4,"Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured feature embedding. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) , pages 4004– 4012, 2016.",4
Learning to Optimize on SPD Manifolds,CVPR_2020,5,"Lingxi Xie, Qi Tian, Richang Hong, Shuicheng Yan, and Bo Zhang. Hierarchical part matching for ﬁne-grained visual categorization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) , pages 1641–1648, 2013.",5
On indirect assessment of heart rate in video,CVPR_2020,1,"W. Verkruysse, L. O. Svaasand, and J. S. Nelson, “Remote plethysmographic imaging using ambient light,” OPT EX- PRESS , vol. 16, no. 26, pp. 21434–21445, 2008.",2
On indirect assessment of heart rate in video,CVPR_2020,2,"C. S. Peserico, P. V . Mezzaroba, G. A. Nogueira, S. M. F. d. Moraes, and F. A. Machado, “Comparison between direct and indirect methods for the determination of the maximal oxygen uptake in female runners (comparac ¸ ˜ao en- tre os m ´etodos direto e indireto de determinac ¸ ˜ao do con- sumo m ´aximo de oxig ˆenio em mulheres corredoras),” Re- vista Brasileira de Medicina do Esporte , vol. 17, pp. 270 – 273, 08 2011.",2
On indirect assessment of heart rate in video,CVPR_2020,3,"A. Plaza-Florido, J. H. Migueles, J. Mora-Gonzalez, P. Molina-Garcia, M. Rodriguez-Ayllon, C. Cadenas- Sanchez, I. Esteban-Cornejo, S. Navarrete, R. Maria Lozano, N. Michels, J. Sacha, and F. B. Ortega, “The role of heart rate on the associations between body composition and heart rate variability in children with overweight/obesity: The ac- tivebrains project,” Frontiers in Physiology , vol. 10, p. 895, 2019.",2
On indirect assessment of heart rate in video,CVPR_2020,4,"X. Li, H. Han, H. Lu, X. Niu, Z. Yu, A. Dantcheva, G. Zhao, and S. Shan, “The 1st Challenge on Remote Physiological Signal Sensing (RePSS),” arXiv , 2020.",2
On indirect assessment of heart rate in video,CVPR_2020,5,"X. Li, I. Alikhani, J. Shi, T. Seppanen, J. Junttila, K. Majamaa-V oltti, M. Tulppo, and G. Zhao, “The OBF Database: A Large Face Video Database for Remote Phys- iological Signal Measurement and Atrial Fibrillation Detec- tion,” in 2018 13th IEEE International Conference on Au- tomatic Face Gesture Recognition (FG 2018) , pp. 242–249, May 2018.",2
CvxNet:,CVPR_2020,1,"Thomas Binford. Visual perception by computer. In IEEE Conference of Systems and Control , 1971. 2",2
CvxNet:,CVPR_2020,2,"Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J Black. Keep it smpl: Automatic estimation of 3d human pose and shape from a single image. In Proceedings of the European Conference on Computer Vision , 2016. 2",6
CvxNet:,CVPR_2020,3,"Eric Heiden, David Millard, Hejia Zhang, and Gaurav S Sukhatme. Interactive differentiable simulation. arXiv preprint arXiv:1905.10706 , 2019. 2",4
CvxNet:,CVPR_2020,4,"Jyh-Ming Lien and Nancy M Amato. Approximate convex decomposition of polyhedra. In Computer Aided Geometric Design (Proc. of the Symposium on Solid and physical mod- eling) , pages 121–131. ACM, 2007. 3",1
CvxNet:,CVPR_2020,5,"Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3d point clouds. In International Conference on Machine Learning , pages 40–49, 2018. 1",4
SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving,CVPR_2020,1,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. arXiv preprint arXiv:1502.03167 , 2015. 5[19] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial net- works. In CVPR , 2017. 3",1
SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving,CVPR_2020,2,Deepgtav v2. 2,2
SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving,CVPR_2020,3,"Waymo open dataset: An autonomous driving dataset, 2019. 3,5",3
SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving,CVPR_2020,4,"Kara-Ali Aliev, Dmitry Ulyanov, and Victor Lempitsky. Neu- ral point-based graphics. arXiv preprint arXiv:1906.08240 , 2019. 3",3
SurfelGAN: Synthesizing Realistic Sensor Data for Autonomous Driving,CVPR_2020,5,"Martin Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasser- stein gan. arXiv preprint arXiv:1701.07875 , 2017. 3",3
Weakly Supervised Visual Semantic Parsing,CVPR_2020,1,"Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International Journal of Computer Vi- sion, 123(1):32–73, 2017.",11
Weakly Supervised Visual Semantic Parsing,CVPR_2020,2,"Xiaodan Liang, Lisa Lee, and Eric P Xing. Deep variation- structured reinforcement learning for visual relationship and attribute detection. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 848–857, 2017.",3
Weakly Supervised Visual Semantic Parsing,CVPR_2020,3,"Yong Liu, Ruiping Wang, Shiguang Shan, and Xilin Chen. Structure inference net: object detection using scene-level context and instance-level relationships. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6985–6994, 2018.",4
Weakly Supervised Visual Semantic Parsing,CVPR_2020,4,"Todd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine , 13(6):47–60, 1996.",1
Weakly Supervised Visual Semantic Parsing,CVPR_2020,5,"Hanwang Zhang, Zawlin Kyaw, Jinyang Yu, and Shih-Fu Chang. Ppr-fcn: weakly supervised visual relation detection via parallel pairwise r-fcn. In Proceedings of the IEEE Inter- national Conference on Computer Vision , pages 4233–4241, 2017.",4
A Programmatic and Semantic Approach to Explaining and Debugging,CVPR_2020,1,Anchor Method Repository. https://github.com/ marcotcr/anchor .5,2
A Programmatic and Semantic Approach to Explaining and Debugging,CVPR_2020,2,The SCENIC Probabilistic Programming Language. https://github.com/BerkeleyLearnVerify/ Scenic .2,2
A Programmatic and Semantic Approach to Explaining and Debugging,CVPR_2020,3,"Babak Alipanahi, Andrew Delong, Matthew T Weirauch, and Brendan J Frey. Predicting the sequence speciﬁcities of DNA-and RNA-binding proteins by deep learning. Nature biotechnology , 2015. 2",4
A Programmatic and Semantic Approach to Explaining and Debugging,CVPR_2020,4,"L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classiﬁ- cation and Regression Trees . Wadsworth and Brooks, Mon- terey, CA, 1984. 5",2
A Programmatic and Semantic Approach to Explaining and Debugging,CVPR_2020,5,"Shan Carter, Zan Armstrong, Ludwig Schubert, Ian John- son, and Chris Olah. Activation atlas. Distill , 2019. https://distill.pub/2019/activation-atlas. 2,3",5
Upright and Stabilized Omnidirectional Depth Estimation for,CVPR_2020,1,"Miriam Sch ¨onbein and Andreas Geiger. Omnidirectional 3d recon- struction in augmented manhattan worlds. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Sys- tems (IROS) , pages 716–723, 2014. 2",1
Upright and Stabilized Omnidirectional Depth Estimation for,CVPR_2020,2,"Bill Triggs, Philip F McLauchlan, Richard I Hartley, and Andrew W Fitzgibbon. Bundle adjustment—a modern synthesis. In Interna- tional Workshop on Vision Algorithms , pages 298–372, 1999. 3",4
Upright and Stabilized Omnidirectional Depth Estimation for,CVPR_2020,3,"Jean-Charles Bazin, C ´edric Demonceaux, Pascal Vasseur, and Inso Kweon. Rotation estimation and vanishing point extraction by om- nidirectional vision in urban environment. The International Journal of Robotics Research (IJRR) , 31(1):63–81, 2012. 2[2] Wenliang Gao and Shaojie Shen. Dual-ﬁsheye omnidirectional stereo. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) , pages 6715–6722, 2017. 1,2",4
Upright and Stabilized Omnidirectional Depth Estimation for,CVPR_2020,4,"Heiko Hirschmuller. Stereo processing by semiglobal matching and mutual information. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence (TPAMI) , 30(2):328–341, 2008. 2",2
Upright and Stabilized Omnidirectional Depth Estimation for,CVPR_2020,5,"Heiko Hirschmuller. Stereo processing by semiglobal matching and mutual information. IEEE Transactions on Pattern Analysis and Ma- chine Intelligence (TPAMI) , 30(2):328–341, 2008. 2",2
ClusterFit: Improving Generalization of Visual Representations,CVPR_2020,1,"Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper- vised visual representation learning by context prediction. In ICCV , pages 1422–1430, 2015.",3
ClusterFit: Improving Generalization of Visual Representations,CVPR_2020,2,"Tommaso Furlanello, Zachary C Lipton, Michael Tschan- nen, Laurent Itti, and Anima Anandkumar. Born again neural networks. arXiv preprint arXiv:1805.04770 , 2018.",5
ClusterFit: Improving Generalization of Visual Representations,CVPR_2020,3,"Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal- ski, Joanna Materzynska, Susanne Westphal, Heuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, et al. The “something something” video database for learning and evaluating visual common sense. InICCV , 2017.",11
ClusterFit: Improving Generalization of Visual Representations,CVPR_2020,4,"James Hays and Alexei A Efros. Im2gps: estimating geo- graphic information from a single image. In 2008 ieee con- ference on computer vision and pattern recognition , pages 1–8. IEEE, 2008.",1
ClusterFit: Improving Generalization of Visual Representations,CVPR_2020,5,"Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531 , 2015.",3
Learning Generative Models of Shape Handles,CVPR_2020,1,"Matheus Gadelha, Rui Wang, and Subhransu Maji. Mul- tiresolution Tree Networks for 3D Point Cloud Processing. InECCV , 2018.",3
Learning Generative Models of Shape Handles,CVPR_2020,2,"Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In In- ternational Conference on Computer Vision , 2019.",6
Learning Generative Models of Shape Handles,CVPR_2020,3,"Zhaoliang Lun, Matheus Gadelha, Evangelos Kalogerakis, Subhransu Maji, and Rui Wang. 3d shape reconstruction from sketches via multi-view convolutional networks. In In- ternational Conference on 3D Vision (3DV) , 2017.",5
Learning Generative Models of Shape Handles,CVPR_2020,4,"Lars Mescheder, Michael Oechsle, Michael Niemeyer, Se- bastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D reconstruction in function space. In The IEEE Conference on Computer Vision and Pattern Recognition , 2019.",5
Learning Generative Models of Shape Handles,CVPR_2020,5,"A. T. Miller, S. Knoop, H. I. Christensen, and P. K. Allen. Automatic grasp planning using shape primitives. In 2003 IEEE International Conference on Robotics and Automation (Cat. No.03CH37422) , volume 2, pages 1824–1829 vol.2, 2003.",2
Three-dimensional Reconstruction of Human Interactions,CVPR_2020,1,"Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total cap- ture: A 3d deformation model for tracking faces, hands, and bodies. In CVPR , 2018. 1",3
Three-dimensional Reconstruction of Human Interactions,CVPR_2020,2,"Diogo C Luvizon, David Picard, and Hedi Tabia. 2d/3d pose estimation and action recognition using multitask deep learn- ing. In CVPR , pages 5137–5146, 2018. 1",3
Three-dimensional Reconstruction of Human Interactions,CVPR_2020,3,"CMU graphics lab. CMU graphics lab motion capture data- base. 2009. http://mocap.cs.cmu.edu/ .1,2",2
Three-dimensional Reconstruction of Human Interactions,CVPR_2020,4,"Luca Ballan, Aparna Taneja, Jürgen Gall, Luc Van Gool, and Marc Pollefeys. Motion capture of hands in action using discriminative salient points. In ECCV , 2012. 2",5
Three-dimensional Reconstruction of Human Interactions,CVPR_2020,5,"Abdallah Benzine, Bertrand Luvison, Quoc Cuong Pham, and Catherine Achard. Deep, robust and single shot 3d multi- person human pose estimation from monocular images. In ICIP , 2019. 1",4
SAINT: Spatially Aware Interpolation NeTwork for Medical Slice Synthesis,CVPR_2020,1,"I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. C. Courville, and Y . Ben- gio. Generative adversarial networks. CoRR , abs/1406.2661, 2014. 3",2
SAINT: Spatially Aware Interpolation NeTwork for Medical Slice Synthesis,CVPR_2020,2,"Y . Han and J. C. Ye. Framing u-net via deep convolu- tional framelets: Application to sparse-view CT. CoRR , abs/1708.08333, 2017. 3",2
SAINT: Spatially Aware Interpolation NeTwork for Medical Slice Synthesis,CVPR_2020,3,"Y . Jo, S. W. Oh, J. Kang, and S. J. Kim. Deep video super- resolution network using dynamic upsampling ﬁlters with- out explicit motion compensation. In 2018 IEEE/CVF Con- ference on Computer Vision and Pattern Recognition , pages 3224–3232, June 2018. 3",2
SAINT: Spatially Aware Interpolation NeTwork for Medical Slice Synthesis,CVPR_2020,4,"S. Bakas et al. Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and ra- diomic features. Scientiﬁc data , 4, 2017. 1",2
SAINT: Spatially Aware Interpolation NeTwork for Medical Slice Synthesis,CVPR_2020,5,"H. Chen, Y . Zhang, Y . Chen, J. Zhang, W. Zhang, H. Sun, Y . Lv, P. Liao, J. Zhou, and G. Wang. Learn: Learned experts’ assessment-based reconstruction network for sparse-data ct. IEEE Transactions on Medical Imaging , 37(6):1333–1347, June 2018. 3",2
Image2StyleGAN++: How to Edit the Embedded Images?,CVPR_2020,1,"A. Creswell and A. A. Bharath. Inverting the generator of a generative adversarial network. IEEE Transactions on Neu- ral Networks and Learning Systems , 2018. 2",2
Image2StyleGAN++: How to Edit the Embedded Images?,CVPR_2020,2,"S. Tulyakov, M.-Y . Liu, X. Yang, and J. Kautz. Moco- gan: Decomposing motion and content for video generation. InThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2018. 2 8304",2
Image2StyleGAN++: How to Edit the Embedded Images?,CVPR_2020,3,"R. Webster, J. Rabin, L. Simon, and F. Jurie. Detecting over- ﬁtting of deep generative networks via latent recovery. 2019. 1",2
Image2StyleGAN++: How to Edit the Embedded Images?,CVPR_2020,4,"J. Yu, Z. Lin, J. Yang, X. Shen, X. Lu, and T. Huang. Free- form image inpainting with gated convolution. 2018. 1",2
Image2StyleGAN++: How to Edit the Embedded Images?,CVPR_2020,5,"J.-Y . Zhu, P. Kr ¨ahenb ¨uhl, E. Shechtman, and A. A. Efros. Generative visual manipulation on the natural image mani- fold. In Proceedings of European Conference on Computer Vision (ECCV) , 2016. 1",2
Gold Seeker: Information Gain from Policy Distributions,CVPR_2020,1,"Ehsan Abbasnejad, Justin Domke, and Scott Sanner. Loss- calibrated monte carlo action selection. In Twenty-Ninth AAAI Conference on Artiﬁcial Intelligence , 2015.",3
Gold Seeker: Information Gain from Policy Distributions,CVPR_2020,2,"Ehsan Abbasnejad, Qi Wu, Qinfeng Shi, and Anton van den Hengel. What’s to know? uncertainty as a guide to asking goal-oriented questions. In The IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) , June 2019.",4
Gold Seeker: Information Gain from Policy Distributions,CVPR_2020,3,"Laurent Itti and Pierre F. Baldi. Bayesian surprise attracts human attention. In Y . Weiss, B. Schölkopf, and J. C. Platt, editors, NIPS , pages 547–554. 2006.",1
Gold Seeker: Information Gain from Policy Distributions,CVPR_2020,4,"Siqi Liu, Zhenhai Zhu, Ning Ye, Sergio Guadarrama, and Kevin Murphy. Optimization of image description metrics using policy gradient methods. arXiv preprint arXiv:1612.00370 , 2016.",5
Gold Seeker: Information Gain from Policy Distributions,CVPR_2020,5,"Rui Zhao and V olker Tresp. Improving goal-oriented visual dialog agents via advanced recurrent nets with tempered pol- icy gradient. In IJCAI , 2018. 13459",1
Visual Chirality,CVPR_2020,1,"C. Doersch, A. Gupta, and A. A. Efros. Unsupervised visual representation learning by context prediction. ICCV , 2015. 3, 4",2
Visual Chirality,CVPR_2020,2,"C. Doersch, S. Singh, A. Gupta, J. Sivic, and A. A. Efros. What makes Paris look like Paris? SIGGRAPH , 31(4), 2012. 3,7",2
Visual Chirality,CVPR_2020,3,"R. Hartley. Cheirality invariants. In Proc. DARPA Image Understanding Workshop , 1993. 3",2
Visual Chirality,CVPR_2020,4,"B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning deep features for discriminative localization. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2921–2929, 2016. 3 12303",2
Visual Chirality,CVPR_2020,5,"Y . Hel-Or, S. Peleg, and H. Hel-Or. How to tell right from left. In CVPR , 1988. 3",2
Learning to Observe: Approximating Human Perceptual Thresholds for,CVPR_2020,1,"Patrick Cavanagh. The artist as neuroscientist. Nature , 434(7031):301, 2005.",2
Learning to Observe: Approximating Human Perceptual Thresholds for,CVPR_2020,2,"Elizabeth A Krupinski, Jeffrey Johnson, Hans Roehrig, John Nafziger, Jiahua Fan, and Jeffery Lubin. Use of a human visual system model to predict observer performance with crt vs lcd display of images. Journal of Digital Imaging , 17(4):258–263, 2004.",6
Learning to Observe: Approximating Human Perceptual Thresholds for,CVPR_2020,3,"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision , pages 2980–2988, 2017.",5
Learning to Observe: Approximating Human Perceptual Thresholds for,CVPR_2020,4,"Alan R Robertson. The cie 1976 color-difference formulae. Color Research & Application , 2(1):7–11, 1977.",1
Learning to Observe: Approximating Human Perceptual Thresholds for,CVPR_2020,5,"Alec Radford, Luke Metz, and Soumith Chintala. Un- supervised representation learning with deep convolu- tional generative adversarial networks. arXiv preprint arXiv:1511.06434 , 2015.",3
Efﬁcient Adversarial Training with Transferable Adversarial Examples,CVPR_2020,1,"Qi-Zhi Cai, Min Du, Chang Liu, and Dawn Song. Curricu- lum adversarial training. In International Joint Conferences on Artiﬁcial Intelligence (IJCAI) , 2018.",4
Efﬁcient Adversarial Training with Transferable Adversarial Examples,CVPR_2020,2,"Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In Inter- national Conference on Learning Representations (ICLR) , 2014.",3
Efﬁcient Adversarial Training with Transferable Adversarial Examples,CVPR_2020,3,"Matthias Hein and Maksym Andriushchenko. Formal guar- antees on the robustness of a classiﬁer against adversarial manipulation. In Advances in Neural Information Process- ing Systems , pages 2266–2276, 2017.",1
Efﬁcient Adversarial Training with Transferable Adversarial Examples,CVPR_2020,4,"Yunseok Jang, Tianchen Zhao, Seunghoon Hong, and Honglak Lee. Adversarial defense via learning to generate diverse attacks. In Proceedings of the IEEE International Conference on Computer Vision , pages 2740–2749, 2019.",4
Efﬁcient Adversarial Training with Transferable Adversarial Examples,CVPR_2020,5,"Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world attacks on deep learning models. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR) , 2019.",9
Advancing High Fidelity Identity Swapping for Forgery Detection,CVPR_2020,1,"and VGGFace [ 34]. While the HEAR-Net is trained using only a portion of faces that have Top- 10% heuristic er- rors in these datasets, and with additional augmentations of synthetic occlusions. Occlusion images are randomly sam- pled from the EgoHands [ 3], GTEA Hand2K [ 15,25,24] and object renderings from ShapeNet [ 9]. 4.1. Comparison with Previous Methods Qualitative Comparison : We compare our method with FaceSwap [ 2], Nirkin et al .[31], DeepFakes [ 1] and IP- GAN [ 5] on the FaceForensics++ [ 36] test images in Figure 5. Comparison with the latest work FSGAN [ 30] is shown in Figure 6. We can see that, since FaceSwap, Nirkin et al., DeepFakes, and FSGAN all follow the strategy that ﬁrst synthesizing the inner face region then blending it into the target face, as expected, they suffer from the blending in- consistency. All faces generated by these methods share exactly the same face contours with their target faces, and ignore the source face shapes (Figure 5rows 1-4, Figure 6 rows 1-2). Besides, their results can not well respect criti- cal informations from the target image, such as the lighting (Figure 5row 3, Figure 6rows 3-5), the image resolutions (Figure 5rows 2 and 4). IPGAN [ 5] suffers from decreased resolutions in all samples, due to its single-level attributes representation. IPGAN cannot well preserve expression of the target face, such as the closed eyes (Figure 5row 2). 5078 Target Source IPGAN FaceSwap Nirkin et al. DeepFakes Ours Figure 5: Comparison with FaceSwap [ 2], Nirkin et al.[31], Deep- Fakes [ 1], IPGAN [ 5] on FaceForensics++ [ 36] face images. Our results better preserve the face shapes of the source identities, and are also more faithful to the target attributes ( e.g. lightings, image resolutions). method ID retrieval ↑pose↓expression ↓ DeepFakes [ 1] 81.96 4.14 2.57 FaceSwap [ 2] 54.19 2.51 2.14 Nirkin et al.[31] 76.57 3.29 2.33 IPGAN [ 5] 82.41 4.04 2.50 Ours 97.38 2.96 2.06 Table 1: Comparison on FaceForensics++ videos. Our method addresses all these issues well. We achieve higher ﬁdelity by well preserving the face shapes of the source (instead of the target), and faithfully respecting the lighting and image resolution of the target (instead of the source). Our method also has the ability to go beyond FS- GAN [ 30] to handle occlusions. Quantitative Comparison : The experiment is constructed on FaceForensics++ [ 36] dataset. For FaceSwap [ 2] and DeepFakes [ 1], the test set consists of 10K face images for each method by evenly sampled 10 frames from each video clip. For IPGAN [ 5], Nirkin et al.[31] and our method, 10Kface images are generated with the same source and target image pairs as the other methods. Then we conduct quantitative comparison with respect to three metrics: ID retrieval ,pose error andexpression error . We extract identity vector using a different face recogni- tion model [ 41] and adopt the cosine similarity to measure the identity distance. For each swapped face from the test set, we search the nearest face in all FaceForensics++ origi- nal video frames and check whether it belongs to the correct source video. The averaged accuracy of all such retrievals is reported as the ID retrieval in Table 1, serving to measure identity preservation ability. Our method achieves higher ID retrieval score with a large margin. We use a pose estimator [ 37] to estimate head pose and a 3D face model [ 10] to retrieve expression vectors. We report the L-2 distances of pose and expression vectors be- Target Source FSGAN OursFigure 6: Comparison with FSGAN [ 30]. Besides the advantages in face quality and ﬁdelity to inputs, our results preserve common occlusions as good as FSGAN. Please also refer to Figures 1,10 and11for more challenging cases. method id. attr. realism DeepFakes [ 1] 13.7 6.8 6.1 FaceSwap [ 2] 12.1 23.7 6.8 Nirkin et al.[31]21.3 7.4 4.2 Ours 52.9 62.1 82.9 Table 2: User study results. We show the averaged selection per- centages of each method. tween the swapped face and its target face in Table 1as the pose and the expression errors. Our method is advantageous in expression preservation while comparable with others in pose preservation. We do not use the face landmark com- parison as [ 30], since face landmarks involve identity infor- mation which should be inconsistent between the swapped face and the target face. Human Evaluation : Three user studies are conducted to evaluate the performance of the proposed model. We let the users select: i) the one having the most similar identity with the source face ; ii) the one sharing the most similar head pose, face expression and scene lighting with the target im- age; iii) the most realistic one . In each study unit, two real face images, the source and the target, and four reshufﬂed face swapping results generated by FaceSwap [ 2], Nirkin et al.[31], DeepFakes [ 1] and ours, are presented. We ask users to select one face that best matches our description. 5079 Source Target Add Cat C ompressedAEI-Net Figure 7: Comparing AEI-Net with three baseline models. The two models Add andCatare for ablation studies of the adaptive embedding integration. The model Compressed is for ablating multi-level attributes representation. For each user, 20 face pairs are randomly drawn from the 1K FaceForensics++ test set without duplication. Finally, we collect answers from 100 human evaluators. The aver- aged selection percentage for each method on each study is presented in Table 2. It shows that our model surpasses the other three methods all in large margins. 4.2. Analysis of the Framework Adaptive Embedding Integration : To verify the necessity of adaptive integration using attentional masks, we compare AEI-Net with two baseline models: i) Add: element-wise plus operations is adopted in AAD layers instead of using masksMkas in Equation 5. The output activation hk out of this model is directly calculated with hk out=Ak+Ik; ii)Cat: element-wise concatenation is adopted without us- ing masks Mk. The output activation becomes hk out= Concat[Ak,Ik]. Results of the two baseline models, as well as the AEI-Net, are compared in Figure 7. Without a soft mask for fusing embeddings adaptively, the faces gen- erated by baseline models are relatively blurry and contain lots of ghosting artifacts. We also visualize the masks Mkof AAD layers on dif- ferent levels in Figure 8, where a brighter pixel indicates a higher weight for identity embedding in Equation 5. It shows that the identity embedding takes more effect in low level layers. Its effective region becomes sparser in mid- dle levels, where it activates only in some key regions that strongly relates to the face identity, such as the locations of eyes, mouth and face contours. Multi-level Attributes : To verify whether it is necessary to extract multi-level attributes, we compare with another baseline model called Compressed , which shares the same network structure with AEI-Net, but only utilizes the ﬁrst three level embeddings zk att,k=1,2,3. Its last embedding 𝑋𝑠 𝑋𝑡෠𝑌𝑠,𝑡 4 × 4 16 × 16 8 × 8 32 × 32 64 × 64 128 × 128Figure 8: Visualizing attentional masks Mkof AAD layers on different feature levels. These visualizations reﬂect that identity embeddings are mostly effective in low and middle feature levels. Query Top-1 Top-2 Top-3…… Figure 9: Query results using attributes embedding. z3 attis fed into all higher level AAD integrations. Its results are also compared in Figure 7. Similar to IPGAN [ 5], its results suffer from artifacts like blurriness, since a lot of attributes information from the target images are lost. To understand what is encoded in the attributes embed- ding, we concatenate the embeddings zk att(bilinearly up- sampled to 256⇥256and vectorized) from all levels as a uniﬁed attribute representation. We conduct PCA to reduce vector dimensions as 512. We then perform tests querying faces from the training set with the nearest L-2 distances of such vectors. The three results illustrated in Figure 9verify our intention, that the attributes embeddings can well reﬂect face attributes, such as the head pose, hair color, expression and even the existence of sunglasses on the face. Thus it also explains why our AEI-Net sometimes can preserve oc- clusions like sunglasses on the target face even without a second stage (Figure 10(8)). Second Stage Reﬁnement : Multiple samples are displayed with both one-stage results ˆYs,tand two-stage results Ys,t in Figure 10. It shows that the AEI-Net is able to generate high-ﬁdelity face swapping results, but sometimes its output ˆYs,tdoes not preserve occlusions in the target. Fortunately, the HEAR-Net in the second stage is able to recover them. The HEAR-Net can handle occlusions of various kinds, such as the medal (1), hand (2), hair (3), face painting (4), mask (5), translucent object (6), eyeglasses (7), headscarf (8) and ﬂoating text (9). Besides, it is also able to correct the color-shift that might occasionally happen in ˆYs,t(10). Moreover, the HEAR-Net can help rectify the face shape when the target face has a very large pose (6). 4.3. More Results on Wild Faces Furthermore, we demonstrate the strong capability of FaceShifter by testing wild face images downloaded from the Internet. As shown in Figure 11, our method can han- dle face images under various conditions, including large 5080 𝑋 𝑡𝑋 𝑠 ෠𝑌𝑠,𝑡 𝑌𝑠,𝑡 (1) (2) (3) (4) (5) (6) (7) (8) (9) (10) Figure 10: Second-stage reﬁning results presenting the strong adaptability of HEAR-Net on various kinds of errors, including occlusions, reﬂections, slightly shifted pose and color etc. Target Source Result Figure 11: Our face swapping results on wild face images under various challenging conditions. All results are generated using a single well-trained two-stage model. poses, uncommon lightings and occlusions of very chal- lenging kinds. 4.4. Examining Forged Face Detection Algorithms Finally, we examine the performance of different face forgery detection algorithms on our face swapping results. First, we randomly generate 5,000face swapping images and5,000 real images. Then we apply the model from FF++ [ 36] and Face X-Ray [ 23] and show the detection re- sults in Table 3. We can notice that Face X-Ray has impres- sive results on our generated images. 5. Conclusions In this paper, we proposed a novel framework named FaceShifter for high ﬁdelity and occlusion aware face swap-Methods AUC AP EER FF++ [ 36] 52.22 52.87 0.4805 Face X-Ray [ 23]96.82 90.53 0.0956 Table 3: Results in terms of AUC, AP and EER for FF++ [ 36] and Face X-ray [ 23] on our generated faces. ping. The AEI-Net in the ﬁrst stage adaptively integrates the identity and the attributes for synthesizing high ﬁdelity re- sults. The HEAR-Net in the second stage recovers anomaly region in a self-supervised way without any manual anno- tations. The proposed framework shows superior perfor- mance in generating realistic face images given any face pairs without subject speciﬁc training. Extensive exper- iments demonstrate that the proposed framework signiﬁ- cantly outperforms previous face swapping methods, setting up a new benchmark for face forensics researches. 5081 References",2
Advancing High Fidelity Identity Swapping for Forgery Detection,CVPR_2020,2,"performs face reenactment and face swapping together. It follows a similar reenact and blend strategy with [ 32,27]. Although FSGAN utilizes an occlusion-aware face segmen- tation network for preserving target occlusions, it hardly re- spects target attributes like the lighting or image resolution, it can neither preserve the face shape of the source identity. 3. Methods Our method requires two input images, i.e., a source im- ageXsto provide identity and a target image Xtto provide attributes, e.g., pose, expression, scene lighting and back- ground. The swapped face image is generated through a two-stage framework, called FaceShifter. In the ﬁrst stage, we use an Adaptive Embedding Integration Network (AEI- Net) to generate a high ﬁdelity face swapping result ˆYs,t based on information integration. In the second stage, we use the Heuristic Error Acknowledging Network (HEAR- Net) to handle the facial occlusions and reﬁne the result, the ﬁnal result is denoted by Ys,t. 3.1. Adaptive Embedding Integration Network In the ﬁrst stage, we aim to generate a high ﬁdelity face imageˆYs,t, which should preserve the identity of the source Xsand the attributes ( e.g. pose, expression, lighting, back- ground) of the target Xt. To achieve this goal, our method consist of 3 components: i) the Identity Encoder zid(Xs), which extracts identity from the source image Xs; ii) the Multi-level Attributes Encoder zatt(Xt), which extracts at- tributes of the target image Xt; iii)Adaptive Attentional De- normalization (AAD) Generator , which generates swapped face image. Figure 3(a) shows whole network structure. Identity Encoder : We use a pretrained state-of-the-art face recognition model [ 13] as identity encoder. The identity embedding zid(Xs)is deﬁned to be the last feature vec- tor generated before the ﬁnal FC layer. We believe that by training on a large quantity of 2D face data, such a facerecognition model can provide more representative identity embeddings than the 3D-based models like 3DMM [ 7,8]. Multi-level Attributes Encoder : Face attributes, such as pose, expression, lighting and background, require more spatial informations than identity. In order to preserve such details, we propose to represent the attributes embedding as multi-level feature maps, instead of compressing it into a single vector as previous methods [ 5,29]. In speciﬁc, we feed the target image Xtinto a U-Net-like structure. Then we deﬁne the attributes embedding as the feature maps gen- erated from the U-Net decoder. More formally, we deﬁne zatt(Xt)=  z1 att(Xt),z2 att(Xt),···zn att(Xt) ,(1) wherezk att(Xt)represents the k-th level feature map from the U-Net decoder, nis the number of feature levels. Our attributes embedding network does not require any attribute annotations, it extracts the attributes using self- supervised training: we require that the generated swapped faceˆYxtand the target image Xthave the same attributes embedding. The loss function will be introduce in Equa- tion7. In the experimental part (Section 4.2), we also study what the attributes embedding has learned. Adaptive Attentional Denormalization Generator :W e then integrate such two embeddings zid(Xs)andzatt(Xt) for generating a raw swapped face ˆYs,t. Previous meth- ods [ 5,29] simply integrate them through feature concate- nation. It will lead to relatively blurry results. Instead, we propose a novel Adaptive Attentional Denormalization (AAD) layer to accomplish this task in a more adaptive fashion. Inspired by the mechanisms of SPADE [ 33] and AdaIN [ 14,16], the proposed AAD layers leverage denor- malizations for feature integration in multiple feature levels. As shown in Figure 3(c), in the k-th feature level, let hk in denote the activation map that is fed into an AAD layer, which should be a 3D tensor of size Ck⇥Hk⇥Wk, with Ckbeing the number of channels and Hk⇥Wkbeing the spatial dimensions. Before integration, we perform batch normalization [ 17] onhk in: ¯hk=hk in µk σk. (2) Hereµk2RCkandσk2RCkare the means and stan- dard deviations of the channel-wise activations within hk in’s mini-batch. Then, we design 3 parallel branches from ¯hk for 1) attributes integration, 2) identity integration, 3) adap- tively attention mask. For attributes embedding integration, let zk attbe the at- tributes embedding on this feature level, which should be a 3D tensor of size Ck att⇥Hk⇥Wk. In order to integrate zk attinto the activation, we compute an attribute activation Akby denormalizing the normalized ¯hkaccording to the attributes embedding, formulated as Ak=γk att⌦¯hk+βk att, (3) 5076 𝑋𝑠AAD ResBlk… … AAD ResBlkAAD ResBlkAAD ResBlkAEI-Net …AADReLU3x3 ConvAADReLU3x3 ConvAAD ResBlk 𝑋𝑡 𝒛𝑖𝑑𝒛𝑎𝑡𝑡1𝒛𝑎𝑡𝑡2 𝒛𝑎𝑡𝑡𝑛−1𝒛𝑎𝑡𝑡𝑛 ෠𝑌𝑠,𝑡𝒛𝑎𝑡𝑡1,2,…,𝑛 𝒛𝑖𝑑 Identity Encoderconv FCBatch NormConv+ Sigmoid1 − 𝑴𝑘⊗ 𝑨𝑘+ 𝑴𝑘⊗ 𝑰𝑘𝒛𝑎𝑡𝑡𝑘 𝛾𝑎𝑡𝑡𝑘𝛽𝑎𝑡𝑡𝑘 𝛾𝑖𝑑𝑘 𝛽𝑖𝑑𝑘𝒛𝑖𝑑𝒉𝑖𝑛𝑘 ഥ𝒉𝑘𝒉𝑜𝑢𝑡𝑘AAD 𝑴𝑘 (a) (b) (c)Multi-level Attributes Encoder AAD Generator Figure 3: AEI-Net for the ﬁrst stage. AEI-Net is composed of an Identity Encoder, a Multi-level Attributes Encoder, and an AAD- Generator. The AAD-Generator integrates informations of identity and attributes in multiple feature levels using cascaded AAD ResBlks, which is built on AAD layers. whereγk attandβk attare two modulation parameters both convolved from zk att. They share the same tensor dimen- sions with ¯hk. The computed γk attandβk attare multiplied and added to ¯hkelement-wise. For identity embedding integration, let zk idbe the identity embedding, which should be a 1D vector of size Cid.W e also integrate zk idby computing an identity activation Ikin a similar way to integrating attributes. It is formulated as Ik=γk id⌦¯hk+βk id, (4) whereγk id2RCkandβk id2RCkare another two modula- tion parameters generated from zidthrough FC layers. One key design of the AAD layer is to adaptively ad- just the effective regions of the identity embedding and the attributes embedding, so that they can participate in synthe- sizing different parts of the face. For example, the identity embedding should focus relatively more on synthesizing the face parts that are most discriminative for distinguish- ing identities, e.g. eyes, mouth and face contour. There- fore, we adopt an attention mechanism into the AAD layer. Speciﬁcally, we generate an attentional mask Mkusing¯hk through convolutions and a sigmoid operation. The values ofMkare between 0and1. Finally, the output of this AAD layer hk outcan be ob- tained as a element-wise combination of the two activations AkandIk, weighted by the mask Mk, as shown in Fig- ure3(c). It is formulated as hk out=( 1 Mk)⌦Ak+Mk⌦Ik. (5) The AAD-Generator is then built with multiple AAD layers. As illustrated in Figure 3(a), after extracting the identity embedding zidfrom source Xs, and the attributes embedding zattfrom target Xt, we cascade AAD Residual Blocks (AAD ResBlks) to generate the swapped face ˆYs,t, the structure of the AAD ResBlks is shown in Figure 3(b). For the AAD ResBlk on the k-th feature level, it ﬁrst takes the up-sampled activation from the previous level as input, then integrates this input with zidandzk att. The ﬁnal output imageˆYs,tis convolved from the last activation.Training Losses We utilize adversarial training for AEI- Net. LetLadvbe the adversarial loss for making ˆYs,treal- istic. It is implemented as a multi-scale discriminator [ 33] on the downsampled output images. In addition, an iden- tity preservation loss is used to preserve the identity of the source. It is formulated as Lid=1 cos(zid(ˆYs,t),zid(Xs)), (6) wherecos(·,·)represents the cosine similarity of two vec- tors. We also deﬁne the attributes preservation loss as L- 2 distances between the multi-level attributes embeddings fromXtandˆYs,t. It is formulated as Latt=1 2nX k=1   zk att(ˆYs,t) zk att(Xt)   2 2. (7) When the source and target images are the same in a training sample, we deﬁne a reconstruction loss as pixel level L-2 distances between the target image XtandˆYs,t Lrec=8 < :1 2   ˆYs,t Xt   2 2ifXt=Xs 0 otherwise. (8) The AEI-Net is ﬁnally trained with a weighted sum of above losses as LAEI-Net=Ladv+λattLatt+λidLid+λrecLrec,(9) withλatt=λrec= 10,λid=5. The trainable modules of AEI-Net include the Multi-level Attributes Encoder and the ADD-Generator. 3.2. Heuristic Error Acknowledging Reﬁnement Network Although the face swap result ˆYs,tgenerated with AEI- Net in the ﬁrst stage can well retain target attributes like pose, expression and scene lighting, it often fails to pre- serve the occlusions appeared on the target face Xt. Previ- ous methods [ 31,30] address face occlusions with an addi- tional face segmentation network. It is trained on face data containing occlusion-aware face masks, which require lots of manual annotations. Besides, such a supervised approach may hardly recognize unseen occlusion types. 5077 ෠𝑌𝑠,𝑡 𝑋 𝑡 𝑋 𝑠෠𝑌𝑡,𝑡 𝑌𝑠,𝑡 … …෠𝑌𝑠,𝑡 𝑋 𝑡− ෠𝑌𝑡,𝑡 HEAR-Net (a) (b) Figure 4: HEAR-Net for the second stage. ˆYt,tis the reconstruc- tion of the target image Xt,i.e.,ˆYt,t=AEI-Net (Xt,Xt).ˆYs,t is the swapped face from the ﬁrst stage. We proposed a heuristic method to handle facial occlu- sions. As shown in Figure 4(a), when the target face was occluded, some occlusions might disappear in the swapped face, e.g., the hair covering the face or the chains hang from the turban. Meanwhile, we observe that if we feed the same image as both the source and target images into a well trained AEI-Net, these occlusions would also disappear in the reconstructed image. Thus, the error between the recon- structed image and its input can be leveraged to locate face occlusions. We call it the heuristic error of the input image, since it heuristically indicates where anomalies happen. Inspired by the above observation, we make use of a novel HEAR-Net to generate a reﬁned face image. We ﬁrst get the heuristic error of the target image as ∆Yt=Xt AEI-Net (Xt,Xt). (10) Then we feed the heuristic error ∆Ytand the result of the ﬁrst stage ˆYs,tinto a U-Net structure, and obtain the reﬁned imageYs,t: Ys,t=HEAR-Net (ˆYs,t,∆Yt). (11) The pipeline of HEAR-Net is illustrated in Figure 4(b). We train HEAR-Net in a fully self-supervised way, with- out using any manual annotations. Given any target face imageXt, with or without occlusion regions, we utilize the following losses for training HEAR-Net. The ﬁrst is an identity preservation loss to preserve the identity of the source. Similar as stage one, it is formulated as L0 id=1 cos(zid(Ys,t),zid(Xs)). (12) The change loss L0 chgguarantees the consistency between the results of the ﬁrst stage and the second stage: L0 chg=   ˆYs,t Ys,t   . (13) The reconstruction loss L0 recrestricts that the second stage is able to reconstruct the input when the source and target images are the same:L0 rec=( 1 2kYs,t Xtk2 2ifXt=Xs 0 otherwise. (14) Since the number of occluded faces is very limited in most face datasets, we propose to augment data with syn- thetic occlusions. The occlusions are randomly sampled from a variety of datasets, including the EgoHands [ 3], GTEA Hand2K [ 15,25,24] and ShapeNet [ 9]. They are blended onto existing face images after random rotations, rescaling and color matching. Note that we do not utilize any occlusion mask supervision during training, even from these synthetic occlusions . Finally, HEAR-Net is trained with a sum of above losses: LHEAR-Net=L0 rec+L0 id+L0 chg. (15) 4. Experiments Implementation Detail : For each face image, we ﬁrst align and crop the face using ﬁve point landmarks extracted with",1
Advancing High Fidelity Identity Swapping for Forgery Detection,CVPR_2020,3,"Ryota Natsume, Tatsuya Yatagawa, and Shigeo Morishima. Fsnet: An identity-aware generative model for image-based face swapping. In Asian Conference on Computer Vision , pages 117–132. Springer, 2018. 1,2,3 5082",3
Advancing High Fidelity Identity Swapping for Forgery Detection,CVPR_2020,4,"Yuan Lin, Shengjin Wang, Qian Lin, and Feng Tang. Face swapping under large pose variations: A 3d model based ap- proach. In 2012 IEEE International Conference on Multime- dia and Expo , pages 333–338. IEEE, 2012. 1",4
Advancing High Fidelity Identity Swapping for Forgery Detection,CVPR_2020,5,"Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196 , 2017. 5",4
Illumination-based Transformations Improve Skin Lesion Segmentation in,CVPR_2020,1,"Lei Bi, Jinman Kim, Euijoon Ahn, and Dagan Feng. Au- tomatic skin lesion analysis using large-scale dermoscopy images and deep residual networks. arXiv preprint arXiv:1703.04197 , 2017.",4
Illumination-based Transformations Improve Skin Lesion Segmentation in,CVPR_2020,2,"M. Emre Celebi, Hitoshi Iyatomi, and Gerald Schaefer. Con- trast enhancement in dermoscopy images by maximizing a histogram bimodality measure. In 2009 16th IEEE Interna- tional Conference on Image Processing (ICIP) . IEEE, Nov. 2009.",2
Illumination-based Transformations Improve Skin Lesion Segmentation in,CVPR_2020,3,"Noel CF Codella, David Gutman, M Emre Celebi, Brian Helba, Michael A Marchetti, Stephen W Dusza, Aadi Kalloo, Konstantinos Liopyris, Nabin Mishra, Harald Kittler, et al. Skin lesion analysis toward melanoma detection: A challenge at the 2017 International Symposium on Biomedi- cal Imaging (ISBI), hosted by the International Skin Imaging Collaboration (ISIC). In 2018 IEEE 15th International Sym- posium on Biomedical Imaging (ISBI 2018) , pages 168–172. IEEE, 2018.",11
Illumination-based Transformations Improve Skin Lesion Segmentation in,CVPR_2020,4,"M Hiraoka, M Firbank, M Essenpreis, M Cope, SR Arridge, P Van Der Zee, and DT Delpy. A Monte Carlo investigation of optical pathlength in inhomogeneous tissue and its appli- cation to near-infrared spectroscopy. Physics in Medicine & Biology , 38(12):1859, 1993.",7
Illumination-based Transformations Improve Skin Lesion Segmentation in,CVPR_2020,5,"Zahra Mirikharaji and Ghassan Hamarneh. Star shape prior in fully convolutional networks for skin lesion segmenta- 9 tion. In International Conference on Medical Image Com- puting and Computer-Assisted Intervention , pages 737–745. Springer, 2018.",1
Cascaded Deep Video Deblurring Using Temporal Sharpness Prior,CVPR_2020,1,"Shuochen Su, Mauricio Delbracio, Jue Wang, Guillermo S- apiro, Wolfgang Heidrich, and Oliver Wang. Deep video deblurring for hand-held cameras. In CVPR , pages 237–246, 2017. 1,2,4,5,6,7,8",6
Cascaded Deep Video Deblurring Using Temporal Sharpness Prior,CVPR_2020,2,"Seungjun Nah, Tae Hyun Kim, and Kyoung Mu Lee. Deep multi-scale convolutional neural network for dynamic scene deblurring. In CVPR , pages 257–265, 2017. 5,6,7",3
Cascaded Deep Video Deblurring Using Temporal Sharpness Prior,CVPR_2020,3,"Jinshan Pan, Deqing Sun, Hanspeter Pﬁster, and Ming- Hsuan Yang. Deblurring images via dark channel prior. IEEE TPAMI , 40(10):2315–2328, 2018. 5,7",4
Cascaded Deep Video Deblurring Using Temporal Sharpness Prior,CVPR_2020,4,"Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. PWC-Net: CNNs for optical ﬂow using pyramid, warping, and cost volume. In CVPR , pages 8934–8943, 2018. 3,4,5",4
Cascaded Deep Video Deblurring Using Temporal Sharpness Prior,CVPR_2020,5,"Xin Tao, Hongyun Gao, Xiaoyong Shen, Jue Wang, and Ji- aya Jia. Scale-recurrent network for deep image deblurring. InCVPR , pages 8174–8182, 2018. 4,5,6,7",5
Rethinking Data Augmentation for Image Super-resolution:,CVPR_2020,1,"Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei Zhang. Toward real-world single image super-resolution: A new benchmark and a new model. arXiv preprint arXiv:1904.00523 , 2019. 1,2,3,5,7",5
Rethinking Data Augmentation for Image Super-resolution:,CVPR_2020,2,"Terrance DeVries and Graham W Taylor. Improved regular- ization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552 , 2017. 1,2,3",1
Rethinking Data Augmentation for Image Super-resolution:,CVPR_2020,3,"HR Sheikh. Live image quality assessment database release 2.http://live. ece. utexas. edu/research/quality , 2005. 8",2
Rethinking Data Augmentation for Image Super-resolution:,CVPR_2020,4,"Jonathan Tompson, Ross Goroshin, Arjun Jain, Yann LeCun, and Christoph Bregler. Efﬁcient object localization using convolutional networks. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 648–656, 2015. 2,3",5
Rethinking Data Augmentation for Image Super-resolution:,CVPR_2020,5,"Ruicheng Feng, Jinjin Gu, Yu Qiao, and Chao Dong. Sup- pressing model overﬁtting for image super-resolution net- works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops , pages 0–0, 2019. 1,2",4
The 1st Challenge on Remote Physiological Signal Sensing (RePSS),CVPR_2020,1,"Guha Balakrishnan, Fredo Durand, and John Guttag. Detect- ing pulse from head motions in video. In Proc. IEEE CVPR , pages 3430–3437, 2013. 1",3
The 1st Challenge on Remote Physiological Signal Sensing (RePSS),CVPR_2020,2,"Gerard De Haan and Vincent Jeanne. Robust pulse rate from chrominance-based rppg. IEEE Trans. Biomed. Eng. , 60(10):2878–2886, 2013. 1,5",1
The 1st Challenge on Remote Physiological Signal Sensing (RePSS),CVPR_2020,3,"Ming-Zher Poh, Daniel J McDuff, and Rosalind W Picard. Non-contact, automated cardiac pulse measurements using video imaging and blind source separation. Opt. Express , 18(10):10762–10774, 2010. 1",3
The 1st Challenge on Remote Physiological Signal Sensing (RePSS),CVPR_2020,4,"Xiaobai Li, Jie Chen, Guoying Zhao, and Matti Pietikainen. Remote heart rate measurement from face videos under re- alistic situations. In Proc. IEEE CVPR , pages 4264–4271, 2014. 1",4
The 1st Challenge on Remote Physiological Signal Sensing (RePSS),CVPR_2020,5,"Xuesong Niu, Shiguang Shan, Hu Han, and Xilin Chen. Rhythmnet: End-to-end heart rate estimation from face via spatial-temporal representation. IEEE Trans. Image Process- ing, 2019. 1,2",4
Density Map Guided Object Detection in Aerial Images,CVPR_2020,1,"F. Ozge Unel, Burak O. Ozkalayci, and Cevahir Cigla. The power of tiling for small object detection. In The IEEE Con- ference on Computer Vision and Pattern Recognition (CVPR) Workshops , June 2019.",2
Density Map Guided Object Detection in Aerial Images,CVPR_2020,2,"Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Proceedings of the 28th International Conference on Neural Information Processing Systems - Vol- ume 1 , NIPS’15, page 91–99, Cambridge, MA, USA, 2015. MIT Press.",4
Density Map Guided Object Detection in Aerial Images,CVPR_2020,3,"T. Lin, P. Goyal, R. Girshick, K. He, and P. Doll ´ar. Focal loss for dense object detection. In 2017 IEEE International Conference on Computer Vision (ICCV) , pages 2999–3007, Oct 2017.",2
Density Map Guided Object Detection in Aerial Images,CVPR_2020,4,"Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng Yang Fu, and Alexander C. Berg. SSD: Single shot multibox detector. In Lecture Notesin Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics) , 2016.",7
Density Map Guided Object Detection in Aerial Images,CVPR_2020,5,"Jianan Li, Xiaodan Liang, Yunchao Wei, Tingfa Xu, Jiashi Feng, and Shuicheng Yan. Perceptual Generative Adver- sarial Networks for Small Object Detection. arXiv e-prints , page arXiv:1706.05274, Jun 2017.",6
DUNIT: Detection-based Unsupervised Image-to-Image Translation,CVPR_2020,1,"Ross B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR , abs/1311.2524, 2013. 3",2
DUNIT: Detection-based Unsupervised Image-to-Image Translation,CVPR_2020,2,"Lixin Duan, Ivor Tsang, and Dezhi Xu. Domain trans- fer multiple kernel learning. IEEE Transactions on Pattern Analysis and Machine Intelligence , 34:1 – 1, 05 2011. 3",3
DUNIT: Detection-based Unsupervised Image-to-Image Translation,CVPR_2020,3,"Yuhua Chen, Wen Li, Christos Sakaridis, Dengxin Dai, and Luc Van Gool. Domain adaptive faster R-CNN for object detection in the wild. CoRR , abs/1803.03243, 2018. 1,3,7, 8",5
DUNIT: Detection-based Unsupervised Image-to-Image Translation,CVPR_2020,4,"Yun-Chun Chen, Yen-Yu Lin, Ming-Hsuan Yang, and Jia- Bin Huang. Crdoco: Pixel-level domain transfer with cross- domain consistency, 2020. 3",4
DUNIT: Detection-based Unsupervised Image-to-Image Translation,CVPR_2020,5,"Fabio Maria Carlucci, Lorenzo Porzi, Barbara Caputo, Elisa Ricci, and Samuel Rota Bulò. Autodial: Automatic domain alignment layers. CoRR , abs/1704.08082, 2017. 3",5
Self-Supervised Object Detection and Retrieval Using Unlabeled Videos,CVPR_2020,1,"top references to be manually annotated for testing purposes only. We call this data set (and its annotations) of 10 objects HowTo10 and will release it publicly. We include in our corpus challenging nouns such as Bike that corresponds to both Bicycle and Motorbike, or Gun that refers to Pistol, Ri- ﬂe and Glue Gun. This results a total of 4,537 frames from the videos, with an average of 453 frames per-object. Our transcript-based frame selection introduces 55% noisily la- beled frames on average (i.e., only 45% of object frames are correctly labeled and contain the object of interest on aver- age). The statistics of our data set and the recall rate for the Selected Search region proposal, are shown in Table 1. 4.2. Comparison to SoTA models and upper bound We compare our results to two different state-of-the-art weakly-supervised models - PCL [23] and SPN [30] in a setting of a single-class detection, as well as multi-class de- tection. For each model we use the code from the authors’ GitHub. For SPN code we found that during test time the authors discard predictions that don’t match the image-level labels. For a fair evaluation and comparison, we do not use image-level labels during test time. For the single-class test, to mimic the self-supervised scenario, we fed both models with noisy positive samples as well as clean negative sam- ples. For each object category, the models were trained 6 (a) Drum (b) Bike (c) Guitar Figure 4: Potential score . Object & Background samples are denoted by redandblue, respectively. Opacity repre- sents score (darker points have higher score). Object sam- ples ( red) are more likely to be darker and, thus sampled as positive regions, reducing the label noise.on two classes: the positive class with noisy labels, and the negative class labeled as background. For the multi- class comparison, both models were trained and tested with multi-class labeled data. As an upper bound reference, we report the performance of the fully-supervised binary de- tection version of our method, where the detector, namely object region classiﬁer, is trained with ground truth labels. These comparisons emphasize the challenge of learning ob- ject detection from our unlabeled and unconstrained videos, manifesting motion blur, extreme views (far-ﬁeld and close- ups), and large occlusions. Qualitative retrieval examples are shown in Fig. 3. 4.3. Evaluation We randomly split our data into 80%-20% train-test sets containing mutually exclusive frames, and evaluate the per- formance on 3random folds. The training and test sets con- tain on average 362 and 91 frames per object, respectively. Since our task is self-supervised, we allowed frames from the same video to participate in both train and test sets. For quantitative evaluation, we manually annotated the bound- ing boxes of 10object categories. Note that annotations were used only for the testing and were not available at training, to keep the method annotation-free. As the eval- uation criterion, we use the standard detection mean av- erage precision (mAP) at intersection-over-union (IoU) of 0.5. For the case of single-class detection, training and test- ing were performed for each object category separately, ac- cording to the ”pseudo-labels” acquired from the transcript and on the selected frames of the current object (i.e., the noisy positive set). For a fair comparison to WSOD multi- class models, we tested each of our single-class models on all frames. Note that since the positive set is noisy (see Table 1), the evaluation was in fact also applied to frames without the objects. 4.4. Detection results To the best of our knowledge, this is the ﬁrst self- supervised object detection method trained and evaluated using standard evaluation practices. The results are sum- marized in Table 2, for single-class and multi-class detec- tion. For single-class detection, the performance of stan- dard WSOD methods suffer greatly. In fact most of the times it fails to converge at all. However, since our model does not explicitly rely on discriminative parts, it is able to perform well even with single-class noisy data. Our single- class model attains a mAP of 17.7%, while PCL [23] and SPN [30] reach mAP of 7.2% and 0.8% (computed only on converged objects), respectively. While SPN attains an mAP of 11.9% for the multi-class detection task, our model obtains 16.0%, mAP, showing ∼34% relative improvement. This is an outcome of lack of robustness of SPN to label noise. Interestingly, when SPN is trained only on clean la- 7 beled data (with correct weak labels), it only reaches 13.5% mAP. This indicates the difﬁculty involved in object detec- tion in our HowTo10 data set (due to variety, size, video artifacts, etc.). Yet, this is still lower than the 16.0% mAP that our model attains, when trained on noisy data. Since the detection part of our model is basic, we attribute this to the fact that multiple single-class models optimized sepa- rately perform better (as ensemble), or as good as a single multi-class model that is optimized for all objects at once - a setting which SPN fails to converge with. As the upper bound, we present the results from our trained region clas- siﬁer (see Fig. 2) when fed with true region labels (starting with the SS Recall as in Table 1). Although we are still far from this extreme labeling scenario, we believe that our results and labeled data can motivate others to tackle the challenging task of self-supervised detection. 4.5. Retrieval results Our model is able to learn and retrieve the appearance andnames of objects without any manual labeling involved, simply by ”watching and listening” to unconstrained un- labeled videos from the How2 data set [16]. To retrieve the topninstances, for a given object, we choose the clus- ter with highest potential score (choosing other top scor- ing clusters is possible as well). Filtering out regions using DSD, we extract the nclosest samples to the cluster’s cen- ter. Qualitative retrieval results for 10 objects for n= 8are shown in Fig. 3. 4.6. Implementation details For clustering, we use K= 50 , and set τ= 50 in (1). We set the positive ratio threshold as Pk≥0.6. In our region classiﬁer we use 3 FC layers (1024,1024,2) with a ReLU activation in layers 1-2 and a softmax activation for the output layer. Dropout is used for the two hidden lay- ers with probability of 0.8. The classiﬁer is trained with the cross-entropy loss function. We use ADAM for opti- mization with a learning rate of 10−4. The learning rate is decreased by a factor of 0.6 every 6 epochs. We train our model for 35 epochs for all objects. All experiments were done on a Tesla K80 GPU. After initial feature extraction, a single epoch duration (W-DEC, DSD & detector training) is around 15 minutes, amounting to nearly 9 hours for an object. 5. Analysis 5.1. Potential score function In this section we demonstrate the effectiveness of our potential score function (1) in producing semantically meaningful clusters. In the absence of ground truth labels during training, we use the potential score function as an approximation. An effective approximation allows us to ig-nore noisy images/regions during training. We visualize the correlation of the potential score function with ground truth labels using t-SNE for three different objects in Fig. 4. 5.2. Single­class detection and scalability Scaling is an important aspect of any system. Speciﬁ- cally, detection systems must be scalable in two ways: 1) data size and 2) number of objects. We argue that a self su- pervised single-class detection model is in fact scalable in both ways. It is known that self-supervision allows learn- ing from abundant unlabeled data extracted from the web. However, the availability of data is just the ﬁrst part. A model must be capable of utilizing a big data set for a prac- tical task. Scaling a multi-class object detection model for a large number of objects is hard, as it requires a big com- plex model that must be trained from scratch on the entire data set every time a new object is added. On the other hand, using multiple single-class models in parallel instead allows adding new objects more easily. Training is done with a smaller model and on a small portion of the data. Unfortunately, WSOD models, which are also capable of using (weakly-labeled) web data, struggle with single-class detection (as we show in Table 2). As opposed to standard WSOD models, the proposed self-supervised model is capa- ble of training a single-class detector since it does not rely on discriminative regions explicitly. Therefore, it is better scalable both in the data size and in the number of objects. 6. Summary We have presented a model for the challenging tasks of self-supervised object detection and retrieval using unla- beled videos. Considering a large corpus of instructional videos with closed captions, we select frames that corre- spond to the transcript where the object name is mentioned. We pose the problem as weakly binary and noisily-labeled supervised learning. Our object detection is based on a model that captures regions with a common theme across the selected frames, distinguished from frames from dis- parate videos. This new region-level and single-class ap- proach shows promising results in the detection of objects with high appearance variability and multiple sub-classes arising from the language ambiguities. Additionally, being self-supervised and single-class, it is easily scalable with the number of objects and size of data set. We evaluate our method in terms of detection mean average precision for single-class, as well as multi-class detection. We report an upper bound performance and demonstrate superior results compared to top performing weakly-supervised approaches. Our model handles noisy labels in the weak setting, and is capable of detecting objects in challenging scenarios with- out any human labeling. 8 References",2
Self-Supervised Object Detection and Retrieval Using Unlabeled Videos,CVPR_2020,2,"Bohan Zhuang, Lingqiao Liu, Yao Li, Chunhua Shen, and Ian Reid. Attend in groups: a weakly-supervised deep learn- ing framework for learning from web data. In CVPR , 2017. 9",5
Self-Supervised Object Detection and Retrieval Using Unlabeled Videos,CVPR_2020,3,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Proceed- ings of the Thirty-First AAAI Conference on Artiﬁcial Intelli- gence, February 4-9, 2017, San Francisco, California, USA. , pages 4278–4284, 2017.",4
Self-Supervised Object Detection and Retrieval Using Unlabeled Videos,CVPR_2020,4,"Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl V on- drick, Josh McDermott, and Antonio Torralba. The sound of pixels. In ECCV , 2018.",6
Self-Supervised Object Detection and Retrieval Using Unlabeled Videos,CVPR_2020,5,"Peng Tang, Xinggang Wang, Song Bai, Wei Shen, Xiang Bai, Wenyu Liu, and Alan Loddon Yuille. PCL: Proposal cluster learning for weakly supervised object detection. IEEE trans- actions on pattern analysis and machine intelligence , 2018.",7
Generative-discriminative Feature Representations for Open-set Recognition,CVPR_2020,1,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei- Fei. ImageNet: A Large-Scale Hierarchical Image Database. InThe IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2009. 6",2
Generative-discriminative Feature Representations for Open-set Recognition,CVPR_2020,2,"Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsu- pervised visual representation learning by context prediction. InProceedings of the 2015 IEEE International Conference on Computer Vision (ICCV) , ICCV ’15, pages 1422–1430, 2015. 2",3
Generative-discriminative Feature Representations for Open-set Recognition,CVPR_2020,3,"Carl Doersch and Andrew Zisserman. Multi-task self- supervised visual learning. In The IEEE International Con- ference on Computer Vision (ICCV) , Oct 2017. 3",1
Generative-discriminative Feature Representations for Open-set Recognition,CVPR_2020,4,"Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un- supervised representation learning by predicting image rota- tions. ArXiv , abs/1803.07728, 2018. 3",3
Generative-discriminative Feature Representations for Open-set Recognition,CVPR_2020,5,"Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un- supervised representation learning by predicting image rota- tions. In 6th International Conference on Learning Repre- sentations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings , 2018. 3",3
LT-Net: Label Transfer by Learning Reversible Voxel-wise Correspondence for,CVPR_2020,1,"Pierrick Coup ´e, Jos ´e V . Manj ´on, Vladimir Fonov, Jens Pruessner, Montserrat Robles, and D. Louis Collins. Patch- based segmentation using expert priors: Application to hippocampus and ventricle segmentation. NeuroImage , 54(2):940–954, 2011.",2
LT-Net: Label Transfer by Learning Reversible Voxel-wise Correspondence for,CVPR_2020,2,"Rolf A. Heckemann, Joseph V . Hajnal, Paul Aljabar, Daniel Rueckert, and Alexander Hammers. Automatic anatomical brain MRI segmentation combining label propagation and decision fusion. NeuroImage , 33(1):115–126, 2006.",2
LT-Net: Label Transfer by Learning Reversible Voxel-wise Correspondence for,CVPR_2020,3,"Arno Klein, Brett Mensh, Satrajit Ghosh, Jason Tourville, and Joy Hirsch. Mindboggle: Automated brain labeling with multiple atlases. BMC Medical Imaging , 5(1):7, 2005.",5
LT-Net: Label Transfer by Learning Reversible Voxel-wise Correspondence for,CVPR_2020,4,"Claudio Michaelis, Ivan Ustyuzhaninov, Matthias Bethge, and Alexander S. Ecker. One-shot instance segmentation. CoRR , abs/1811.11507, 2018.",4
LT-Net: Label Transfer by Learning Reversible Voxel-wise Correspondence for,CVPR_2020,5,"Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net: Fully convolutional neural networks for volumetric medical image segmentation. In Fourth International Con- ference on 3D Vision (3DV) , pages 565–571. IEEE, 2016.",3
Correspondence-Free Material Reconstruction using Sparse Surface Constraints,CVPR_2020,1,"Nachiket H Gokhale, Paul E Barbone, and Assad A Oberai. Solution of the nonlinear elasticity imaging in- verse problem: the compressible case. Inverse Prob- lems, 24(4):045010, 2008.",3
Correspondence-Free Material Reconstruction using Sparse Surface Constraints,CVPR_2020,2,"M. Nießner, M. Zollh ¨ofer, S. Izadi, and M. Stam- minger. Real-time 3D reconstruction at scale using voxel hashing. ACM ToG , 32(6):169, 2013.",2
Correspondence-Free Material Reconstruction using Sparse Surface Constraints,CVPR_2020,3,"Bin Wang, Longhua Wu, KangKang Yin, Uri Ascher, Libin Liu, and Hui Huang. Deformation capture and modeling of soft objects. ACM Transactions on Graph- ics (TOG) , 34(4):94, 2015.",6
Correspondence-Free Material Reconstruction using Sparse Surface Constraints,CVPR_2020,4,"T. Whelan, S. Leutenegger, R.F. Salas-Moreno, B. Glocker, and A.J. Davison. Elasticfusion: Dense slam without a pose graph. In RSS, 2015.",2
Correspondence-Free Material Reconstruction using Sparse Surface Constraints,CVPR_2020,5,"Jun Wu, R ¨udiger Westermann, and Christian Dick. A survey of physically based simulation of cuts in deformable bodies. Computer Graphics Forum , 34(6):161–187, 2015.",3
A Real-Time Cross-modality Correlation Filtering Method for Referring,CVPR_2020,1,"Jo ˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking with kernelized correlation ﬁl- ters. TPAMI , 2014.",4
A Real-Time Cross-modality Correlation Filtering Method for Referring,CVPR_2020,2,"Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalan- tidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV , 2017.",11
A Real-Time Cross-modality Correlation Filtering Method for Referring,CVPR_2020,3,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV , 2014.[18] Jingyu Liu, Liang Wang, and Ming-Hsuan Yang. Referring expression generation and comprehension via attributes. In ICCV , 2017.",8
A Real-Time Cross-modality Correlation Filtering Method for Referring,CVPR_2020,4,"Raymond Yeh, Jinjun Xiong, Wen-Mei Hwu, Minh Do, and Alexander Schwing. Interpretable and globally optimal pre- diction for textual grounding using image concepts. In NIPS , 2017.",5
A Real-Time Cross-modality Correlation Filtering Method for Referring,CVPR_2020,5,"Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Dar- rell. Deep layer aggregation. In CVPR , 2018.",4
Self-Supervised Viewpoint Learning From Image Collections,CVPR_2020,1,"Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In CVPR , 2017. 5",1
Self-Supervised Viewpoint Learning From Image Collections,CVPR_2020,2,"Martin Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasserstein generative adversarial networks. In ICML , 2017. 4,5",3
Self-Supervised Viewpoint Learning From Image Collections,CVPR_2020,3,"Rodrigo Benenson, Stefan Popov, and Vittorio Ferrari. Large-scale interactive object segmentation with human an- notators. In CVPR , 2019. 7",3
Self-Supervised Viewpoint Learning From Image Collections,CVPR_2020,4,"V olker Blanz, Thomas Vetter, et al. A morphable model for the synthesis of 3d faces. In Siggraph , 1999. 6",3
Self-Supervised Viewpoint Learning From Image Collections,CVPR_2020,5,"Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem? In CVPR , 2017. 2,6,7",1
LUVLi Face Alignment: Estimating Landmarks’,CVPR_2020,1,"Olivier Chapelle and Mingrui Wu. Gradient descent opti- mization of smoothed information retrieval metrics. Infor- mation retrieval , 2010. 3",1
LUVLi Face Alignment: Estimating Landmarks’,CVPR_2020,2,"Djork-Arn ´e Clevert, Thomas Unterthiner, and Sepp Hochre- iter. Fast and accurate deep network learning by exponential linear units (ELUs). In ICLR , 2016. 5",3
LUVLi Face Alignment: Estimating Landmarks’,CVPR_2020,3,"Zhen-Hua Feng, Josef Kittler, Muhammad Awais, Patrik Hu- ber, and Xiao-Jun Wu. Wing loss for robust facial landmark localisation with convolutional neural networks. In CVPR , 2018. 7,8,16",5
LUVLi Face Alignment: Estimating Landmarks’,CVPR_2020,4,"Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Weinberger. On calibration of modern neural networks. In ICML , 2017. 1",4
LUVLi Face Alignment: Estimating Landmarks’,CVPR_2020,5,"Ali Harakeh, Michael Smart, and Steven Waslander. BayesOD: A bayesian approach for uncertainty estimation in deep object detectors. arXiv preprint arXiv:1903.03838 , 2019. 3",3
Deep Unfolding Network for Image Super-Resolution,CVPR_2020,1,"Sina Farsiu, Dirk Robinson, Michael Elad, and Peyman Mi- lanfar. Advances and challenges in super-resolution. In-ternational Journal of Imaging Systems and Technology , 14(2):47–57, 2004. 1",4
Deep Unfolding Network for Image Super-Resolution,CVPR_2020,2,"Anat Levin, Yair Weiss, Fredo Durand, and William T Free- man. Understanding and evaluating blind deconvolution al- gorithms. In CVPR , pages 1964–1971, 2009. 6",4
Deep Unfolding Network for Image Super-Resolution,CVPR_2020,3,"Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate image super-resolution using very deep convolutional net- works. In CVPR , pages 1646–1654, 2016. 2",3
Deep Unfolding Network for Image Super-Resolution,CVPR_2020,4,"Yu Chen, Ying Tai, Xiaoming Liu, Chunhua Shen, and Jian Yang. Fsrnet: End-to-end learning face super-resolution with facial priors. In CVPR , pages 2492–2501, 2018. 3",5
Deep Unfolding Network for Image Super-Resolution,CVPR_2020,5,"Alexia Jolicoeur-Martineau. The relativistic discrim- inator: a key element missing from standard GAN. arXiv:1807.00734 , 2018. 5",2
Deep Learning based Corn Kernel Classiﬁcation,CVPR_2020,1,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural net- works. In Advances in neural information processing sys- tems, pages 1097–1105, 2012. 1,3",3
Deep Learning based Corn Kernel Classiﬁcation,CVPR_2020,2,"Waleed Abdulla. Mask R-CNN for object detection and instance segmentation on keras and tensorﬂow. https: //github.com/matterport/Mask_RCNN , 2017. 4",2
Deep Learning based Corn Kernel Classiﬁcation,CVPR_2020,3,"Xiao Chen, Yi Xun, Wei Li, and Junxiong Zhang. Combin- ing discriminant analysis and neural networks for corn vari- ety identiﬁcation. Computers and electronics in agriculture , 71:S48–S53, 2010. 3",4
Deep Learning based Corn Kernel Classiﬁcation,CVPR_2020,4,"Pierre Dubosclard, Stanislas Larnier, Hubert Konik, Ariane Herbulot, and Michel Devy. Automated visual grading of grain kernels by machine vision. In Twelfth International Conference on Quality Control by Artiﬁcial Vision 2015 , vol- ume 9534, page 95340H. International Society for Optics and Photonics, 2015. 1,2",5
Deep Learning based Corn Kernel Classiﬁcation,CVPR_2020,5,"M Effendi, M Jannah, and U Effendi. Corn quality identiﬁ- cation using image processing with k-nearest neighbor clas- siﬁer based on color and texture features. In IOP Conference Series: Earth and Environmental Science , pages 012–066. IOP Publishing, 2019. 3",3
Unsupervised Batch Normalization,CVPR_2020,1,"A. J. Amiri, S. Y . Loo, and H. Zhang. Semi-supervised monocular depth estimation with left-right consistency using deep neural network, 2019. 2",2
Unsupervised Batch Normalization,CVPR_2020,2,"J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. 2",2
Unsupervised Batch Normalization,CVPR_2020,3,"D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. Raffel. Mixmatch: A holistic approach to semi-supervised learning. arXiv:1905.02249 , 2019. 2",2
Unsupervised Batch Normalization,CVPR_2020,4,"N. Bjorck, C. P. Gomes, B. Selman, and K. Q. Weinberger. Understanding batch normalization. In Advances in Neural Information Processing Systems , pages 7694–7705, 2018. 2",2
Unsupervised Batch Normalization,CVPR_2020,5,"L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam. Encoder-decoder with atrous separable convolution for se- mantic image segmentation. In ECCV , 2018. 2",2
Learning A Meta-Ensemble Technique For Skin Lesion Classiﬁcation,CVPR_2020,1,"Ulzii-Orshikh Dorj, Keun-Kwang Lee, Jae-Young Choi, and Malrey Lee. The skin cancer classiﬁcation using deep convo- lutional neural network. Multimedia Tools and Applications , 77(8):9909–9924, 2018.",4
Learning A Meta-Ensemble Technique For Skin Lesion Classiﬁcation,CVPR_2020,2,"David H. Wolpert. Stacked generalization. Neural Networks , 5(2):241 – 259, 1992.",2
Learning A Meta-Ensemble Technique For Skin Lesion Classiﬁcation,CVPR_2020,3,"Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kil- ian Q Weinberger. Densely connected convolutional net- works. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2017.",4
Learning A Meta-Ensemble Technique For Skin Lesion Classiﬁcation,CVPR_2020,4,"ISIC 2019 Training Data , (accessed March 21, 2020). https://challenge2019.isic-archive.com/data.html.",3
Learning A Meta-Ensemble Technique For Skin Lesion Classiﬁcation,CVPR_2020,5,"Melanoma Overview A Dangerous Skin Cancer , (accessed March 21, 2020). https://www.skincancer.org/skin-cancer- information/melanoma/.",3
Variable Rate Image Compression Method with Dead-zone Quantizer,CVPR_2020,1,"Yoojin Choi, El-Khamy Mostafa, and Lee Jungwon. Variable rate deep image compression with a conditional autoencoder. pages 3146–3154, 2019.",3
Variable Rate Image Compression Method with Dead-zone Quantizer,CVPR_2020,2,"David Minnen, Johannes Ball ´e, and George D Toderici. Joint autoregressive and hierarchical priors for learned image compression. In Advances in Neural Information Processing Systems , pages 10771–10780, 2018.",3
Variable Rate Image Compression Method with Dead-zone Quantizer,CVPR_2020,3,"Vivek K. Goyal. Theoretical foundations of transform cod- ing. pages 18(5):9, 21, 2001.",2
Variable Rate Image Compression Method with Dead-zone Quantizer,CVPR_2020,4,"Thomas Wedi and Steffen Wittmann. Quantization offsets for video coding. pages V ol. 1:324–327, 2005. Figure 5. Reconstructed images with different Q, model optimized with MS-SSIM and offset= 0.45",1
Variable Rate Image Compression Method with Dead-zone Quantizer,CVPR_2020,5,"Lucas Theis, Wenzhe Shi, Andrew Cunningham, and Ferenc Husz ´ar. Lossy image compression with compressive autoen- coders. arXiv preprint arXiv:1703.00395 , 2017.",4
Adversarial Latent Autoencoders,CVPR_2020,1,"Alessandro Achille and Stefano Soatto. Emergence of in- variance and disentanglement in deep representations. The Journal of Machine Learning Research , 19(1):1947–1980, 2018. 1,2,5",1
Adversarial Latent Autoencoders,CVPR_2020,2,"M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein GAN. InarXiv:1701.07875 , 2017. 1,2",2
Adversarial Latent Autoencoders,CVPR_2020,3,"A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural image synthesis. In ICLR , 2019. 1,2",2
Adversarial Latent Autoencoders,CVPR_2020,4,"R. T. Q. Chen, X. Li, R. Grosse, and R. Duvenaud. Isolating sources of disentanglement in variational autoencoders. In NeurIPS , 2018. 1,2",2
Adversarial Latent Autoencoders,CVPR_2020,5,"Emily L Denton, Soumith Chintala, Rob Fergus, et al. Deep generative image models using a laplacian pyramid of adver- sarial networks. In Advances in neural information process- ing systems (NIPS) , pages 1486–1494, 2015. 2",4
Universal Litmus Patterns:,CVPR_2020,1,"Yann LeCun, Corinna Cortes, and Christopher J.C. Burges. The MNIST database of handwritten digits [http://yann.lecun.com/exdb/mnist/index.html], 1998. 2",3
Universal Litmus Patterns:,CVPR_2020,2,"J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. Man vs. computer: Benchmarking machine learning algorithms for trafﬁc sign recognition. Neural Networks , 32:323–332, 2012. 2,5",2
Universal Litmus Patterns:,CVPR_2020,3,"Tiny ImageNet. https://tiny-imagenet. herokuapp.com/ . Accessed: 2019-11-01. 2,5",2
Universal Litmus Patterns:,CVPR_2020,4,"Dario Amodei, Sundaram Ananthanarayanan, Rishita Anub- hai, Jingliang Bai, Eric Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Qiang Cheng, Guoliang Chen, et al. Deep speech 2: End-to-end speech recognition in english and man- darin. In ICML , pages 173–182, 2016. 1",11
Universal Litmus Patterns:,CVPR_2020,5,"Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316 , 2016. 1",11
Domain Balancing: Face Recognition on Long-Tailed Domains,CVPR_2020,1,"Chen Huang, Yining Li, Chen Change Loy, and Xiaoou Tang. Learning deep representation for imbalanced classiﬁ- cation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 5375–5384, 2016.",4
Domain Balancing: Face Recognition on Long-Tailed Domains,CVPR_2020,2,"Ira Kemelmacher-Shlizerman, Steven M Seitz, Daniel Miller, and Evan Brossard. The megaface benchmark: 1 million faces for recognition at scale. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 4873–4882, 2016.",4
Domain Balancing: Face Recognition on Long-Tailed Domains,CVPR_2020,3,"Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 212–220, 2017.",6
Domain Balancing: Face Recognition on Long-Tailed Domains,CVPR_2020,4,"Ziwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2537–2546, 2019.",6
Domain Balancing: Face Recognition on Long-Tailed Domains,CVPR_2020,5,"Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio Savarese. Deep metric learning via lifted structured fea- ture embedding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4004– 4012, 2016.",4
Fractional Data Distillation Model for Anomaly Detection in Trafﬁc Videos,CVPR_2020,1,"OlafenwaMoses. Yolo v3 pretrained weights on coco. https://github.com/OlafenwaMoses/ImageAI/releases/tag/1.0/, May 2018.",2
Fractional Data Distillation Model for Anomaly Detection in Trafﬁc Videos,CVPR_2020,2,"Yanshan Li, Weiming Liu, and Qinghua Huang. Trafﬁc anomaly detection based on image descriptor in videos. Mul- timedia tools and applications , 75(5):2487–2505, 2016.",3
Fractional Data Distillation Model for Anomaly Detection in Trafﬁc Videos,CVPR_2020,3,"Milind Naphade, Zheng Tang, Ming-Ching Chang, David C Anastasiu, Anuj Sharma, Rama Chellappa, Shuo Wang, Pranamesh Chakraborty, Tingting Huang, Jenq-Neng Hwang, et al. The 2019 ai city challenge. In CVPR Work- shops , 2019.",11
Fractional Data Distillation Model for Anomaly Detection in Trafﬁc Videos,CVPR_2020,4,"Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv preprint arXiv:1804.02767 , 2018.",1
Fractional Data Distillation Model for Anomaly Detection in Trafﬁc Videos,CVPR_2020,5,"Linu Shine, Anitha Edison, and CV Jiji. A comparative study of faster r-cnn models for anomaly detection in 2019 ai city challenge. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition Workshops , pages 306– 314, 2019.[18] Waqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6479–6488, 2018.",3
Boosting Few-Shot Learning with Adaptive Margin Loss,CVPR_2020,1,"then these class representations are used as references to in- fer labels of test samples. In the following, we introduce the framework of metric-based meta-learning approaches. Meta-Training. In each episode of meta-training, we sam- ple ant-wayns-shot classiﬁcation task from the base class dataset. Speciﬁcally, we randomly choose ntclasses from base class set Cbasefor the episodic training, denoted as Ct. We randomly select nssamples from each episodic training class and combine them to form a small training set, which is called support set S. Moreover, we also randomly se- lect some other samples from each episodic training class and combine them to form a small test set, which is called query set Q. In the current episode, all samples from both query set and support set are embedded into the embedding space by using an embedding module F. Then, the meta-learner generates class representations r1,r2,···,rntby using the samples from support set S. For example, Prototypical Net- works [ 27] generates class representations by averaging the embeddings of support samples by class. After that, the meta-learner uses a metric module D(e.g., cosine similar-ity) to measure the similarity between every query point (x,y)∈Qand the current class representations in the embedding space. Based on these similarities, the meta- learner incurs a classiﬁcation loss for each point in the cur- rent query set. The meta-learner then back-propagates the gradient of the total loss of all query samples. The classiﬁ- cation loss can be formulated as: Lcls=−1 |Q|/summationdisplay (x,y)∈QlogeD(F(x),ry) /summationtext k∈CteD(F(x),rk), (1) whereD(F(x),rk)denotes the similarity between sample xand thek-th class representation rkpredicted by the meta- learner. Meta-Test. In an episode of meta-test, a novel classiﬁcation task is similar to a training base classiﬁcation task. Specif- ically, the labeled few-shot sample set and unlabeled test examples are used to form the support set and query set, re- spectively. Then they are fed into the learned model with predicted classiﬁcation results of query samples as outputs. Different metric-based meta-learning approaches differ in the form of the class representation generation mod- ule and metric module, our work introduces different mar- gin loss to improve current metric-based meta-learning ap- proaches. 3.2. Naive Additive Margin Loss An intuitive idea to learn a discriminative embedding space is to add a margin between the predicted results of different classes. This helps to increase the inter-class dis- tance in the embedding space and make it easier to recog- nize test novel samples. To achieve this, we propose a naive additive margin loss (NAML), which can be formulated as: Lna=−1 |Q|/summationdisplay (x,y)∈Qlogpna(y|x,S), (2) where pna(y|x,S) =eD(F(x),ry) eD(F(x),ry)+/summationtext k∈Ct\{y}eD(F(x),rk)+m. The above naive additive margin loss assumes all classes should be equally far away from each other and thus add a ﬁxed margin among all classes. In this way, this loss forces the embedding module Fto extract more separable visual features for samples from different classes, which beneﬁts the FSL. However, the ﬁxed additive margin may lead to mistakes on test samples of similar classes, especially for the FSL where very limited number of labelled samples are provided in the novel classes. 12578 Meta Training Task dog wolf sofa cabinet sofawolfSemantic Vectors dog cabinetWord Embedding Model Metric ModulePrediction Classification LossMeta Test Task cat tiger table chairBase Class Dataset Novel Class DatasetEmbedding Module Metric ModulePrediction Adaptive Margin Loss1. Meta Training Stage 2. Meta Test Stage Embedding ModuleAdaptive Margin Generator Figure 2. The overview of the proposed approach. Our approach consists of two stages: 1) In each episode of the meta-training stage, we ﬁrst sample a meta-training task from the base class dataset. Then, the names of classes in the meta-training task are fed into a word embedding model to extract semantic vectors for classes. After that, we propose an adaptive margin generator to produce margin penalty for each pair of classes ( e.g., the class relevant margin generator proposed in Section 3.3or the task relevant margin generator proposed in Section 3.4). Finally, we integrate the margin penalty into the classiﬁcation loss and thus obtain an adaptive margin loss. A meta-learner consisting of an embedding module and a metric module is trained by minimizing the adaptive margin loss. 2) In the meta-test stage, with the embedding module and metric module learned in the meta-training stage, we use a simple softmax (without any margin) to predict the labels of test novel samples. 3.3. Class­Relevant Additive Margin Loss To better separate similar classes in the feature embed- ding space, the margin between two classes should be adap- tive, i.e., the margin should be larger for similar classes than dissimilar classes. To achieve such adaptive margin in a principled manner, we design a class-relevant additive mar- gin loss (CRAML), where semantic similarities between classes are introduced to adjust the margin. Before introducing the class-relevant additive margin loss, we ﬁrst describe how to measure the semantic simi- larity between classes in a semantic space. Speciﬁcally, we represent each class name using a semantic vector extracted by a word embedding model ( e.g., Glove [ 21]). As illus- trated in Figure 2, we feed a class name, such as wolf or dog, into the word embedding model, and it will embed the class name into the semantic space and return a semantic word vector. Then, we construct a class-relevant margin generator M. For each pair of classes, class iand class j, Muses their semantic word vectors eiandejas inputs and generates their margin mcr i,jas follows: mcr i,j:=M(ei,ej) =α·sim(ei,ej)+β, (3) wheresimdenotes a metric ( e.g., cosine similarity) to mea- sure the semantic similarity between classes. We use α andβto denote the scale and bias parameters for the class- relevant margin generator, respectively.By introducing the class-relevant margin generator into the classiﬁcation loss, we obtain a class-relevant additive margin loss as follows. Lcr=−1 |Q|/summationdisplay (x,y)∈Qlogpcr(y|x,S), (4) where pcr(y|x,S)=eD(F(x),ry) eD(F(x),ry)+/summationtext k∈Ct\{y}eD(F(x),rk))+mcr y,k. By exploiting the semantic similarity between classes properly, our class-relevant margin loss makes the samples from similar classes to be more separable in the embedding space. The more discriminative embedding space will help better recognize test novel class samples. 3.4. Task­Relevant Additive Margin Loss So far, we assume that the margin is task-irrelevant. A dynamic task-relevant margin generator, which considers the semantic context among all classes in a meta-training task, should generate more suitable margin between differ- ent classes. By comparing each class with other classes among a meta-training task, our task-relevant margin gener- ator can measure the relatively semantic similarity between classes. Thus, the generator will add larger margin for rela- tively similar classes and smaller margin for relatively dis- similar classes. Therefore, we incorporate the generator into 12579 sim ( , )sim ( , )Semantic Vectors dogTask-Relevant Margin Generator cabinet sofawolfClasses in a task Word Embeddingsim ( , )Margin between dog and cabinet Margin between dog and wolf Margin between dog and sofa Figure 3. The illustration of the architecture of our task-relevant margin generator. the classiﬁcation loss and obtain the task-relevant additive margin loss (TRAML). Speciﬁcally, given a class y∈Ctin a meta-training task, the generator will produce the margins between class yand the other classes Ct\{y}in the task according to their se- mantic similarities, namely, {mtr y,k}k∈Ct\{y}=G/parenleftbig {sim(ey,ek)}k∈Ct\{y}/parenrightbig , (5) wheremtr y,kdenotes the task-relevant margin between class yand class k, andGdenotes the task-relevant margin gener- ator, whose architecture is illustrated in Figure 3. As shown in this ﬁgure, for a query sample ( e.g., a dog image) with labely∈Ct, we ﬁrst compute the similarities between its semantic vector eyand the semantic vectors of the other classes in the task ( e.g., class wolf, sofa and cabinet), re- spectively. Then, these semantic similarities1are fed into the a fully-connected network to generate task-relevant mar- gin for each class pair. By considering the context among all the classes in a meta-training task, our task-relevant margin generator can better measure the similarity among classes, thus generate more suitable margin for each class pair. By integrating our task-relevant margin generator into the classiﬁcation loss, we can obtain a task-relevant additive margin loss given in Equation 6and the outline of comput- ing task-relevant additive margin loss for a training episode is given in Algorithm 1. Ltr=−1 |Q|/summationdisplay (x,y)∈Qlogptr(y|x,S), (6) where ptr(y|x,S) =eD(F(x),ry) eD(F(x),ry)+/summationtext k∈Ct\{y}eD(F(x),rk)+mtr y,k. In a test episode, with the learned embedding module and metric module, we use the simple softmax function (without any margin) to predict the label of unlabeled data, i.e., we don’t need to use semantic vectors of novel classes during the test stage, which makes our model ﬂexible for any novel class. 1The order of input similarities has little impact on the performance.Algorithm 1 Task-relevant additive margin loss computa- tion for a training episode in few-shot learning Input: Base class set Cbase, task-relevant generator G. Output: Task-relevant additive margin loss Ltr. 1:Randomly sample ntclasses from base class set Cbase to form an episodic training class set Ct; 2:Randomly sample nsimages per class in Ctto form a support set S; 3:Randomly sample nqimages per class in Ctto form a query set Q; 4:Obtain the semantic vector for each class in Ctby feed- ing its class name into a word embedding model; 5:For each query sample, compute the task-relevant mar- gins between its class yand the classes in Ct\{y} by using task-relevant margin generator Gaccording to Equation 5; 6:Compute the task-relevant additive margin loss Ltrac- cording to Equation 6. 3.5. Extension to Generalized Few­Shot Learning Although the proposed approach is originally designed for the standard FSL, it can be easily extended to the gener- alized FSL: simply including test data from both base and novel classes, and their labels are predicted from all classes in both base and novel class set in the test stage. This setting is much more challenging and realistic than the standard FSL, where test data are from only novel classes. Note that, our adaptive margin loss is ﬂexible for the generalized FSL: the embedding module and the metric module trained by the adaptive loss with all training samples from base classes can be directly used for label inference of test samples from the disjoint space of both base and novel classes. Experimental results show that our method can improve the state-of-the- art alternative and create a new state-of-the-art for metric- based meta-learning approaches. 4. Experiments and Discussions In this section, we evaluate our approach by conducting three groups of experiments: 1) standard FSL setting where 12580 Model Backbone TypeTest Accuracy 5-way 1-shot 5-way 5-shot Matching Networks [ 31] 4Conv Metric 43.56 ±0.84 55.31 ±0.73 Prototypical Network [ 27] 4Conv Metric 49.42 ±0.78 68.20 ±0.66 Relation Networks [ 27] 4Conv Metric 50.44 ±0.82 65.32 ±0.70 GCR [ 15] 4Conv Metric 53.21 ±0.40 72.34 ±0.32 Memory Matching Network [ 3] 4Conv Metric 53.37 ±0.48 66.97 ±0.35 Dynamic FSL [ 8] 4Conv Metric 56.20 ±0.86 73.00 ±0.64 Prototypical Network [ 27] ResNet12 Metric 56.52 ±0.45 74.28 ±0.20 TADAM [ 20] ResNet12 Metric 58.50 ±0.30 76.70 ±0.38 DC [ 17] ResNet12 Metric 62.53 ±0.19 78.95 ±0.13 TapNet [ 36] ResNet12 Metric 61.65 ±0.15 76.36 ±0.10 ECMSFMT [ 24] ResNet12 Metric 59.00 77.46 AM3 (Prototypical Network) [ 35] ResNet12 Metric 65.21 ±0.49 75.20 ±0.36 MAML [ 7] 4Conv Gradient 48.70 ±1.84 63.11 ±0.92 MAML++ [ 1] 4Conv Gradient 52.15 ±0.26 68.32 ±0.44 iMAML [ 22] 4Conv Gradient 49.30 ±1.88 - LCC [ 19] 4Conv Gradient 54.6 ±0.4 71.1 ±0.4 CAML [ 11] ResNet12 Gradient 59.23 ±0.99 72.35 ±0.18 MTL [ 28] ResNet12 Gradient 61.20 ±1.80 75.50 ±0.80 MetaOptNet-SVM [ 12] ResNet12 Gradient 62.64 ±0.61 78.63 ±0.46 Prototypical Network + TRAML (OURS) ResNet12 Metric 60.31 ±0.48 77.94 ±0.57 AM3 (Prototypical Network) + TRAML (OURS) ResNet12 Metric 67.10±0.52 79.54±0.60 Table 1. Comparative results for FSL on the miniImageNet dataset. The averaged accuracy (%) on 600 test episodes is given followed by the 95% conﬁdence intervals (%). Notations: ‘4Conv’ – feature embedding module as in [ 27],i.e., four stacked convolutions layers of 64 ﬁlters; ‘ResNet12’ – the feature embedding module as in [ 20],i.e., ResNet12 architecture containing four residual blocks of three stacked 3 ×3 convolutional layers; ‘Metric’ – metric-based meta-learning approaches for FSL; ‘Gradient’ – gradient-based meta-learning approaches for FSL. the label space of test data is restricted to a few novel classes at each test episode, 2) generalized FSL setting where the label space of test data is extended to both base classes and novel classes, and 3) further evaluation including ablation study and comparison with other margin losses. 4.1. Standard Few­Shot Learning 4.1.1 Datasets and Settings Under the standard FSL setting [ 27,31], we evaluate our ap- proach on the most popular benchmark, i.e., miniImageNet. It contains 100 classes randomly selected from ImageNet",2
Boosting Few-Shot Learning with Adaptive Margin Loss,CVPR_2020,2,"as the baseline model and differs only in which loss is used to train the model: ‘Original Classiﬁcation Loss’ 12582 ModelTest Accuracy 5-way 1-shot 5-way 5-shot Original Classiﬁcation Loss 65.21 ±0.49 75.20 ±0.36 Naive Additive Margin Loss 65.42 ±0.25 75.48 ±0.34 Class-Relevant Additive Margin Loss 66.36 ±0.57 77.21 ±0.48 Our Full Model 67.10±0.52 79.54±0.60 Table 3. Ablation study for FSL on the miniImageNet dataset un- der the standard FSL setting. The evaluation metric is the same as in Table 1. – model training using the softmax loss provided in [ 35]; ‘Naive Additive Margin Loss’ – model training by the loss proposed in Section 3.2; ‘Class-Relevant Additive Margin Loss’ – model training by the loss proposed in Section 3.3. Table 3presents the comparative results of the above losses on the miniImageNet dataset under the standard FSL setting. It can be observed that: 1) Training metric-based meta-learning approaches with our adaptive margin loss leads to signiﬁcant improvements (see Our Full Model vs. Original Classiﬁcation Loss). This provides strong supports for our main contribution on embedding learning for FSL. 2) The model trained by the proposed naive additive margin loss shows slight performance improvement over the model trained by original classiﬁcation loss. This means that sim- ply adding a ﬁxed margin into the classiﬁcation loss has lim- ited effectiveness in FSL. 3) Thanks to the adaptive margin produced by the class-relevant margin generator, our class- relevant margin additive loss is shown to beneﬁt the embed- ding learning for FSL (see Class-Relevant Additive Margin Loss vs. Naive Additive Margin Loss). 4) By considering the semantic context among classes in a meta-training task, our task-relevant additive margin loss yields better results than the class-relevant margin loss. Moreover, we observe that the learned coefﬁcient αin Eq. ( 3) is positive, which veriﬁes our intuition that the margin between similar classes should be larger than the one between dissimilar classes. 4.3.2 Comparison with Other Margin Losses To validate the effectiveness of the proposed adaptive mar- gin loss, we compare our approach with two margin losses which are widely used in face recognition. Each of them uses the AM3 (Prototypical Networks) [ 35] as the baseline model and differs only in which loss is used to train the model. The two margin losses are: 1) Additive angular mar- gin loss [ 4], which add an additive angular margin to the angle between the weight vector and feature embeddings. 2) Additive cosine margin loss [ 33], which directly adds a cosine margin to the target logits. Note that, both of these two methods add margin penalty to the target logits com- puted by the dot product between feature embeddings and weight vectors. This is different from Prototypical NetworkModelTest Accuracy 5-way 1-shot 5-way 5-shot Additive angular margin loss [ 4] 66.21±0.46 77.30 ±0.71 Additive cosine margin loss [ 33] 65.96±0.56 76.93 ±0.49 Our Full Model (cosine) 66.92 ±0.43 79.08 ±0.52 Our Full Model (euclidean) 67.10±0.52 79.54±0.60 Table 4. Comparative classiﬁcation accuracies (%) of two other margin losses on the miniImageNet dataset under the standard FSL setting. Notations: ‘Our Full Model (cosine)’ – implementing our task-relevant additive margin loss on AM3 (Prototypical Network)",1
Boosting Few-Shot Learning with Adaptive Margin Loss,CVPR_2020,3,"with cosine distance as metric in the embedding space; ‘Our Full Model (euclidean)’ – implementing our task-relevant additive margin loss on AM3 (Prototypical Network) [ 35] with euclidean distance as metric in the embedding space. and its variants, which use the opposite of the euclidean dis- tances between class representations and feature embedding as the logits. For fair comparison, we replace the opposite of euclidean metric used in AM3 (Prototypical Network) [ 35] with the cosine distance, and train the AM3 model with our task-relevant margin loss (the model is denoted by ‘Our Full Model (cosine)’ in Table 4) . Table 4presents the comparative results of the two mar- gin losses and our losses on the miniImageNet dataset under the standard FSL setting. We can observe that our method is shown to be more effective than the two competitors. It can be expected that, our method is designed for the FSL problem. That is, our method involves semantic similarity among classes in meta-training task to learn a more suit- able margin penalty, compared with a ﬁxed one generated by [4,33]. The suitable margin of each pair of classes helps to learn more discriminative embedding space and thus bet- ter distinguish samples from different novel classes. 5. Conclusion In this paper, we propose an adaptive margin principle, which can effectively enhance the discriminative power of embedding space for few-shot image recognition. We ﬁrst develop a class-relevant additive margin loss which com- bines the standard classiﬁcation loss with an adaptive mar- gin generator based semantic similarity between classes. Then, by considering the semantic context among classes in a meta-training task, a task-relevant additive margin loss is further proposed to learn more discriminiative embbeding space for FSL. Furthermore, we also extend the proposed model to the more realistic generalized FSL setting. Ex- perimental results demonstrate that our method is effective under both of the two FSL settings. Acknowledgment. This work is supported by National Key R&D Program of China (2018YFB1402600), BJNSF (L172037) and Beijing Acedemy of Artiﬁcial Intelligence. 12583 References",1
Boosting Few-Shot Learning with Adaptive Margin Loss,CVPR_2020,4,"Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your maml. In ICLR , 2018.",3
Boosting Few-Shot Learning with Adaptive Margin Loss,CVPR_2020,5,"and each class contains 600 images with resolution of 84×84. Following the widely used setting in prior works [27,31], we take 64 classes for training, 16 for validation and 20 for testing. In the training stage, the 64 training classes and 16 validation classes are respectively regarded as base classes and novel classes to decide the model hy- perparameters. Following the standard setting adopted by most existing few-shot learning works [ 3,8,27,29,31], we conduct 5-way 1-shot/5-shot classiﬁcation on the miniIma- geNet dataset. In 1-shot and 5-shot scenarios, each query set has 15 images per class, while each support set con- tains 1 and 5 image(s) per class, respectively. For a trainingepisode, images in the support sets and query sets are ran- domly selected from the base class set. In a test episode, im- ages in the support sets and the query sets are randomly se- lected from the novel class set. The evaluation metric for the miniImageNet dataset is deﬁned as the top-1 classiﬁcation accuracy on randomly selected 600 test episodes. We test our task-relevant additive margin loss with two backbone metric-based meta learning approaches: Prototypical Net- works [ 27] and its most recent improvement AM3 (Proto- typical Networks) [ 35] which are the state-of-the-art metric- based meta learning methods for FSL. 4.1.2 Implementation Details Our feature embedding module mirrors the ResNet12 archi- tecture used by [ 20], which consists of four residual blocks. Each block comprises three stacked 3 ×3 convolutional layers. Each block is followed by max pooling. We use the same feature extractor on images in both the support set and query set. The fully-connected network in the re- lation module consists of two fully-connected layers, each followed by a batch normalization layer and a ReLU non- linearity layer. The word embedding model we used in this paper is Glove [ 21]. 12581 ModelNovel All ns=1 2 5 10 20 ns=1 2 5 10 20 Logistic regression (from [ 34]) 38.4 51.1 64.8 71.6 76.6 40.8 49.9 64.2 71.9 76.9 Logistic regression w/H (from [ 9]) 40.7 50.8 62.0 69.3 76.5 52.2 59.4 67.6 72.8 76.9 Prototypical Network [ 27] (from [ 34]) 39.3 54.4 66.3 71.2 73.9 49.5 61.0 69.7 72.9 74.6 Matching Networks [ 31] (from [ 34]) 43.6 54.0 66.0 72.5 76.9 54.4 61.0 69.0 73.7 76.5 Squared Gradient Magnitude w/H [ 9] - - - - - 54.3 62.1 71.3 75.8 78.1 Batch Squared Gradient Magnitude [ 9] - - - - - 49.3 60.5 71.4 75.8 78.5 Prototype Matching Nets [ 34] 43.3 55.7 68.4 74.0 77.0 55.8 63.1 71.1 75.0 77.1 Prototype Matching Nets w/H [ 34] 45.8 57.8 69.0 74.3 77.4 57.6 64.7 71.9 75.2 77.5 Dynamic FSL [ 8] 46.0 57.5 69.2 74.8 78.1 58.2 65.2 72.2 76.5 78.7 Dynamic FSL + TRAML (OURS) 48.1 59.2 70.3 76.4 79.4 59.2 66.2 73.6 77.3 80.2 Table 2. Comparative results for generalized FSL on the ImageNet2012 dataset. The top-5 accuracies (%) on the novel classes and on all classes are used as the evaluation metrics for this dataset. Methods with “w/ H” use mechanisms that hallucinate extra training examples for the novel classes. 4.1.3 Experimental Results Table 1provides comparative results for FSL on the mini- ImageNet dataset. We can observe that: 1) our approach signiﬁcantly improve the performance of baseline mod- els (i.e., Prototypical Network [ 27] and AM3 (Prototypi- cal Networks [ 35]). This indicates that the proposed task- relevant additive margin loss can boost performance of metric-based meta-learning approaches very effectively. 2) Our approach clearly outperforms the state-of-the-art FSL model on both 5-way 1-shot and 5-way 5-shot settings, thanks to the discriminative feature embedding learned by the proposed task-relevant additive margin loss. 4.2. Generalized Few­Shot Learning 4.2.1 Dataset and Settings To further evaluate the effectiveness of our approach, we test our approach in a more challenging yet practical gener- alized FSL setting, where the label space of test data is ex- tended to both base and novel classes. Following [ 8,9,34], we conduct experiment on the large-scale ImageNet2012 dataset. This benchmark splits the 1000 ImageNet classes into 389 base classes and 611 novel classes; 193 of the base classes and 300 of the novel classes are used for cross val- idation and the remaining 196 base classes and 311 novel classes are used for the ﬁnal evaluation (for more details we refer to [ 9,34]). As in [ 8], the embedding module we used is ResNet10 network that gets as input images of 224 ×224 resolu- tion. We compare our model with several generalized FSL alternatives: Matching Networks [ 31], Prototypical Net- works [ 27], Logistic Regression [ 34], Batch Squared Gradi- ent Magnitude [ 9], Squared Gradient Magnitude With Hal- lucination [ 9], Prototype Matching Nets [ 34], and Dynamic FSL [ 8]. We implement our task-relevant additive margin loss onthe state-of-the-art model ( i.e., Dynamic FSL [ 8]). Fol- lowing [ 34], we ﬁrst train the embedding module ( i.e., ResNet10) by using our task-relevant additive margin loss with all base classes. Then we extract features for all train- ing samples with the learned embedding module and save them to disk. The weight generator in Dynamic FSL [ 8] will use these pre-computed features as inputs. Finally, we train the weight generator by replacing the original classiﬁ- cation loss with our task-relevant additive margin loss. The evaluation metric is the top-5 accuracy on the novel classes and on all classes. We repeat the above experiment 5 times (sampling each time a different set of training images for the novel classes) and report the mean accuracy. 4.2.2 Results Table 2provides the comparative results of generalized FSL on the large-scale ImageNet2012 dataset. We can observe that: 1) our approach achieves the best results on all evalua- tion metrics. This indicates that, with the discriminative em- bedding space learned by our task-relevant additive margin loss, our approach has the strongest generalization ability under this more challenging setting. 2) Our approach yields consist performance improvement over the state-of-the-art generalized FSL model ( i.e., Dynamic FSL [ 8]) on the 1- shot, 2-shot, 5-shot, 10-shot, and 20-shot settings. This fur- ther validates the effectiveness of our approach. 4.3. Further Evaluation 4.3.1 Ablation Study on Key Components We compare our full model with a number of stripped down versions to evaluate the effectiveness of the key components of our approach. Speciﬁcally, three of such loss are com- pared, each of which uses the AM3 (Prototypical Networks)",2
Learning to Dress 3D People in Generative Clothing,CVPR_2020,1,"Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James Davis. SCAPE: shape completion and animation of people. In ACM Transactions on Graphics (TOG) , volume 24, pages 408–416. ACM, 2005. 1,3",6
Learning to Dress 3D People in Generative Clothing,CVPR_2020,2,"Micha ¨el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Con- volutional neural networks on graphs with fast localized spectral ﬁl- tering. In Advances in Neural Information Processing Systems , pages 3844–3852, 2016. 2,3",3
Learning to Dress 3D People in Generative Clothing,CVPR_2020,3,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information pro- cessing systems , pages 2672–2680, 2014. 2,4",8
Learning to Dress 3D People in Generative Clothing,CVPR_2020,4,"Hanbyul Joo, Tomas Simon, and Yaser Sheikh. Total Capture: A 3D deformation model for tracking faces, hands, and bodies. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 1,3",3
Learning to Dress 3D People in Generative Clothing,CVPR_2020,5,"Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Ger- ard Pons-Moll. Multi-Garment Net: Learning to Dress 3D People from Images. In The IEEE International Conference on Computer Vision (ICCV) , 2019. 2,3,4",4
As Seen on TV: Automatic Basketball Video Production using Gaussian-based,CVPR_2020,1,"Michael L Gleicher and Feng Liu. Re-cinematography: im- proving the camera dynamics of casual video. In Proceed- ings of the 15th ACM international conference on Multime- dia, pages 27–36, 2007.",1
As Seen on TV: Automatic Basketball Video Production using Gaussian-based,CVPR_2020,2,"Jim Owens. Television sports production . CRC Press, 2015.",2
As Seen on TV: Automatic Basketball Video Production using Gaussian-based,CVPR_2020,3,Unleashing the New Era of Long-Tail OTT Content. https://www.sportbusiness.com/2020/01/genius-sports- whitepaper-unleashing-the-new-era-of-long-tail-ott- content/ . Accessed: 2020-03-02.,2
As Seen on TV: Automatic Basketball Video Production using Gaussian-based,CVPR_2020,4,"Yasuo Ariki, Shintaro Kubota, and Masahito Kumano. Au- tomatic production system of soccer sports video by digital camera work based on situation recognition. In Eighth IEEE International Symposium on Multimedia (ISM’06) , pages 851–860. IEEE, 2006.",3
As Seen on TV: Automatic Basketball Video Production using Gaussian-based,CVPR_2020,5,"Alina Bialkowski, Patrick Lucey, Peter Carr, Simon Den- man, Iain Matthews, and Sridha Sridharan. Recognising team activities from noisy data. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops , pages 984–990, 2013.",6
Self-Supervised Learning of Video-Induced Visual Invariances,CVPR_2020,1,"Co-training with labeled images We also consider the case where one has access to a limited number of labeled images in addition to the video data. Combining image- based SSL losses with a supervised loss applied to a subset of the images was studied previously by [ 68]. They found that this approach leads to a state-of-the-art semi-supervised models, and improves the performance of supervised mod- els when all images are labeled. Here, we consider the re- lated setup where the SSL loss is computed on video data, and the supervised loss is based on image data from a differ- ent data set. Speciﬁcally, we additionally apply ffollowed by a linear classiﬁer to mini-batches of labeled images and compute the cross-entropy loss LSUPbetween the predic- tions and the image labels. The total loss is then computed asLSSL+γLSUP, whereγ >0balances the contributions of the self-supervised and supervised loss terms.",2
Self-Supervised Learning of Video-Induced Visual Invariances,CVPR_2020,2,"To deﬁne the frame/shot-level loss LS, we propose to build on any SSL loss designed for images, such as classify- ing exemplars [ 15], solving jigsaw puzzles of image patches [43], or rotation prediction [ 18]. For learning shot-induced invariances, one can take two approaches: (i) apply the image-based SSL loss independently to each frame so that the shot-induced invariances are learned implicitly through the combination of pooling function and video-level prediction task, or (ii) explicitly ensure that the embeddings of the frames from the same shot are similar by adding a triplet or a contrastive loss to the image-based SSL loss.",5
Self-Supervised Learning of Video-Induced Visual Invariances,CVPR_2020,3,"In contrast to action recognition networks, which learn video representations that have to be discriminative w.r.t. changes between frames, our framework targets learning representations that are invariant to such changes. Never- theless, discriminative tasks useful for learning representa- tions for action recognition, such as predicting whether a se- quence of frames is played forward or backward [ 64], ver- ifying whether the frames are ordered or shufﬂed [ 40], or predicting features corresponding to future frames [ 21], can be useful to learn abstract transferable representations when applied to sensibly chosen groups of aggregated frames .",2
Self-Supervised Learning of Video-Induced Visual Invariances,CVPR_2020,4,"references therein]. However, self-supervised models are now performing well on tasks such as surface normal estimation, detection, and navigation [ 19]. The VTAB benchmark evaluates the transferability of representations beyond object classiﬁcation in the natural image domain to many domains and task semantics such as counting and localization [ 69]. Similarly, recent developments in natural language processing (NLP) have lead to representations that transfer effectively to many diverse tasks [ 11].",2
Self-Supervised Learning of Video-Induced Visual Invariances,CVPR_2020,5,Learning video-induced visual invariances,2
Fast and Flexible Image Blind Denoising via Competition of Experts,CVPR_2020,1,"S. Diamond, V . Sitzmann, S. Boyd, G. Wetzstein, and F. Heide. Dirty pixels: Optimizing image classiﬁca- tion architectures for raw sensor data. arXiv preprint arXiv:1701.06487 , 2017. 1",2
Fast and Flexible Image Blind Denoising via Competition of Experts,CVPR_2020,2,"D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR , 2015. 4",2
Fast and Flexible Image Blind Denoising via Competition of Experts,CVPR_2020,3,"S. Lee, S. Purushwalkam, M. Cogswell, V . Ranjan, D. Cran- dall, and D. Batra. Stochastic multiple choice learning for training diverse deep ensembles. In NIPS , 2016. 2",2
Fast and Flexible Image Blind Denoising via Competition of Experts,CVPR_2020,4,"Y . Tang and C. Eliasmith. Deep networks for robust visual recognition. In ICML , 2010. 1",2
Fast and Flexible Image Blind Denoising via Competition of Experts,CVPR_2020,5,"T. Plotz and S. Roth. Benchmarking denoising algorithms with real photographs. In CVPR , 2017. 1",2
Image Super-Resolution with Cross-Scale Non-Local Attention,CVPR_2020,1,"Gilad Freedman and Raanan Fattal. Image and video upscal- ing from local self-examples. ACM Transactions on Graph- ics (TOG) , 30(2):12, 2011. 2",1
Image Super-Resolution with Cross-Scale Non-Local Attention,CVPR_2020,2,"Tomer Michaeli and Michal Irani. Nonparametric blind super-resolution. In Proceedings of the IEEE International Conference on Computer Vision , pages 945–952, 2013. 2",1
Image Super-Resolution with Cross-Scale Non-Local Attention,CVPR_2020,3,"Marco Bevilacqua, Aline Roumy, Christine Guillemot, and Marie Line Alberi-Morel. Low-complexity single-image super-resolution based on nonnegative neighbor embedding. InProceedings of the British Machine Vision Conference , 2012. 5",4
Image Super-Resolution with Cross-Scale Non-Local Attention,CVPR_2020,4,"Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and Lei Zhang. Second-order attention network for single im- age super-resolution. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 11065– 11074, 2019. 2,3,4,5,6,7",5
Image Super-Resolution with Cross-Scale Non-Local Attention,CVPR_2020,5,"Hasan Demirel and Gholamreza Anbarjafari. Discrete wavelet transform-based satellite image resolution enhance- ment. IEEE transactions on geoscience and remote sensing , 49(6):1997–2004, 2011. 1",1
Modality Shifting Attention Network for Multi-modal Video Question Answering,CVPR_2020,1,"Jiyang Gao, Runzhou Ge, Kan Chen, and Ram Nevatia. Motion-appearance co-memory networks for video question answering. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2018. 2",4
Modality Shifting Attention Network for Multi-modal Video Question Answering,CVPR_2020,2,"Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, and Chang D. Yoo. Gaining extra supervision via multi-task learning for multi-modal video question answering. In IJCNN , 2019. 2,3,7",5
Modality Shifting Attention Network for Multi-modal Video Question Answering,CVPR_2020,3,"Wenjie Pei, Jiyuan Zhang, Xiangrong Wang, Lei Ke, Xiaoy- ong Shen, and Yu-Wing Tai. Memory-attended recurrent net- work for video captioning. In IEEE International Conference on Computer Vision and Pattern Recognition (CVPR) , 2019. 1",6
Modality Shifting Attention Network for Multi-modal Video Question Answering,CVPR_2020,4,"Jeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word representation. InEMNLP , 2014. 7",3
Modality Shifting Attention Network for Multi-modal Video Question Answering,CVPR_2020,5,"Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In IEEE Conference on Computer Vision and Pattern Recog- nition (CVPR) , July 2017. 3",1
Screencast Tutorial Video Understanding,CVPR_2020,1,"H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and T. Serre. Hmdb: A large video database for human motion recogni- tion. In ICCV , 2011. 2",2
Screencast Tutorial Video Understanding,CVPR_2020,2,"Kunpeng Li, Yulun Zhang, Kai Li, Yuanyuan Li, and Yun Fu. Visual semantic reasoning for image-text matching. In ICCV , 2019. 5",5
Screencast Tutorial Video Understanding,CVPR_2020,3,"Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal, Ivan Laptev, Josef Sivic, and Simon Lacoste-Julien. Unsu- pervised learning from narrated instruction videos. In Com- puter Vision and Pattern Recognition (CVPR) , 2016. 2",6
Screencast Tutorial Video Understanding,CVPR_2020,4,"Pei-Yu Chi, Sally Ahn, Amanda Ren, Mira Dontcheva, Wilmot Li, and Bj ¨orn Hartmann. Mixt: automatic genera- tion of step-by-step mixed media tutorials. In UIST . ACM, 2012. 2",6
Screencast Tutorial Video Understanding,CVPR_2020,5,"Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. In ICCV , 2019. 2,4",6
Semi-supervised 3D Face Representation Learning,CVPR_2020,1,"V olker Blanz and Thomas Vetter. Face recognition based on ﬁtting a 3D morphable model. IEEE Transactions on Pattern Analysis and Machine Intelligence , 25(9):1063–1074, Sep. 2003. 1,2",1
Semi-supervised 3D Face Representation Learning,CVPR_2020,2,"Chen Cao, Qiming Hou, and Kun Zhou. Displaced dynamic expression regression for real-time facial tracking and ani- mation. ACM Trans. Graph. , 33(4):43:1–43:10, July 2014. 1",3
Semi-supervised 3D Face Representation Learning,CVPR_2020,3,"Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun Zhou. Facewarehouse: A 3D facial expression database for visual computing. IEEE Transactions on Visualization and Computer Graphics , 20(3):413–425, March 2014. 2",5
Semi-supervised 3D Face Representation Learning,CVPR_2020,4,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems 27 , pages 2672– 2680. Curran Associates, Inc., 2014. 2",8
Semi-supervised 3D Face Representation Learning,CVPR_2020,5,"Yuval Nirkin, Iacopo Masi, Anh Tuan Tran, Tal Hassner, and Gerard Medioni. On face segmentation, face swapping, and face perception. In 2018 13th IEEE International Conference on Automatic Face Gesture Recognition (FG 2018) , pages 98–105, May 2018. 4",5
Towards Discriminability and Diversity:,CVPR_2020,1,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. 2009.",6
Towards Discriminability and Diversity:,CVPR_2020,2,"Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas- cal Germain, Hugo Larochelle, Franc ¸ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial train- ing of neural networks. The Journal of Machine Learning Research , 17(1):2096–2030, 2016.",8
Towards Discriminability and Diversity:,CVPR_2020,3,"George A Miller. Wordnet: a lexical database for english. Communications of the ACM , 38(11):39–41, 1995.",1
Towards Discriminability and Diversity:,CVPR_2020,4,"Mart ´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe- mawat, Geoffrey Irving, Michael Isard, et al. Tensorﬂow: A system for large-scale machine learning. In 12th{USENIX} Symposium on Operating Systems Design and Implementa- tion ({OSDI}16), pages 265–283, 2016.",11
Towards Discriminability and Diversity:,CVPR_2020,5,"David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A holistic approach to semi-supervised learning. arXiv preprint arXiv:1905.02249 , 2019.",6
S2A: Wasserstein GAN with Spatio-Spectral Laplacian Attention for,CVPR_2020,1,"Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji- tendra Malik. Contour detection and hierarchical image seg- mentation. IEEE transactions on pattern analysis and ma- chine intelligence , 33(5):898–916, 2010. 1",4
S2A: Wasserstein GAN with Spatio-Spectral Laplacian Attention for,CVPR_2020,2,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473 , 2014. 1,3",3
S2A: Wasserstein GAN with Spatio-Spectral Laplacian Attention for,CVPR_2020,3,"Mario Beaulieu, Samuel Foucher, Dan Haberman, and Colin Stewart. Deep image-to-image transfer applied to resolution enhancement of sentinel-2 images. In IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Sympo- sium , pages 2611–2614. IEEE, 2018. 2",4
S2A: Wasserstein GAN with Spatio-Spectral Laplacian Attention for,CVPR_2020,4,"Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single image super-resolution from transformed self-exemplars. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 5197–5206, 2015. 1",3
S2A: Wasserstein GAN with Spatio-Spectral Laplacian Attention for,CVPR_2020,5,"Saeed Anwar and Nick Barnes. Densely residual laplacian super-resolution. arXiv preprint arXiv:1906.12021 , 2019. 1, 3",1
Memory Aggregation Networks for,CVPR_2020,1,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV , pages 801–818, 2018. 4,6",5
Memory Aggregation Networks for,CVPR_2020,2,"Xue Bai, Jue Wang, David Simons, and Guillermo Sapiro. Video snapcut: robust video object cutout using localized classiﬁers. ACM Transactions on Graphics (ToG) , 28(3):70, 2009. 1,2",4
Memory Aggregation Networks for,CVPR_2020,3,"Linchao Bao, Baoyuan Wu, and Wei Liu. Cnn in mrf: Video object segmentation via inference in a cnn-based higher- order spatio-temporal mrf. In CVPR , pages 5977–5986, 2018. 2",3
Memory Aggregation Networks for,CVPR_2020,4,"Arnaud Benard and Michael Gygli. Interactive video object segmentation in the wild. arXiv preprint arXiv:1801.00269 , 2017. 2,3",1
Memory Aggregation Networks for,CVPR_2020,5,"Benjamin Bratt. Rotoscoping . Routledge, 2012. 1",2
You2Me: Inferring Body Pose in Egocentric Video,CVPR_2020,1,"A. Graves. Generating sequences with recurrent neural net- works. arXiv preprint arXiv:1308.0850 , 2013. 4",2
You2Me: Inferring Body Pose in Egocentric Video,CVPR_2020,2,"A. Alahi, K. Goel, V . Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese. Social lstm: Human trajectory prediction in crowded spaces. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2016. 2",2
You2Me: Inferring Body Pose in Egocentric Video,CVPR_2020,3,"A. Alahi, V . Ramanathan, and L. Fei-Fei. Socially-aware large-scale crowd forecasting. In IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) , 2014. 2",2
You2Me: Inferring Body Pose in Egocentric Video,CVPR_2020,4,"S. Alletto, G. Serra, S. Calderara, and R. Cucchiara. Un- derstanding social relationships in egocentric vision. Pattern Recognition , 2015. 2",2
You2Me: Inferring Body Pose in Egocentric Video,CVPR_2020,5,"S. Alletto, G. Serra, S. Calderara, F. Solera, and R. Cuc- chiara. From ego to nos-vision: Detecting social relation- ships in ﬁrst-person views. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops , 2014. 2",2
Learning Sparse Neural Networks Through Mixture-Distributed Regularization,CVPR_2020,1,"Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the void: Op- timizing control variates for black-box gradient estimation. InInternational Conference on Learning Representations (ICLR) , 2018. 2",5
Learning Sparse Neural Networks Through Mixture-Distributed Regularization,CVPR_2020,2,"Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems , pages 598–605, 1990. 2",3
Learning Sparse Neural Networks Through Mixture-Distributed Regularization,CVPR_2020,3,"Antonio Polino, Razvan Pascanu, and Dan Alistarh. Model compression via distillation and quantization. In International Conference on Learning Representations (ICLR) , 2018. 1",3
Learning Sparse Neural Networks Through Mixture-Distributed Regularization,CVPR_2020,4,"Jose M Alvarez and Mathieu Salzmann. Learning the num- ber of neurons in deep networks. In Advances in Neural Information Processing Systems , pages 2270–2278, 2016. 2",1
Learning Sparse Neural Networks Through Mixture-Distributed Regularization,CVPR_2020,5,"Jose M Alvarez and Mathieu Salzmann. Compression-aware training of deep networks. In Advances in Neural Information Processing Systems , pages 856–867, 2017. 2",1
Mask Encoding for Single Shot Instance Segmentation∗,CVPR_2020,1,"Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. YOLACT: real-time instance segmentation. In Proc. IEEE Int. Conf. Comp. Vis. , pages 9157–9166, 2019. 2,8",4
Mask Encoding for Single Shot Instance Segmentation∗,CVPR_2020,2,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected CRFs. IEEE Trans. Pattern Anal. Mach. Intell. , 40(4):834–848, 2017. 5",5
Mask Encoding for Single Shot Instance Segmentation∗,CVPR_2020,3,"Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn. , pages 3213–3223, 2016. 2",9
Mask Encoding for Single Shot Instance Segmentation∗,CVPR_2020,4,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical im- age database. In Proc. IEEE Conf. Comp. Vis. Patt. Recogn. , pages 248–255, 2009. 6",6
Mask Encoding for Single Shot Instance Segmentation∗,CVPR_2020,5,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and Lawrence Zitnick. Microsoft COCO: Common objects in context. In Proc. Eur. Conf. Comp. Vis. , pages 740–755. Springer, 2014. 1,2,5",8
Pixel Consensus Voting for Panoptic Segmentation,CVPR_2020,1,"Dana H Ballard. Generalizing the hough transform to detect arbitrary shapes. Pattern recognition , 13(2):111–122, 1981.",1
Pixel Consensus Voting for Panoptic Segmentation,CVPR_2020,2,"Yanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang. Attention-guided uniﬁed network for panoptic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7026–7035, 2019.",7
Pixel Consensus Voting for Panoptic Segmentation,CVPR_2020,3,"Ita Lifshitz, Ethan Fetaya, and Shimon Ullman. Human pose estimation using deep consensus voting. In European Con- ference on Computer Vision , pages 246–260. Springer, 2016.",3
Pixel Consensus Voting for Panoptic Segmentation,CVPR_2020,4,"Anurag Arnab and Philip HS Torr. Pixelwise instance seg- mentation with a dynamically instantiated network. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 441–450, 2017.",1
Pixel Consensus Voting for Panoptic Segmentation,CVPR_2020,5,"Min Bai and Raquel Urtasun. Deep watershed transform for instance segmentation. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 5221–5229, 2017. 9471",1
Abstract,CVPR_2020,1,"A. Morales, D. Morocho, J. Fierrez, and R. Vera-Rodriguez. Signature authentication based on human intervention: performance and complementarity with automatic systems. IET Biometrics 6(4):307-315, 2017.",2
Abstract,CVPR_2020,2,"P. Maergner, V. Pondenkandath, M. Alberti, M. Liwicki, K. Riesen, R. Ingold, et al., Combining graph edit distance and triplet networks for offline signature verification, Pattern Recognition Letters, 125:527-533, 2019.",2
Abstract,CVPR_2020,3,"M. I. Malik, M. Liwicki, and A. Dengel, Local features for off-line forensic signature verification, Advances in Digital Handwritten Signature Processi ng, World Scientific, 95-109, 2014.",2
Abstract,CVPR_2020,4,"V. Nguyen, Y. Kawazoe, T. Wakabayashi, U. Pal, and M. Blumenstein, Performance Analysis of the Gradient Feature and the Modified Direction Feature for Off-line Signature Verification, ICFHR, 303-307, 2010.",2
Abstract,CVPR_2020,5,"M. Okawa, From BoVW to VLAD with KAZE features: Offline signature verification considering cognitive 13256 processes of forensic experts, Pattern Recognition Letters, 113:75-82, 2018.",2
Cross-Domain Semantic Segmentation via Domain-Invariant,CVPR_2020,1,"Yawei Luo, Ping Liu, Tao Guan, Junqing Yu, and Yi Yang. Signiﬁcance-aware information bottleneck for domain adaptive semantic segmentation. ICCV , 2019.",5
Cross-Domain Semantic Segmentation via Domain-Invariant,CVPR_2020,2,"Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: ground truth from computer games. In ECCV , pages 102–118, 2016.",4
Cross-Domain Semantic Segmentation via Domain-Invariant,CVPR_2020,3,"Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A Alemi. Inception-v4, inception- resnet and the impact of residual connections on learn- ing. In AAAI , 2017.",4
Cross-Domain Semantic Segmentation via Domain-Invariant,CVPR_2020,4,"Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, and Manmohan Chandraker. Domain adaptation for struc- tured output via discriminative patch representations. InICCV , 2019.",4
Cross-Domain Semantic Segmentation via Domain-Invariant,CVPR_2020,5,"Tuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick Perez. Dada: depth- aware domain adaptation in semantic segmentation. In ICCV , 2019.",5
Towards Inheritable Models for Open-Set Domain Adaptation,CVPR_2020,1,"Boris Chidlovskii, St ´ephane Clinchant, and Gabriela Csurka. Domain adaptation in the absence of source domain data. In ACM SIGKDD . ACM, 2016. 2",3
Towards Inheritable Models for Open-Set Domain Adaptation,CVPR_2020,2,"Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy M Hospedales. Deeper, broader and artier domain generaliza- tion. In ICCV , 2017. 2",4
Towards Inheritable Models for Open-Set Domain Adaptation,CVPR_2020,3,"Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR , 2015. 1",3
Towards Inheritable Models for Open-Set Domain Adaptation,CVPR_2020,4,"Subhankar Roy, Aliaksandr Siarohin, Enver Sangineto, Samuel Rota Bulo, Nicu Sebe, and Elisa Ricci. Unsuper- vised domain adaptation using feature-whitening and con- sensus loss. In CVPR , 2019. 2",6
Towards Inheritable Models for Open-Set Domain Adaptation,CVPR_2020,5,"Walter J Scheirer, Anderson de Rezende Rocha, Archana Sapkota, and Terrance E Boult. Toward open set recogni- tion. IEEE transactions on pattern analysis and machine intelligence , 35(7):1757–1772, 2012. 7",4
Zero-Assignment Constraint for Graph Matching with Outliers,CVPR_2020,1,"A. V olgenant. Solving the k-cardinality assignment problem by transformation. European Journal of Op- erational Research , 157(2):322–331, 2004.",2
Zero-Assignment Constraint for Graph Matching with Outliers,CVPR_2020,2,"Runzhong Wang, Junchi Yan, and Xiaokang Yang. Learning combinatorial embedding networks for deep graph matching. In ICCV , 2019.",3
Zero-Assignment Constraint for Graph Matching with Outliers,CVPR_2020,3,"Junchi Yan, Xu-Cheng Yin, Weiyao Lin, Cheng Deng, Hongyuan Zha, and Xiaokang Yang. A short survey of recent advances in graph matching. In ICMR , 2016.",6
Zero-Assignment Constraint for Graph Matching with Outliers,CVPR_2020,4,"Mikhail Zaslavskiy, Francis Bach, and Jean-Philippe Vert. A path following algorithm for the graph match- ing problem. IEEE TPAMI , 31(12):2227–2242, 2009.",3
Zero-Assignment Constraint for Graph Matching with Outliers,CVPR_2020,5,"Ron Zass and Amnon Shashua. Probabilistic graph and hypergraph matching. In CVPR , 2008.",1
SketchyCOCO: Image Generation from Freehand Scene Sketches,CVPR_2020,1,"Mathias Eitz, Ronald Richter, Tamy Boubekeur, Kristian Hildebrand, and Marc Alexa. Sketch-based shape retrieval. ACM Transactions on graphics (TOG) , 31(4):31, 2012.",5
SketchyCOCO: Image Generation from Freehand Scene Sketches,CVPR_2020,2,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in neural information pro- cessing systems , pages 5767–5777, 2017.",5
SketchyCOCO: Image Generation from Freehand Scene Sketches,CVPR_2020,3,"Chenfanfu Jiang, Yixin Zhu, Siyuan Qi, Siyuan Huang, Jenny Lin, Xiongwen Guo, Lap-Fai Yu, Demetri Terzopou- los, and Song-Chun Zhu. Conﬁgurable, photorealistic image rendering and ground truth synthesis by sampling stochas- tic grammars representing indoor scenes. arXiv preprint arXiv:1704.00112 , 2, 2017.",9
SketchyCOCO: Image Generation from Freehand Scene Sketches,CVPR_2020,4,"Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive nor- malization. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 2337–2346, 2019.",4
SketchyCOCO: Image Generation from Freehand Scene Sketches,CVPR_2020,5,"Martin Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875 , 2017.",3
Warp to the Future: Joint Forecasting of Features and Feature Motion,CVPR_2020,1,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. 5",4
Warp to the Future: Joint Forecasting of Features and Feature Motion,CVPR_2020,2,"Yijun Li, Chen Fang, Jimei Yang, Zhaowen Wang, Xin Lu, and Ming-Hsuan Yang. Flow-grounded spatial-temporal video prediction from still images. In Proceedings of the Eu- ropean Conference on Computer Vision (ECCV) , pages 600– 615, 2018. 3",6
Warp to the Future: Joint Forecasting of Features and Feature Motion,CVPR_2020,3,"Fitsum A Reda, Guilin Liu, Kevin J Shih, Robert Kirby, Jon Barker, David Tarjan, Andrew Tao, and Bryan Catanzaro. Sdc-net: Video prediction using spatially-displaced convolu- tion. In Proceedings of the European Conference on Com- puter Vision (ECCV) , pages 718–733, 2018. 1",8
Warp to the Future: Joint Forecasting of Features and Feature Motion,CVPR_2020,4,"Pauline Luc, Natalia Neverova, Camille Couprie, Jakob Ver- beek, and Yann LeCun. Predicting deeper into the future of semantic segmentation. In Proceedings of the IEEE In- ternational Conference on Computer Vision , pages 648–657, 2017. 1,2,5",5
Warp to the Future: Joint Forecasting of Features and Feature Motion,CVPR_2020,5,"Pauline Luc, Camille Couprie, Yann Lecun, and Jakob Ver- beek. Predicting future instance segmentation by forecasting convolutional features. In Proceedings of the European Con- ference on Computer Vision (ECCV) , pages 584–599, 2018. 2,3,4,5",4
One-Shot Domain Adaptation For Face Generation,CVPR_2020,1,"T. J. De Carvalho, C. Riess, E. Angelopoulou, H. Pedrini, and A. de Rezende Rocha. Exposing digital image forgeries by illumination color classiﬁcation. IEEE Transactions on Information Forensics and Security , 8(7):1182–1194, 2013. 2",2
One-Shot Domain Adaptation For Face Generation,CVPR_2020,2,"M. Dixit, R. Kwitt, M. Niethammer, and N. Vasconcelos. Aga: Attribute-guided augmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, pages 7455–7463, 2017. 2",2
One-Shot Domain Adaptation For Face Generation,CVPR_2020,3,"I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and A. C. Courville. Improved training of wasserstein gans. InAdvances in neural information processing systems , pages 5767–5777, 2017. 3",2
One-Shot Domain Adaptation For Face Generation,CVPR_2020,4,"A. Brock, T. Lim, J. M. Ritchie, and N. Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093 , 2016. 3",2
One-Shot Domain Adaptation For Face Generation,CVPR_2020,5,"M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875 , 2017. 3",2
Semantic Correspondence as an Optimal Transport Problem,CVPR_2020,1,"Charlotte Bunne, David Alvarez-Melis, Andreas Krause, and Stefanie Jegelka. Learning generative models across incom- parable spaces. In ICML , 2019. 2",4
Semantic Correspondence as an Optimal Transport Problem,CVPR_2020,2,"Seungryong Kim, Dongbo Min, Stephen Lin, and Kwanghoon Sohn. Dctm: Discrete-continuous transforma- tion matching for semantic ﬂow. In ICCV , 2017. 8",4
Semantic Correspondence as an Optimal Transport Problem,CVPR_2020,3,"Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discrimi- native localization. In CVPR , 2016. 2,4 4472",5
Semantic Correspondence as an Optimal Transport Problem,CVPR_2020,4,"Richard Sinkhorn. Diagonal equivalence to matrices with prescribed row and column sums. The American Mathemat- ical Monthly , 74(4):402–405, 1967. 5 4471",2
Semantic Correspondence as an Optimal Transport Problem,CVPR_2020,5,"Bumsub Ham, Minsu Cho, Cordelia Schmid, and Jean Ponce. Proposal ﬂow: Semantic correspondences from ob- ject proposals. IEEE TPAMI , 40(7):1711–1725, 2017. 6",4
Understanding Road Layout from Videos as a Whole,CVPR_2020,1,"Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780, 1997.",1
Understanding Road Layout from Videos as a Whole,CVPR_2020,2,"Lars Kunze, Tom Bruls, Tarlan Suleymanov, and Paul New- man. Reading between the Lanes: Road Layout Reconstruc- tion from Partially Segmented Scenes. In International Con- ference on Intelligent Transportation Systems (ITSC) , 2018.",4
Understanding Road Layout from Videos as a Whole,CVPR_2020,3,"Mike Schuster and Kuldip K Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing , 45(11):2673–2681, 1997.",1
Understanding Road Layout from Videos as a Whole,CVPR_2020,4,"Sunando Sengupta, Paul Sturgess, `Lubor Ladický, and Philip H. S. Torr. Automatic Dense Visual Semantic Mapping from Street-Level Imagery. In IROS , 2012.",4
Understanding Road Layout from Videos as a Whole,CVPR_2020,5,"Shubham Tulsiani, Richard Tucker, and Noah Snavely. Layer- structured 3D Scene Inference via View Synthesis. In ECCV , 2018.",3
Listen to Look: Action Recognition by Previewing Audio,CVPR_2020,1,"S. Buch, V . Escorcia, C. Shen, B. Ghanem, and J. Car- los Niebles. Sst: Single-stream temporal action proposals. InCVPR , 2017.",2
Listen to Look: Action Recognition by Previewing Audio,CVPR_2020,2,"A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Has- sidim, W. T. Freeman, and M. Rubinstein. Looking to lis- ten at the cocktail party: A speaker-independent audio-visual model for speech separation. In SIGGRAPH , 2018.",2
Listen to Look: Action Recognition by Previewing Audio,CVPR_2020,3,"M. Jain, J. Van Gemert, H. J ´egou, P. Bouthemy, and C. G. Snoek. Action localization with tubelets from motion. In CVPR , 2014.",2
Listen to Look: Action Recognition by Previewing Audio,CVPR_2020,4,"A. Owens, P. Isola, J. McDermott, A. Torralba, E. H. Adel- son, and W. T. Freeman. Visually indicated sounds. In CVPR , 2016.",2
Listen to Look: Action Recognition by Previewing Audio,CVPR_2020,5,"A. Owens, J. Wu, J. H. McDermott, W. T. Freeman, and A. Torralba. Ambient sound provides supervision for visual learning. In ECCV , 2016.",2
Continual Reinforcement Learning in 3D Non-stationary Environments,CVPR_2020,1,"Ian J Goodfellow, Mehdi Mirza, Da Xiao, Aaron Courville, and Yoshua Bengio. An Empirical Investigation of Catas- trophic Forgeting in Gradient-Based Neural Networks. arXiv preprint arXiv:1312.6211 , 2013. 4",5
Continual Reinforcement Learning in 3D Non-stationary Environments,CVPR_2020,2,"Vincenzo Lomonaco, Karan Desai, Eugenio Culurciello, and Davide Maltoni. Continual Reinforcement Learn- ing in 3D Non-stationary Environments. Arxiv pre-print arXiv:1905.10112v1 , 2019. 8",4
Continual Reinforcement Learning in 3D Non-stationary Environments,CVPR_2020,3,"Martial Mermillod, Aur ´elia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: investigating the continuum from catastrophic forgetting to age-limited learning effects. Fron- tiers in psychology , 4(August):504, 2013. 4",3
Continual Reinforcement Learning in 3D Non-stationary Environments,CVPR_2020,4,"Maruan Al-shedivat, Trapit Bansal, Yuri Burda, Ilya Sutskever, Igor Mordatch, and Pieter Abbeel. Continuous Adaptation via Meta-Learning in Nonstationary and Competi- tive Environments. In International Conference on Learning Representations (ICLR) , pages 1–21, 2018. 2,4",6
Continual Reinforcement Learning in 3D Non-stationary Environments,CVPR_2020,5,"Rahaf Aljundi, Klaas Kelchtermans, and Tinne Tuyte- laars. Task-Free Continual Learning. arXiv preprint arXiv:1812.03596 , pages 1–14, 2018. 2",3
SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth,CVPR_2020,1,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition , pages 770–778, 2016. 3",4
SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth,CVPR_2020,2,"Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision , pages 2961–2969, 2017. 3, 4,5",4
SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth,CVPR_2020,3,"David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep net- work. In Advances in neural information processing systems , pages 2366–2374, 2014. 1,5",3
SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth,CVPR_2020,4,"Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition , pages 3431–3440, 2015. 5",3
SDC-Depth: Semantic Divide-and-Conquer Network for Monocular Depth,CVPR_2020,5,"Cl ´ement Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE International Conference on Computer Vision , pages 3828–3838, 2019. 2",4
Agreement Between Saliency Maps and Human-Labeled Regions of Interest:,CVPR_2020,1,"ings and scale references, which have been shown to signif- icantly increase the false positive rate on a melanoma detec- tion task [ 17,36]. Thus, it is important to characterize the impact of these photograph attributes on model behavior. In addition to diversity in photo composition, the pa- tient cohort used in [ 15] included images from patients of varying age, biological sex, and skin tone, which often ex- hibit different visual appearance and prevalence of derma- tology conditions. Previous work [ 3] has demonstrated dis- parities in model performance across sex and skin tone on a facial recognition task, while [ 14] found no measurable correlation between model performance and skin tone val- ues on skin disease classiﬁcation tasks. The model in [ 15] showed slight variations on model accuracy across these de- mographic characteristics. To the best of our knowledge, however, there has been no previous analysis of how agree- ment in classiﬁcation explanations by humans and models vary within such a diverse patient cohort. Figure 1shows sample images demonstrating the vari- ability in image content, skin tone, and patient age. 2.3. Saliency Maps and Integrated Gradients What constitutes a useful model explanation is a topic of signiﬁcant debate [ 9,11,13]. One subset of these techniques explain how a model processes particular inputs [ 23,25,26, Figure 1: Sample images in the dataset. Some images are masked to omit sensitive information; the model had access to the full image, including this information. 30,31,39]. Other techniques describe what each component of a neural network does [ 2,16,24,40]. Yet another set alters the network to explicitly include an interpretability compo- nent or to generate explanations [ 12,20,37]. Our work uses saliency maps, a model explanation technique from the ﬁrst of these subsets. A saliency map assigns a score to each input pixel specifying how ‘important’ it was for the model performing the task. In this paper, we use Integrated Gradients [ 31] to gener- ate saliency maps. In this technique, a baseline image x′is speciﬁed. For each input x, the gradient of the model out- put score function Fis computed at each of mimages on the line segment between x′andx. The average gradient is scaled by the difference in pixel intensity to produce an importance score for each pixel iin the image: IGi(x) = (xi−x′ i)1 mm/summationdisplay k=1∂F(x′+k m(x−x′)) ∂xi We chose Integrated Gradients because this technique satis- ﬁes the properties of sensitivity and implementation invari- ance [ 31], unlike other explanation techniques mentioned above. Internal experiments on a single-image classiﬁcation model on the dataset from [ 15] showed minimal differences between saliency maps generated via Integrated Gradients and GradCAM [ 23], suggesting that our conclusions are not highly sensitive to the saliency map generation method. We combine Integrated Gradients with the Smooth- Grad [ 27] technique, in which saliency maps are averaged over noisy versions of each input. 2.4. Quantitative Explanation Evaluation Previous work on quantitative evaluation of model expla- nations has focused on the quality of the model explanation as a representation of the inner workings of the model. [ 2] quantiﬁes how well the activations of neurons segment human-labeled concepts within an image. This allows ﬁne- grained understanding of individual neurons within the net- work, but it is challenging to transfer this approach to new models because a large number of explicit concept-labels must be hand-crafted and collected for new datasets. Al- ternatively, [ 22] proposed a perturbation-based approachthat removes pixels with the highest attributions and exam- ines the effect on the network’s classiﬁcation score. Relat- edly, [ 6] proposed to crop input images to bounding boxes containing all salient regions and determine whether the network is still able to classify appropriately. This class of metrics quantiﬁes whether saliency maps accurately rep- resent the image pixels used to make network predictions, but does not measure whether such pixels agree with human intuition behind the decision-making process.",1
Agreement Between Saliency Maps and Human-Labeled Regions of Interest:,CVPR_2020,2,"takes an important step toward this goal by quanti- tatively comparing saliency maps with segmentations of an explicitly speciﬁed potential source of bias (ink markings on skin images). However, this requires pre-existing knowl- edge of image components that might bias the model. In- stead, we quantitatively compare saliency maps with human segmentations of regions that we expect should be used for model learning. This allows us to identify potential sources of bias without hypothesizing a priori that might exist. 3. Methods 3.1. Image Dataset We used the dataset in [ 15], comprised of 19,870 adult patient dermatology cases submitted to a teledermatology company from 2010 to 2018. Each case comprised 1-6 clinical images taken by medical assistants, along with 45 metadata ﬁelds specifying the patient’s demographic infor- mation, medical history, and symptoms. All data were de- identiﬁed according to HIPAA Safe Harbor prior to transfer to study authors. The protocol was reviewed by Advarra IRB (Columbia, MD), which determined that it was exempt from further review under 45 CFR 46. The reference standard skin condition for each case was based on aggregated opinions of multiple dermatologists from a panel of U.S. and Indian board-certiﬁed dermatol- ogists. When reviewing the images and additional med- ical information associated with a case, each dermatolo- gist independently generated a differential diagnosis, com- prised of a set of diagnosis codes and their respective conﬁ- dence. Diagnosis codes were drawn from the Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT) or entered as free-text if no appropriate SNOMED term was found. These diagnosis codes were manually mapped to a short list of 419 conditions of the appropriate granularity by three U.S. board-certiﬁed dermatologists. Ultimately, the 26 most prevalent conditions on this list were chosen, along with an ‘Other’ category assigned to the remaining condi- tions. Cases that contained multiple conditions or that were deemed not diagnosable were removed from the dataset. Cases collected between 2010 and 2017 were designated as the development set to train and optimize the model (16,114 cases), whereas cases collected afterwards were used as the test set to conduct all evaluations (3,756 cases). No patients were present in both the development and test set. More details about the construction of the dermatology dataset are available in [ 15]. 3.2. Classiﬁcation Model We used the network from [ 15] to predict condition la- bels from the images and metadata associated with each in- put case. Up to 6 input images per case were fed as input to a neural network with the Inception-v4 [ 33] architecture. The pre-logit layers of each of these sub-networks were av- eraged and concatenated with a one-hot vector encoding of the case metadata; this ﬁnal representation was fed to a soft- max layer outputting a vector of classiﬁcation scores for each of the 27 classes (26 conditions + ‘Other’ category). The weights of each Inception-v4-like arm of the net- work architecture were initialized to weights used for classi- ﬁcation of the ImageNet dataset [ 7]. Then, network weights were optimized using stochastic gradient descent on the training cases, augmented by random cropping, rotation, ﬂipping, and color perturbation to each image. Training proceeded for 100,000 steps with a batch size of 8. The ensemble version of the model was shown to per- form comparably with dermatologists and superior to pri- mary care physicians and nurse practitioners on a test sub- set, with top-1 accuracy of 66%, 63%, 44%, and 40% re- spectively. Additional details about the model architecture, training, and evaluation procedure are available in [ 15]. 3.3. Pathology ROI Label Collection We collected at least one ‘region-of-interest’ (ROI) la- bel for 1907 images from 1309 cases in the test set, sam- pled randomly from the distribution of conditions present in that dataset. For each image, three dermatologist- trained graders speciﬁed polygon-shaped regions contain- ing pathology as binary masks; pixels with pathology were assigned to 1, and all other pixels were assigned to 0. Graders ﬁrst indicated whether a skin condition was visi- ble within each image. For images with clearly-visible skin conditions, graders then labeled polygonal ROIs on each image. Some images were ambiguous to segment into a single ROI; for example, rash-like conditions often present as diffuse markings dispersed across large sections or sev- eral patches across the image. Graders determined whether there were fewer than ﬁve distinct pathology ROIs in the image. If so, graders individually outlined each ROI. If more than ﬁve ROIs were present in the image, graders in- dicated this and did not provide an ROI for the image. All subsequent analysis was done only on images for which all ROIs were labeled. For each image, ROI labels of differ- ent graders were combined by pixelwise majority vote to produce a consensus ROI label. We collected consensus ROI labels from three graders out of a pool of 50 graders for each image in the evalu- Figure 2: Distribution of Fleiss kappa scores indicating inter-rater agreement of ROI labels. Dark blue vertical lines represent individual datapoints within each histogram bin. ation set. The Fleiss kappa value, which measures inter- rater agreement, was 0.65 ±0.27 across the dataset; its dis- tribution is shown in Figure 2. Images with Fleiss kappa below 0.4 were discarded, resulting in a ﬁltered dataset of 1526 images from 1083 cases, with an average Fleiss kappa value of 0.76 ±0.16. This dataset is henceforth referred to as the ‘saliency evaluation dataset’. A diagram demonstrat- ing how ROIs were generated and used to compute model- human explanation agreement is shown in Figure 3. 3.4. Saliency Map Generation For each image in the saliency evaluation dataset, we used Integrated Gradients, described in Section 2.3, to gen- erate saliency maps. Each saliency map was generated us- ing SmoothGrad [ 27] on a 50-step path between a black baseline and input image. The ﬁnal saliency map was nor- malized to range from -1 to 1. All saliency maps generated are visualized using the techniques described in [ 32]. 3.5. Quantifying Explanation Agreement We used two metrics to quantify model-human explana- tion agreement on each image: a thresholded Dice score and Spearman’s rank correlation coefﬁcient. We then used case-level agreement metrics to compute dataset summary statistics as in Sections 4.2 and 4.3. In particular, we com- puted these statistics on only the image with the highest DS and SRCC across the case, in order to avoid overweighting cases with multiple images and to understand the model’s behavior on the most informative images. The thresholded Dice score (DS) was determined by computing the Dice score [ 8,28] between a thresholded ver- sion of the continuous valued Integrated Gradients scores and the binary grader-based ROI labels. Choosing an appro- priate threshold is non-trivial; the salient regions of differ- ent images vary in size and relative intensity. We chose the Figure 3: Process for generating explanation agreement metrics. ROIs are labeled by 3 dermatology-trained graders. Majority consensus selects the ﬁnal ROI that is compared to both the raw saliency map and a binary version of this map. threshold for each image to be the multiple of 0.01 between 0 and 1 that maximizes the computed Dice score for that particular image; since we follow this procedure for every image, relative rankings between examples remain valid. We used Spearman’s rank correlation coefﬁcient [ 29] (SRCC) as a complementary metric that does not depend on the choice of a threshold and explicitly characterizes the relative rankings of attributions in the saliency map. SRCC is determined by computing the Pearson correlation coef- ﬁcient between the ranks of the continuous valued scores produced by integrated gradients for each pixel and the cor- responding pixel-wise binary human-graded ROI labels. The pixelwise nature of the saliency maps yields metrics that are lower than the values in the segmentation literature, even when the generated saliency maps and the human- labeled regions of interest qualitatively agree. To provide context for what different agreement scores mean qualita- tively, Figure 4shows sample images, saliency maps, and human labeled ROIs, for different values of the two met- rics. Figure 4: Images with varying thresholded Dice scores and Spearman’s rank correlations. Each row shows an example with the original image (I), image overlaid with ROIs (I+ROI), image overlaid with ROIs and saliency map (I+ROI+SM), and saliency map overlaid with ROIs (ROI+SM). The Dice score metric ranges from 0 to 1, but the maximum value seen in our dataset was approximately 0.5. Some images are masked to omit sensitive information. 4. Experiments & Results 4.1. On which input examples is model­human ex­ planation agreement lowest? We ranked all images by DS and SRCC and qualitatively examined images with the best and worst model-human explanation agreement amongst correctly- and incorrectly- classiﬁed examples. Figure 5shows example images for which the DS and SRCC fall within either the top 10% or the bottom 10% of the dataset; these consist of both correctly and incorrectly-classiﬁed images. To further un- derstand low model-human agreement cases, we collected body part labels for images within the bottom 10% of the dataset. Trained human graders were instructed to label all body parts present in each image. The representation of various body parts present in the images within the bottom 10% of the dataset is shown in Figure 6. 4.2. How does model­human explanation agreement vary across different skin conditions? We analyzed the DS and SRCC across cases stratiﬁed by disease type. We further stratiﬁed this comparison by pre- Figure 5: Examples with lowest (left) and highest (right) model-human explanation agreement between the model saliency map (SM) and human-labelled ROI, as measured by both thresholded Dice score and Spearman’s rank correlation. Examples are further divided as correctly (top) or incorrectly (bottom) classiﬁed. Some images are masked to omit sensitive information. Figure 6: The fraction of examples containing a particular body part in the bottom 10% of the dataset in terms of both Dice score and Spearman’s rank correlation. A large pro- portion of images with facial features (oral cavity, lip, and chin) had low model-human explanation agreement. diction accuracy. Figure 7shows these results for conditions with at least 5 cases containing ROI-labeled images. Figure 7also shows example images, ROIs, and saliency maps from the bottom-three conditions in terms of both DSand SRCC. Amongst images from correctly classiﬁed cases, those with androgenetic alopecia and acne demonstrated notably lower mean Dice scores compared to other condi- tions. Amongst images from incorrectly classiﬁed cases, melanoma and seborrheic keratosis demonstrated notably lower mean Dice scores compared to other conditions. 4.3. How does model­human explanation agreement vary across different demographic groups? We performed statistical analysis to compare case-level model-human explanation agreement for sex, skin tone, and age, each in terms of DS and SRCC. For these six analyses, we used a Bonferroni-adjusted α= 0.05/6 = 0.0083 . The results in Figure 8indicate a signiﬁcant difference in model- human explanation agreement based on sex (Two-sample t-test|DS: t=-3.67, p <0.001 ; SRCC: t=-3.36, p <0.001 ), but not based on skin tone (1-way ANOV A |DS: f=0.42, p=0.83 ; SRCC: f=2.66, p=0.02 ) or age (Pearson correlation |DS:ρ=0.10, p<0.001 ; SRCC:ρ=0.04, p=0.16 ). 5. Discussion The left column of Figure 5suggests that images with certain body parts (e.g., lips, hair, and ﬁngernails) demon- strate the starkest differences between saliency maps and human-labeled ROIs. This ﬁnding is further veriﬁed via the Figure 7: Model-human explanation agreement by skin condition classes. The scatterplot compares thresholded Dice scores and Spearman’s rank correlation examples of correctly (green) and incorrectly (red) classiﬁed cases. Di- amonds denote conditions amongst the bottom-3 lowest in terms of both DS and SRCC; sample images, ROIs, and saliency maps for these conditions are shown below. Some images are masked to omit sensitive information.body part distribution for low agreement cases in Figure 6 (note that this ﬁgure plots allpresent body parts in an image, not just the ones highlighted by its corresponding saliency map). The model attends more strongly to this “normal anatomy” even when clear skin pathology is present in a spatially distinct location. After examining the example im- ages, a board-certiﬁed dermatologist conﬁrmed the conclu- sion that this anatomy is inappropriate as primary criteria to determine skin conditions for those cases. Though our analysis does not determine what causes speciﬁc model pre- dictions, the repeated presence of these elements in images with low model-human explanation agreement suggests that the model might associate these features with certain condi- tions, instead of focusing on the pathology of interest itself. In contrast, the right column of Figure 5shows cases where the model correctly focused on pathology. This accu- rate saliency localization makes intuitive sense for correctly classiﬁed examples. However, for incorrectly classiﬁed ex- amples, the model focuses on the correct spatial locations but may be misinterpreting the texture. Model explanations beyond saliency maps are needed to understand this issue. Figure 8: Model-human explanation agreement by demo- graphic groups. From top to bottom, the plots represent the thresholded Dice score (left) and Spearman’s rank correla- tion coefﬁcient (right) for different sexes, skin tones, and ages respectively. In all bar plots, error bars indicate a 95% bootstrap conﬁdence interval. When stratiﬁed by model prediction accuracy, androge- netic alopecia and acne images have lower mean human- model explanation than other conditions, a ﬁnding con- sistent with the qualitative observation that the model fre- quently misattends to hair and lips in images. The andro- genetic alopecia ﬁnding indicates that either: (1) the model correctly uses hair as context with which to compare hair loss that occurs in androgenetic alopecia, or (2) the model has learned that images that have hair in them are more likely to be androgenetic alopecia, regardless of the spe- ciﬁc hair loss pattern. Though we cannot deﬁnitively accept either of these hypotheses, qualitative analysis of the exam- ples shows that the model attends to regions where the hair is present and not where the hair is absent, which is incon- sistent with the ﬁrst hypothesis. Similarly, many of the acne images in our dataset show acne on the face. This, com- bined with the attention paid by the model to the patient’s lips instead of the affected lesion, suggest that the model might have learned to associate facial features with acne. Understanding why certain conditions exhibit particu- larly low human-model explanation agreement on incor- rectly classiﬁed exmamples is more difﬁcult. Though sam- ple images from these classes conﬁrm the qualitative ob- servation that the model occasionally attends to ﬁngernails (and other “normal anatomy” image features) instead of relevant lesions, there is no intuitive connection between these characteristics and the corresponding disease cate- gories (melanoma and seborrheic keratosis). These are both conditions with low representation in the training dataset (0.6% and 4.4%, respectively); one hypothesis is that, with- out enough training examples to construct a representation of the underlying pathology, the model instead focuses on auxiliary image components. If so, training with additional melanoma and seborrheic keratosis images could improve both the model-human explanation agreement and the accu- racy on these conditions. Alternatively, these results suggest that incorporating an object-detection or pre-processing al- gorithm to identify or remove these components of images could encourage the model to generalize better. Finally, stratiﬁcation by demographic groups conﬁrmed that skin tone and age do not signiﬁcantly affect human- model explanation agreement, even though these character- istics affect the visual appearance of skin and hair. On the other hand, stratiﬁcation by sex didyield a slight, statis- tically signiﬁcant difference in human-model explanation agreement. This might be due to the difference in natural distributions of conditions with varying levels of agreement. In all of our experiments, we observed similar trends in the thresholded Dice score and Spearman’s rank correlation. While these metrics differ in speciﬁc examples as shown in Figure 4, the summary statistics across different data strati- ﬁcations demonstrate similar trends for both metrics. One concern about our method is that collected ROIsrepresent only pathological regions, and that it is reason- able for the model to attend to other image context (e.g. to understand skin tone, compare to normal skin character- istics, etc.). We address this issue via adaptive threshold- ing of the Dice score. In particular, while we might expect the model to attend to external context, it seems unlikely that these “non-pathology ROIs” should be more important than pathology ROIs. We pick a threshold on the saliency map to produce a ‘saliency segmentation’ that maximizes the Dice score with the labeled pathology ROI. If the model is behaving correctly, we expect there to exist a threshold at which the saliency segmentation includes pathology ROIs but not non-pathology ROIs, yielding a high Dice score. Thus, even if the model does use non-pathology ROIs, if it selects the correct the pathology ROIs, we would expect a high Dice score. By a similar argument, we would expect a high Spearman’s rank correlation, since that metric is ex- plicitly based on pixel rankings; even if the model uses both non-pathology and pathology ROIs, we would expect the pathology ROIs to rank higher. However, the assumption that pathology ROIs should be more important than non- pathology ROIs might not hold. Future work to address this would increase the robustness of our technique. Another potential concern is that our method is sensi- tive to the underlying saliency map generation technique. However, our strategy is applicable to maps generated via anytechnique. Thus, the high-level method in this paper is applicable even as saliency map generation techniques con- tinue to improve. Further, this high-level method could be applied to maps generated via a suite of techniques and used to identify persistent trends. 6. Conclusion We conducted a quantitative assessment of the agree- ment between model-based saliency maps and human- labeled regions-of-interest in a skin condition classiﬁca- tion task from consumer-grade camera images. We also computed statistics that summarize trends in this agreement for different skin conditions and demographics. We found that several examples for which model-human explanation agreement were lowest were cases in which the model iden- tiﬁed normal anatomy (e.g. lips, hair, and ﬁngernails with- out pathology) as important for diagnosing disease, par- ticularly for correctly-classiﬁed androgenetic alopecia and acne examples and for incorrectly-classiﬁed melanoma and seborrheic keratosis examples. Further, we found signiﬁ- cant differences in model-human explanation agreement be- tween different sexes, but not between groups of different age or skin tone. These ﬁndings suggest future data collec- tion and model development strategies that could improve network performance and generalizability. References",2
Agreement Between Saliency Maps and Human-Labeled Regions of Interest:,CVPR_2020,3,"ings and scale references, which have been shown to signif- icantly increase the false positive rate on a melanoma detec- tion task [ 17,36]. Thus, it is important to characterize the impact of these photograph attributes on model behavior. In addition to diversity in photo composition, the pa- tient cohort used in [ 15] included images from patients of varying age, biological sex, and skin tone, which often ex- hibit different visual appearance and prevalence of derma- tology conditions. Previous work [ 3] has demonstrated dis- parities in model performance across sex and skin tone on a facial recognition task, while [ 14] found no measurable correlation between model performance and skin tone val- ues on skin disease classiﬁcation tasks. The model in [ 15] showed slight variations on model accuracy across these de- mographic characteristics. To the best of our knowledge, however, there has been no previous analysis of how agree- ment in classiﬁcation explanations by humans and models vary within such a diverse patient cohort. Figure 1shows sample images demonstrating the vari- ability in image content, skin tone, and patient age. 2.3. Saliency Maps and Integrated Gradients What constitutes a useful model explanation is a topic of signiﬁcant debate [ 9,11,13]. One subset of these techniques explain how a model processes particular inputs [ 23,25,26, Figure 1: Sample images in the dataset. Some images are masked to omit sensitive information; the model had access to the full image, including this information. 30,31,39]. Other techniques describe what each component of a neural network does [ 2,16,24,40]. Yet another set alters the network to explicitly include an interpretability compo- nent or to generate explanations [ 12,20,37]. Our work uses saliency maps, a model explanation technique from the ﬁrst of these subsets. A saliency map assigns a score to each input pixel specifying how ‘important’ it was for the model performing the task. In this paper, we use Integrated Gradients [ 31] to gener- ate saliency maps. In this technique, a baseline image x′is speciﬁed. For each input x, the gradient of the model out- put score function Fis computed at each of mimages on the line segment between x′andx. The average gradient is scaled by the difference in pixel intensity to produce an importance score for each pixel iin the image: IGi(x) = (xi−x′ i)1 mm/summationdisplay k=1∂F(x′+k m(x−x′)) ∂xi We chose Integrated Gradients because this technique satis- ﬁes the properties of sensitivity and implementation invari- ance [ 31], unlike other explanation techniques mentioned above. Internal experiments on a single-image classiﬁcation model on the dataset from [ 15] showed minimal differences between saliency maps generated via Integrated Gradients and GradCAM [ 23], suggesting that our conclusions are not highly sensitive to the saliency map generation method. We combine Integrated Gradients with the Smooth- Grad [ 27] technique, in which saliency maps are averaged over noisy versions of each input. 2.4. Quantitative Explanation Evaluation Previous work on quantitative evaluation of model expla- nations has focused on the quality of the model explanation as a representation of the inner workings of the model. [ 2] quantiﬁes how well the activations of neurons segment human-labeled concepts within an image. This allows ﬁne- grained understanding of individual neurons within the net- work, but it is challenging to transfer this approach to new models because a large number of explicit concept-labels must be hand-crafted and collected for new datasets. Al- ternatively, [ 22] proposed a perturbation-based approachthat removes pixels with the highest attributions and exam- ines the effect on the network’s classiﬁcation score. Relat- edly, [ 6] proposed to crop input images to bounding boxes containing all salient regions and determine whether the network is still able to classify appropriately. This class of metrics quantiﬁes whether saliency maps accurately rep- resent the image pixels used to make network predictions, but does not measure whether such pixels agree with human intuition behind the decision-making process.",1
Agreement Between Saliency Maps and Human-Labeled Regions of Interest:,CVPR_2020,4,"takes an important step toward this goal by quanti- tatively comparing saliency maps with segmentations of an explicitly speciﬁed potential source of bias (ink markings on skin images). However, this requires pre-existing knowl- edge of image components that might bias the model. In- stead, we quantitatively compare saliency maps with human segmentations of regions that we expect should be used for model learning. This allows us to identify potential sources of bias without hypothesizing a priori that might exist. 3. Methods 3.1. Image Dataset We used the dataset in [ 15], comprised of 19,870 adult patient dermatology cases submitted to a teledermatology company from 2010 to 2018. Each case comprised 1-6 clinical images taken by medical assistants, along with 45 metadata ﬁelds specifying the patient’s demographic infor- mation, medical history, and symptoms. All data were de- identiﬁed according to HIPAA Safe Harbor prior to transfer to study authors. The protocol was reviewed by Advarra IRB (Columbia, MD), which determined that it was exempt from further review under 45 CFR 46. The reference standard skin condition for each case was based on aggregated opinions of multiple dermatologists from a panel of U.S. and Indian board-certiﬁed dermatol- ogists. When reviewing the images and additional med- ical information associated with a case, each dermatolo- gist independently generated a differential diagnosis, com- prised of a set of diagnosis codes and their respective conﬁ- dence. Diagnosis codes were drawn from the Systematized Nomenclature of Medicine-Clinical Terms (SNOMED-CT) or entered as free-text if no appropriate SNOMED term was found. These diagnosis codes were manually mapped to a short list of 419 conditions of the appropriate granularity by three U.S. board-certiﬁed dermatologists. Ultimately, the 26 most prevalent conditions on this list were chosen, along with an ‘Other’ category assigned to the remaining condi- tions. Cases that contained multiple conditions or that were deemed not diagnosable were removed from the dataset. Cases collected between 2010 and 2017 were designated as the development set to train and optimize the model (16,114 cases), whereas cases collected afterwards were used as the test set to conduct all evaluations (3,756 cases). No patients were present in both the development and test set. More details about the construction of the dermatology dataset are available in [ 15]. 3.2. Classiﬁcation Model We used the network from [ 15] to predict condition la- bels from the images and metadata associated with each in- put case. Up to 6 input images per case were fed as input to a neural network with the Inception-v4 [ 33] architecture. The pre-logit layers of each of these sub-networks were av- eraged and concatenated with a one-hot vector encoding of the case metadata; this ﬁnal representation was fed to a soft- max layer outputting a vector of classiﬁcation scores for each of the 27 classes (26 conditions + ‘Other’ category). The weights of each Inception-v4-like arm of the net- work architecture were initialized to weights used for classi- ﬁcation of the ImageNet dataset [ 7]. Then, network weights were optimized using stochastic gradient descent on the training cases, augmented by random cropping, rotation, ﬂipping, and color perturbation to each image. Training proceeded for 100,000 steps with a batch size of 8. The ensemble version of the model was shown to per- form comparably with dermatologists and superior to pri- mary care physicians and nurse practitioners on a test sub- set, with top-1 accuracy of 66%, 63%, 44%, and 40% re- spectively. Additional details about the model architecture, training, and evaluation procedure are available in [ 15]. 3.3. Pathology ROI Label Collection We collected at least one ‘region-of-interest’ (ROI) la- bel for 1907 images from 1309 cases in the test set, sam- pled randomly from the distribution of conditions present in that dataset. For each image, three dermatologist- trained graders speciﬁed polygon-shaped regions contain- ing pathology as binary masks; pixels with pathology were assigned to 1, and all other pixels were assigned to 0. Graders ﬁrst indicated whether a skin condition was visi- ble within each image. For images with clearly-visible skin conditions, graders then labeled polygonal ROIs on each image. Some images were ambiguous to segment into a single ROI; for example, rash-like conditions often present as diffuse markings dispersed across large sections or sev- eral patches across the image. Graders determined whether there were fewer than ﬁve distinct pathology ROIs in the image. If so, graders individually outlined each ROI. If more than ﬁve ROIs were present in the image, graders in- dicated this and did not provide an ROI for the image. All subsequent analysis was done only on images for which all ROIs were labeled. For each image, ROI labels of differ- ent graders were combined by pixelwise majority vote to produce a consensus ROI label. We collected consensus ROI labels from three graders out of a pool of 50 graders for each image in the evalu- Figure 2: Distribution of Fleiss kappa scores indicating inter-rater agreement of ROI labels. Dark blue vertical lines represent individual datapoints within each histogram bin. ation set. The Fleiss kappa value, which measures inter- rater agreement, was 0.65 ±0.27 across the dataset; its dis- tribution is shown in Figure 2. Images with Fleiss kappa below 0.4 were discarded, resulting in a ﬁltered dataset of 1526 images from 1083 cases, with an average Fleiss kappa value of 0.76 ±0.16. This dataset is henceforth referred to as the ‘saliency evaluation dataset’. A diagram demonstrat- ing how ROIs were generated and used to compute model- human explanation agreement is shown in Figure 3. 3.4. Saliency Map Generation For each image in the saliency evaluation dataset, we used Integrated Gradients, described in Section 2.3, to gen- erate saliency maps. Each saliency map was generated us- ing SmoothGrad [ 27] on a 50-step path between a black baseline and input image. The ﬁnal saliency map was nor- malized to range from -1 to 1. All saliency maps generated are visualized using the techniques described in [ 32]. 3.5. Quantifying Explanation Agreement We used two metrics to quantify model-human explana- tion agreement on each image: a thresholded Dice score and Spearman’s rank correlation coefﬁcient. We then used case-level agreement metrics to compute dataset summary statistics as in Sections 4.2 and 4.3. In particular, we com- puted these statistics on only the image with the highest DS and SRCC across the case, in order to avoid overweighting cases with multiple images and to understand the model’s behavior on the most informative images. The thresholded Dice score (DS) was determined by computing the Dice score [ 8,28] between a thresholded ver- sion of the continuous valued Integrated Gradients scores and the binary grader-based ROI labels. Choosing an appro- priate threshold is non-trivial; the salient regions of differ- ent images vary in size and relative intensity. We chose the Figure 3: Process for generating explanation agreement metrics. ROIs are labeled by 3 dermatology-trained graders. Majority consensus selects the ﬁnal ROI that is compared to both the raw saliency map and a binary version of this map. threshold for each image to be the multiple of 0.01 between 0 and 1 that maximizes the computed Dice score for that particular image; since we follow this procedure for every image, relative rankings between examples remain valid. We used Spearman’s rank correlation coefﬁcient [ 29] (SRCC) as a complementary metric that does not depend on the choice of a threshold and explicitly characterizes the relative rankings of attributions in the saliency map. SRCC is determined by computing the Pearson correlation coef- ﬁcient between the ranks of the continuous valued scores produced by integrated gradients for each pixel and the cor- responding pixel-wise binary human-graded ROI labels. The pixelwise nature of the saliency maps yields metrics that are lower than the values in the segmentation literature, even when the generated saliency maps and the human- labeled regions of interest qualitatively agree. To provide context for what different agreement scores mean qualita- tively, Figure 4shows sample images, saliency maps, and human labeled ROIs, for different values of the two met- rics. Figure 4: Images with varying thresholded Dice scores and Spearman’s rank correlations. Each row shows an example with the original image (I), image overlaid with ROIs (I+ROI), image overlaid with ROIs and saliency map (I+ROI+SM), and saliency map overlaid with ROIs (ROI+SM). The Dice score metric ranges from 0 to 1, but the maximum value seen in our dataset was approximately 0.5. Some images are masked to omit sensitive information. 4. Experiments & Results 4.1. On which input examples is model­human ex­ planation agreement lowest? We ranked all images by DS and SRCC and qualitatively examined images with the best and worst model-human explanation agreement amongst correctly- and incorrectly- classiﬁed examples. Figure 5shows example images for which the DS and SRCC fall within either the top 10% or the bottom 10% of the dataset; these consist of both correctly and incorrectly-classiﬁed images. To further un- derstand low model-human agreement cases, we collected body part labels for images within the bottom 10% of the dataset. Trained human graders were instructed to label all body parts present in each image. The representation of various body parts present in the images within the bottom 10% of the dataset is shown in Figure 6. 4.2. How does model­human explanation agreement vary across different skin conditions? We analyzed the DS and SRCC across cases stratiﬁed by disease type. We further stratiﬁed this comparison by pre- Figure 5: Examples with lowest (left) and highest (right) model-human explanation agreement between the model saliency map (SM) and human-labelled ROI, as measured by both thresholded Dice score and Spearman’s rank correlation. Examples are further divided as correctly (top) or incorrectly (bottom) classiﬁed. Some images are masked to omit sensitive information. Figure 6: The fraction of examples containing a particular body part in the bottom 10% of the dataset in terms of both Dice score and Spearman’s rank correlation. A large pro- portion of images with facial features (oral cavity, lip, and chin) had low model-human explanation agreement. diction accuracy. Figure 7shows these results for conditions with at least 5 cases containing ROI-labeled images. Figure 7also shows example images, ROIs, and saliency maps from the bottom-three conditions in terms of both DSand SRCC. Amongst images from correctly classiﬁed cases, those with androgenetic alopecia and acne demonstrated notably lower mean Dice scores compared to other condi- tions. Amongst images from incorrectly classiﬁed cases, melanoma and seborrheic keratosis demonstrated notably lower mean Dice scores compared to other conditions. 4.3. How does model­human explanation agreement vary across different demographic groups? We performed statistical analysis to compare case-level model-human explanation agreement for sex, skin tone, and age, each in terms of DS and SRCC. For these six analyses, we used a Bonferroni-adjusted α= 0.05/6 = 0.0083 . The results in Figure 8indicate a signiﬁcant difference in model- human explanation agreement based on sex (Two-sample t-test|DS: t=-3.67, p <0.001 ; SRCC: t=-3.36, p <0.001 ), but not based on skin tone (1-way ANOV A |DS: f=0.42, p=0.83 ; SRCC: f=2.66, p=0.02 ) or age (Pearson correlation |DS:ρ=0.10, p<0.001 ; SRCC:ρ=0.04, p=0.16 ). 5. Discussion The left column of Figure 5suggests that images with certain body parts (e.g., lips, hair, and ﬁngernails) demon- strate the starkest differences between saliency maps and human-labeled ROIs. This ﬁnding is further veriﬁed via the Figure 7: Model-human explanation agreement by skin condition classes. The scatterplot compares thresholded Dice scores and Spearman’s rank correlation examples of correctly (green) and incorrectly (red) classiﬁed cases. Di- amonds denote conditions amongst the bottom-3 lowest in terms of both DS and SRCC; sample images, ROIs, and saliency maps for these conditions are shown below. Some images are masked to omit sensitive information.body part distribution for low agreement cases in Figure 6 (note that this ﬁgure plots allpresent body parts in an image, not just the ones highlighted by its corresponding saliency map). The model attends more strongly to this “normal anatomy” even when clear skin pathology is present in a spatially distinct location. After examining the example im- ages, a board-certiﬁed dermatologist conﬁrmed the conclu- sion that this anatomy is inappropriate as primary criteria to determine skin conditions for those cases. Though our analysis does not determine what causes speciﬁc model pre- dictions, the repeated presence of these elements in images with low model-human explanation agreement suggests that the model might associate these features with certain condi- tions, instead of focusing on the pathology of interest itself. In contrast, the right column of Figure 5shows cases where the model correctly focused on pathology. This accu- rate saliency localization makes intuitive sense for correctly classiﬁed examples. However, for incorrectly classiﬁed ex- amples, the model focuses on the correct spatial locations but may be misinterpreting the texture. Model explanations beyond saliency maps are needed to understand this issue. Figure 8: Model-human explanation agreement by demo- graphic groups. From top to bottom, the plots represent the thresholded Dice score (left) and Spearman’s rank correla- tion coefﬁcient (right) for different sexes, skin tones, and ages respectively. In all bar plots, error bars indicate a 95% bootstrap conﬁdence interval. When stratiﬁed by model prediction accuracy, androge- netic alopecia and acne images have lower mean human- model explanation than other conditions, a ﬁnding con- sistent with the qualitative observation that the model fre- quently misattends to hair and lips in images. The andro- genetic alopecia ﬁnding indicates that either: (1) the model correctly uses hair as context with which to compare hair loss that occurs in androgenetic alopecia, or (2) the model has learned that images that have hair in them are more likely to be androgenetic alopecia, regardless of the spe- ciﬁc hair loss pattern. Though we cannot deﬁnitively accept either of these hypotheses, qualitative analysis of the exam- ples shows that the model attends to regions where the hair is present and not where the hair is absent, which is incon- sistent with the ﬁrst hypothesis. Similarly, many of the acne images in our dataset show acne on the face. This, com- bined with the attention paid by the model to the patient’s lips instead of the affected lesion, suggest that the model might have learned to associate facial features with acne. Understanding why certain conditions exhibit particu- larly low human-model explanation agreement on incor- rectly classiﬁed exmamples is more difﬁcult. Though sam- ple images from these classes conﬁrm the qualitative ob- servation that the model occasionally attends to ﬁngernails (and other “normal anatomy” image features) instead of relevant lesions, there is no intuitive connection between these characteristics and the corresponding disease cate- gories (melanoma and seborrheic keratosis). These are both conditions with low representation in the training dataset (0.6% and 4.4%, respectively); one hypothesis is that, with- out enough training examples to construct a representation of the underlying pathology, the model instead focuses on auxiliary image components. If so, training with additional melanoma and seborrheic keratosis images could improve both the model-human explanation agreement and the accu- racy on these conditions. Alternatively, these results suggest that incorporating an object-detection or pre-processing al- gorithm to identify or remove these components of images could encourage the model to generalize better. Finally, stratiﬁcation by demographic groups conﬁrmed that skin tone and age do not signiﬁcantly affect human- model explanation agreement, even though these character- istics affect the visual appearance of skin and hair. On the other hand, stratiﬁcation by sex didyield a slight, statis- tically signiﬁcant difference in human-model explanation agreement. This might be due to the difference in natural distributions of conditions with varying levels of agreement. In all of our experiments, we observed similar trends in the thresholded Dice score and Spearman’s rank correlation. While these metrics differ in speciﬁc examples as shown in Figure 4, the summary statistics across different data strati- ﬁcations demonstrate similar trends for both metrics. One concern about our method is that collected ROIsrepresent only pathological regions, and that it is reason- able for the model to attend to other image context (e.g. to understand skin tone, compare to normal skin character- istics, etc.). We address this issue via adaptive threshold- ing of the Dice score. In particular, while we might expect the model to attend to external context, it seems unlikely that these “non-pathology ROIs” should be more important than pathology ROIs. We pick a threshold on the saliency map to produce a ‘saliency segmentation’ that maximizes the Dice score with the labeled pathology ROI. If the model is behaving correctly, we expect there to exist a threshold at which the saliency segmentation includes pathology ROIs but not non-pathology ROIs, yielding a high Dice score. Thus, even if the model does use non-pathology ROIs, if it selects the correct the pathology ROIs, we would expect a high Dice score. By a similar argument, we would expect a high Spearman’s rank correlation, since that metric is ex- plicitly based on pixel rankings; even if the model uses both non-pathology and pathology ROIs, we would expect the pathology ROIs to rank higher. However, the assumption that pathology ROIs should be more important than non- pathology ROIs might not hold. Future work to address this would increase the robustness of our technique. Another potential concern is that our method is sensi- tive to the underlying saliency map generation technique. However, our strategy is applicable to maps generated via anytechnique. Thus, the high-level method in this paper is applicable even as saliency map generation techniques con- tinue to improve. Further, this high-level method could be applied to maps generated via a suite of techniques and used to identify persistent trends. 6. Conclusion We conducted a quantitative assessment of the agree- ment between model-based saliency maps and human- labeled regions-of-interest in a skin condition classiﬁca- tion task from consumer-grade camera images. We also computed statistics that summarize trends in this agreement for different skin conditions and demographics. We found that several examples for which model-human explanation agreement were lowest were cases in which the model iden- tiﬁed normal anatomy (e.g. lips, hair, and ﬁngernails with- out pathology) as important for diagnosing disease, par- ticularly for correctly-classiﬁed androgenetic alopecia and acne examples and for incorrectly-classiﬁed melanoma and seborrheic keratosis examples. Further, we found signiﬁ- cant differences in model-human explanation agreement be- tween different sexes, but not between groups of different age or skin tone. These ﬁndings suggest future data collec- tion and model development strategies that could improve network performance and generalizability. References",2
Agreement Between Saliency Maps and Human-Labeled Regions of Interest:,CVPR_2020,5,"ings and scale references, which have been shown to signif- icantly increase the false positive rate on a melanoma detec- tion task [ 17,36]. Thus, it is important to characterize the impact of these photograph attributes on model behavior. In addition to diversity in photo composition, the pa- tient cohort used in [ 15] included images from patients of varying age, biological sex, and skin tone, which often ex- hibit different visual appearance and prevalence of derma- tology conditions. Previous work [ 3] has demonstrated dis- parities in model performance across sex and skin tone on a facial recognition task, while [ 14] found no measurable correlation between model performance and skin tone val- ues on skin disease classiﬁcation tasks. The model in [ 15] showed slight variations on model accuracy across these de- mographic characteristics. To the best of our knowledge, however, there has been no previous analysis of how agree- ment in classiﬁcation explanations by humans and models vary within such a diverse patient cohort. Figure 1shows sample images demonstrating the vari- ability in image content, skin tone, and patient age. 2.3. Saliency Maps and Integrated Gradients What constitutes a useful model explanation is a topic of signiﬁcant debate [ 9,11,13]. One subset of these techniques explain how a model processes particular inputs [ 23,25,26, Figure 1: Sample images in the dataset. Some images are masked to omit sensitive information; the model had access to the full image, including this information. 30,31,39]. Other techniques describe what each component of a neural network does [ 2,16,24,40]. Yet another set alters the network to explicitly include an interpretability compo- nent or to generate explanations [ 12,20,37]. Our work uses saliency maps, a model explanation technique from the ﬁrst of these subsets. A saliency map assigns a score to each input pixel specifying how ‘important’ it was for the model performing the task. In this paper, we use Integrated Gradients [ 31] to gener- ate saliency maps. In this technique, a baseline image x′is speciﬁed. For each input x, the gradient of the model out- put score function Fis computed at each of mimages on the line segment between x′andx. The average gradient is scaled by the difference in pixel intensity to produce an importance score for each pixel iin the image: IGi(x) = (xi−x′ i)1 mm/summationdisplay k=1∂F(x′+k m(x−x′)) ∂xi We chose Integrated Gradients because this technique satis- ﬁes the properties of sensitivity and implementation invari- ance [ 31], unlike other explanation techniques mentioned above. Internal experiments on a single-image classiﬁcation model on the dataset from [ 15] showed minimal differences between saliency maps generated via Integrated Gradients and GradCAM [ 23], suggesting that our conclusions are not highly sensitive to the saliency map generation method. We combine Integrated Gradients with the Smooth- Grad [ 27] technique, in which saliency maps are averaged over noisy versions of each input. 2.4. Quantitative Explanation Evaluation Previous work on quantitative evaluation of model expla- nations has focused on the quality of the model explanation as a representation of the inner workings of the model. [ 2] quantiﬁes how well the activations of neurons segment human-labeled concepts within an image. This allows ﬁne- grained understanding of individual neurons within the net- work, but it is challenging to transfer this approach to new models because a large number of explicit concept-labels must be hand-crafted and collected for new datasets. Al- ternatively, [ 22] proposed a perturbation-based approachthat removes pixels with the highest attributions and exam- ines the effect on the network’s classiﬁcation score. Relat- edly, [ 6] proposed to crop input images to bounding boxes containing all salient regions and determine whether the network is still able to classify appropriately. This class of metrics quantiﬁes whether saliency maps accurately rep- resent the image pixels used to make network predictions, but does not measure whether such pixels agree with human intuition behind the decision-making process.",1
Dual Embedding Expansion for Vehicle Re-identiﬁcation,CVPR_2020,1,"Relja Arandjelovic and Andrew Zisserman. Three things ev- eryone should know to improve object retrieval. 2012 IEEE Conference on Computer Vision and Pattern Recognition , pages 2911–2918, 2012. 5",1
Dual Embedding Expansion for Vehicle Re-identiﬁcation,CVPR_2020,2,"Abner Ayala-Acevedo, Akash Devgun, Sadri Zahir, and Sid Askary. Vehicle re-identiﬁcation: Pushing the limits of re- identiﬁcation. In CVPR Workshops , 2019. 2",4
Dual Embedding Expansion for Vehicle Re-identiﬁcation,CVPR_2020,3,"Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf: Speeded up robust features. In ECCV , 2006. 2",3
Dual Embedding Expansion for Vehicle Re-identiﬁcation,CVPR_2020,4,"Hao Chen, Benoit Lagadec, and Francois Bremond. Partition and reunion: A two-branch neural network for vehicle re- identiﬁcation. In Proc. CVPR Workshops , pages 184–192, 2019. 2,3,4,7[5] Yu-Cheng Chen, Longlong Jing, Elahe Vahdani, Ling Zhang, Mingyi He, and Yingli Tian. Multi-camera vehicle track- ing and re-identiﬁcation on ai city challenge 2019. In CVPR Workshops , 2019. 2",3
Dual Embedding Expansion for Vehicle Re-identiﬁcation,CVPR_2020,5,"Navneet Dalal and Bill Triggs. Histograms of oriented gra- dients for human detection. In 2005 IEEE computer soci- ety conference on computer vision and pattern recognition (CVPR’05) , volume 1, pages 886–893. IEEE, 2005. 2",1
Hierarchical Conditional Relation Networks for Video Question Answering,CVPR_2020,1,"Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional net- works. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 933–941. JMLR. org, 2017. 3.1",4
Hierarchical Conditional Relation Networks for Video Question Answering,CVPR_2020,2,"Junyeong Kim, Minuk Ma, Kyungsu Kim, Sungjin Kim, and Chang D Yoo. Progressive attention memory network for movie story question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8337–8346, 2019. 2",5
Hierarchical Conditional Relation Networks for Video Question Answering,CVPR_2020,3,"Kyung-Min Kim, Seong-Ho Choi, Jin-Hwa Kim, and Byoung- Tak Zhang. Multimodal dual attention memory for videostory question answering. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 673–688, 2018. 2",4
Hierarchical Conditional Relation Networks for Video Question Answering,CVPR_2020,4,"Lorenzo Baraldi, Costantino Grana, and Rita Cucchiara. Hier- archical boundary-aware neural encoder for video captioning. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 1657–1666, 2017. 2",3
Hierarchical Conditional Relation Networks for Video Question Answering,CVPR_2020,5,"Muhammad Iqbal Hasan Chowdhury, Kien Nguyen, Sridha Sridharan, and Clinton Fookes. Hierarchical relational at- tention for video question answering. In 2018 25th IEEE International Conference on Image Processing (ICIP) , pages 599–603. IEEE, 2018. 4",4
Improving Action Segmentation via Graph Based Temporal Reasoning,CVPR_2020,1,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin- ton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. 4",3
Improving Action Segmentation via Graph Based Temporal Reasoning,CVPR_2020,2,"Mohit Bajaj, Lanjun Wang, and Leonid Sigal. G3raphground: Graph-based language grounding. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) , 2019. 2",3
Improving Action Segmentation via Graph Based Temporal Reasoning,CVPR_2020,3,"Fabien Baradel, Natalia Neverova, Christian Wolf, Julien Mille, and Greg Mori. Object level visual reasoning in videos. In Proceedings of the European Conference on Com- puter Vision (ECCV) , 2018. 6",5
Improving Action Segmentation via Graph Based Temporal Reasoning,CVPR_2020,4,"Subhabrata Bhattacharya, Mahdi M Kalayeh, Rahul Suk- thankar, and Mubarak Shah. Recognition of complex events: Exploiting temporal dynamics between underlying concepts. Inroceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2014. 2",4
Improving Action Segmentation via Graph Based Temporal Reasoning,CVPR_2020,5,"Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2015. 5",4
Video Modeling with Correlation Networks,CVPR_2020,1,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR , pages 248–255. IEEE, 2009. 1,5",6
Video Modeling with Correlation Networks,CVPR_2020,2,"Zhaofan Qiu, Ting Yao, , and Tao Mei. Learning spatio- temporal representation with pseudo-3d residual networks. InICCV , 2017. 1,2,4,8",4
Video Modeling with Correlation Networks,CVPR_2020,3,"Anurag Ranjan and Michael J Black. Optical ﬂow estimation using a spatial pyramid network. In CVPR , volume 2, page 2. IEEE, 2017. 3",1
Video Modeling with Correlation Networks,CVPR_2020,4,"Moez Baccouche, Franck Mamalet, Christian Wolf, Christophe Garcia, and Atilla Baskurt. Sequential deep learning for human action recognition. In International Workshop on Human Behavior Understanding , pages 29–39. Springer, 2011. 1,2",5
Video Modeling with Correlation Networks,CVPR_2020,5,"Gedas Bertasius, Christoph Feichtenhofer, Du Tran, Jianbo Shi, and Lorenzo Torresani. Learning discrimina- tive motion features through detection. arXiv preprint arXiv:1812.04172 , 2018. 7",5
Hierarchical Color Learning in Convolutional Neural Networks,CVPR_2020,1,"Brent Berlin and Paul Kay. Basic color terms: Their univer- sality and evolution . 1969. 1,2",1
Hierarchical Color Learning in Convolutional Neural Networks,CVPR_2020,2,"Paul Kay, Brent Berlin, Luisa Mafﬁ, William R Merriﬁeld, and Richard Cook. The world color survey . CSLI Publica- tions Stanford, CA:, 2009. 1[3] Kenneth Kelly and Deane Judd. The ISCC-NBS method of designating colors and a dictionary of color names . Number 553. US Government Printing Ofﬁce, 1955. 2",5
Hierarchical Color Learning in Convolutional Neural Networks,CVPR_2020,3,"Vittorio Loreto, Animesh Mukherjee, and Francesca Tria. On the origin of the hierarchy of color names. Proceedings of the National Academy of Sciences , 109(18):6819–6824, 2012. 1",3
Hierarchical Color Learning in Convolutional Neural Networks,CVPR_2020,4,"Nikolaus Kriegeskorte. Deep neural networks: a new frame- work for modeling biological vision and brain information processing. Annual review of vision science , 1:417–446, 2015. 1",2
Hierarchical Color Learning in Convolutional Neural Networks,CVPR_2020,5,"Reza Fuad Rachmadi and I Purnama. Vehicle color recog- nition using convolutional neural network. arXiv preprint arXiv:1510.07391 , 2015. 2",1
Don’t Hit Me! Glass Detection in Real-world Scenes,CVPR_2020,1,"Henghui Ding, Xudong Jiang, Bing Shuai, Ai Qun Liu, and Gang Wang. Context contrasted feature and gated multi- scale aggregation for scene segmentation. In CVPR , 2018.",5
Don’t Hit Me! Glass Detection in Real-world Scenes,CVPR_2020,2,"Jonathan Long, Evan Shelhamer, and Trevor Darrell. Ful- ly convolutional networks for semantic segmentation. In CVPR , 2015.",3
Don’t Hit Me! Glass Detection in Real-world Scenes,CVPR_2020,3,"Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So Kweon. Cbam: Convolutional block attention module. InECCV , 2018.",4
Don’t Hit Me! Glass Detection in Real-world Scenes,CVPR_2020,4,"Saining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In CVPR , 2017.",5
Don’t Hit Me! Glass Detection in Real-world Scenes,CVPR_2020,5,"Hang Zhang, Kristin Dana, Jianping Shi, Zhongyue Zhang, Xiaogang Wang, Ambrish Tyagi, and Amit Agrawal. Con- text encoding for semantic segmentation. In CVPR , 2018.",7
Controllable Person Image Synthesis with Attribute-Decomposed GAN,CVPR_2020,1,"Kﬁr Aberman, Rundi Wu, Dani Lischinski, Baoquan Chen, and Daniel Cohen-Or. Learning character-agnostic motion for motion retargeting in 2d. arXiv preprint arXiv:1905.01680 , 2019. 2",5
Controllable Person Image Synthesis with Attribute-Decomposed GAN,CVPR_2020,2,"Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern anal- ysis and machine intelligence , 39(12):2481–2495, 2017. 6",3
Controllable Person Image Synthesis with Attribute-Decomposed GAN,CVPR_2020,3,"Guha Balakrishnan, Amy Zhao, Adrian V Dalca, Fredo Du- rand, and John Guttag. Synthesizing images of humans in unseen poses. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8340– 8348, 2018. 2",5
Controllable Person Image Synthesis with Attribute-Decomposed GAN,CVPR_2020,4,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural image synthesis. arXiv preprint arXiv:1809.11096 , 2018. 2",3
Controllable Person Image Synthesis with Attribute-Decomposed GAN,CVPR_2020,5,"Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part afﬁnity ﬁelds. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 7291–7299, 2017. 3",4
LIDIA: Lightweight Learned Image Denoising with Instance Adaptation,CVPR_2020,1,"Antoni Buades, Bartomeu Coll, and J-M Morel. A non-local algorithm for image denoising. In 2005 IEEE Computer So- ciety Conference on Computer Vision and Pattern Recogni- tion (CVPR’05) , volume 2, pages 60–65. IEEE, 2005. 1",3
LIDIA: Lightweight Learned Image Denoising with Instance Adaptation,CVPR_2020,2,"Joshua Batson and Loic Royer. Noise2self: Blind denoising by self-supervision. arXiv preprint arXiv:1901.11365 , 2019. 4",1
LIDIA: Lightweight Learned Image Denoising with Instance Adaptation,CVPR_2020,3,"Stamatios Lefkimmiatis. Universal denoising networks: a novel cnn architecture for image denoising. In Proceed- ings of the IEEE conference on computer vision and pattern recognition , pages 3204–3213, 2018. 2,5,7",2
LIDIA: Lightweight Learned Image Denoising with Instance Adaptation,CVPR_2020,4,"Jaakko Lehtinen, Jacob Munkberg, Jon Hasselgren, Samuli Laine, Tero Karras, Miika Aittala, and Timo Aila. Noise2noise: Learning image restoration without clean data. arXiv preprint arXiv:1803.04189 , 2018. 4",7
LIDIA: Lightweight Learned Image Denoising with Instance Adaptation,CVPR_2020,5,"Kostadin Dabov, Alessandro Foi, Vladimir Katkovnik, and Karen Egiazarian. Image denoising by sparse 3-d transform- domain collaborative ﬁltering. IEEE Transactions on image processing , 16(8):2080–2095, 2007. 1,3,5,6",4
Solving Mixed-modal Jigsaw Puzzle for,CVPR_2020,1,"Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre- sentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748 , 2018. 5",3
Solving Mixed-modal Jigsaw Puzzle for,CVPR_2020,2,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR , 2015. 4,5",9
Solving Mixed-modal Jigsaw Puzzle for,CVPR_2020,3,"Jia Deng, Wei Dong, R Socher, and Li Jia Li. Imagenet: A large-scale hierarchical image database. In CVPR , 2009. 1",4
Solving Mixed-modal Jigsaw Puzzle for,CVPR_2020,4,"Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV , 2016. 2,4,5",1
Solving Mixed-modal Jigsaw Puzzle for,CVPR_2020,5,"Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Un- supervised representation learning by predicting image rota- tions. arXiv preprint arXiv:1803.07728 , 2018. 4,5",3
Context-Aware Group Captioning via Self-Attention and Contrastive Features,CVPR_2020,1,"Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with hu- man judgments. In Proceedings of the acl workshop on in- trinsic and extrinsic evaluation measures for machine trans- lation and/or summarization , pages 65–72, 2005. 6",1
Context-Aware Group Captioning via Self-Attention and Contrastive Features,CVPR_2020,2,"Mostafa Dehghani, Sascha Rothe, Enrique Alfonseca, and Pascal Fleury. Learning to attend, copy, and generate for session-based query suggestion. In Proceedings of the 2017 ACM on Conference on Information and Knowledge Man- agement , pages 1747–1756. ACM, 2017. 2,3",4
Context-Aware Group Captioning via Self-Attention and Contrastive Features,CVPR_2020,3,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. 5",6
Context-Aware Group Captioning via Self-Attention and Contrastive Features,CVPR_2020,4,"Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation , 9(8):1735–1780, 1997. 2,5",1
Context-Aware Group Captioning via Self-Attention and Contrastive Features,CVPR_2020,5,"Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 6077–6086, 2018. 1,2",7
Searching Central Difference Convolutional Networks for Face Anti-Spooﬁng,CVPR_2020,1,"Timo Ahonen, Abdenour Hadid, and Matti Pietikainen. Face description with local binary patterns: Application to face recognition. IEEE Transactions on Pattern Analysis & Ma- chine Intelligence , (12):2037–2041, 2006. 2",3
Searching Central Difference Convolutional Networks for Face Anti-Spooﬁng,CVPR_2020,2,"Zinelabidine Boulkenafet, Jukka Komulainen, and Abdenour Hadid. Face anti-spooﬁng based on color texture analysis. In IEEE international conference on image processing (ICIP) , pages 2636–2640, 2015. 1,2,3,8",3
Searching Central Difference Convolutional Networks for Face Anti-Spooﬁng,CVPR_2020,3,"Zinelabidine Boulkenafet, Jukka Komulainen, and Abdenour Hadid. Face spooﬁng detection using colour texture analysis. IEEE Transactions on Information Forensics and Security , 11(8):1818–1830, 2016. 1,8",3
Searching Central Difference Convolutional Networks for Face Anti-Spooﬁng,CVPR_2020,4,"Zinelabidine Boulkenafet, Jukka Komulainen, and Abdenour Hadid. Face antispooﬁng using speeded-up robust features and ﬁsher vector encoding. IEEE Signal Processing Letters , 24(2):141–145, 2017. 2",3
Searching Central Difference Convolutional Networks for Face Anti-Spooﬁng,CVPR_2020,5,"Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Smash: one-shot model architecture search through hypernetworks. arXiv preprint arXiv:1708.05344 , 2017. 2",4
Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks,CVPR_2020,1,"Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communica- tions Security , pages 308–318. ACM, 2016. 1",7
Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks,CVPR_2020,2,"Alessandro Achille, Matteo Rovere, and Stefano Soatto. Critical learning periods in deep neural networks. In Inter- national Conference of Learning Representations , 2019. 8",3
Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks,CVPR_2020,3,"Alessandro Achille and Stefano Soatto. Where is the Infor- mation in a Deep Neural Network? arXiv e-prints , page arXiv:1905.12213, May 2019. 2,8",1
Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks,CVPR_2020,4,"Devansh Arpit, Stanisław Jastrz˛ ebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. InProceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 233–242. JMLR. org, 2017. 1",11
Eternal Sunshine of the Spotless Net: Selective Forgetting in Deep Networks,CVPR_2020,5,"Lucas Bourtoule, Varun Chandrasekaran, Christopher Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang, David Lie, and Nicolas Papernot. Machine unlearning. arXiv preprint arXiv:1912.03817 , 2019. 2",8
Large Scale Video Representation Learning via Relational Graph Clustering,CVPR_2020,1,"Unaiza Ahsan, Rishi Madhok, and Irfan Essa. Video jigsaw: Unsupervised learning of spatiotemporal context for video action recognition. In WACV , 2019. 2",3
Large Scale Video Representation Learning via Relational Graph Clustering,CVPR_2020,2,"Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proc. of the IEEE International Conference on Computer Vi- sion (ICCV) , 2017. 2",5
Large Scale Video Representation Learning via Relational Graph Clustering,CVPR_2020,3,"Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Nat- sev, George Toderici, Balakrishnan Varadarajan, and Sud- heendra Vijayanarasimhan. YouTube-8M: A large-scale video classiﬁcation benchmark. arXiv:1609.08675 , 2016. 7, 8",7
Large Scale Video Representation Learning via Relational Graph Clustering,CVPR_2020,4,"MohammadHossein Bateni, Soheil Behnezhad, Mahsa Der- akhshan, MohammadTaghi Hajiaghayi, Raimondas Kiveris, Silvio Lattanzi, and Vahab Mirrokni. Afﬁnity clustering: Hi- erarchical clustering at scale. In Advances in Neural Infor- mation Processing Systems (NIPS) , 2017. 3",7
Large Scale Video Representation Learning via Relational Graph Clustering,CVPR_2020,5,"Qiong Cao, Yiming Ying, and Peng Li. Similarity metric learning for face recognition. In Proc. of the IEEE Interna- tional Conference on Computer Vision (ICCV) , 2013. 2",3
VecRoad: Point-based Iterative Graph Exploration for Road Graphs Extraction,CVPR_2020,1,"D. H. Douglas and T. K. Peucker. Algorithms for the reduc- tion of the number of points required to represent a digitized line or its caricature. Cartographica: Int. J. Geog. Inform. Geovisualization , 10(2):112–122, 1973. 1,2",2
VecRoad: Point-based Iterative Graph Exploration for Road Graphs Extraction,CVPR_2020,2,"A. Van Etten, D. Lindenbaum, and T. M. Bacastow. Spacenet: A remote sensing dataset and challenge series. arXiv preprint arXiv:1807.01232 , 2018. 2,6",2
VecRoad: Point-based Iterative Graph Exploration for Road Graphs Extraction,CVPR_2020,3,"G. Cheng, Y . Wang, S. Xu, H. Wang, S. Xiang, and C. Pan. Automatic road detection and centerline extraction via cas- caded end-to-end convolutional neural network. IEEE Trans. Geosci. Rem. S. , 55(6):3322–3337, 2017. 2,4",2
VecRoad: Point-based Iterative Graph Exploration for Road Graphs Extraction,CVPR_2020,4,"L. Han, P. Tao, and R. R. Martin. Livestock detection in aerial images using a fully convolutional network. Compu- tational Visual Media , 5(2):221–228, 2019. 8",2
VecRoad: Point-based Iterative Graph Exploration for Road Graphs Extraction,CVPR_2020,5,"F. Bastani, S. He, S. Abbar, M. Alizadeh, H. Balakrishnan, S. Chawla, S. Madden, and D. DeWitt. Roadtracer: Auto- matic extraction of road networks from aerial images. In CVPR , 2018. 1,3,4,5,6,7",2
Reposing Humans by Warping 3D Features,CVPR_2020,1,"Artur Grigorev, Artem Sevastopolsky, Alexander Vakhitov, and Victor Lempitsky. Coordinate-based texture inpainting for pose-guided human image generation. In CVPR , 2019.",4
Reposing Humans by Warping 3D Features,CVPR_2020,2,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV , 2016.",4
Reposing Humans by Warping 3D Features,CVPR_2020,3,"Yusuke Horiuchi, Satoshi Iizuka, Edgar Simo-Serra, and Hi- roshi Ishikawa. Spectral normalization and relativistic ad- versarial training for conditional pose generation with self- attention. In MVA , 2019.",4
Reposing Humans by Warping 3D Features,CVPR_2020,4,"Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for ir- regular holes using partial convolutions. In ECCV , 2018.",6
Reposing Humans by Warping 3D Features,CVPR_2020,5,"Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhofer. Deep- V oxels: Learning persistent 3d feature embeddings. In CVPR , 2019.",6
MMDM: Multi-frame and Multi-scale for Image Demoir ´eing,CVPR_2020,1,Sergey Ioffe and Christian Szegedy. Batch normalization: accelerating deep network training by reducing internal co- variate shift. 2015. 4,1
MMDM: Multi-frame and Multi-scale for Image Demoir ´eing,CVPR_2020,2,"Songhyun Yu, Bumjun Park, and Jechang Jeong. Deep itera- tive down-up cnn for image denoising. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion Workshops , 2019. 4",3
MMDM: Multi-frame and Multi-scale for Image Demoir ´eing,CVPR_2020,3,"Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong Han. Acnet: Strengthening the kernel skeletons for power- ful cnn via asymmetric convolution blocks. In Proceedings of the IEEE International Conference on Computer Vision , pages 1911–1920, 2019. 2,4",4
MMDM: Multi-frame and Multi-scale for Image Demoir ´eing,CVPR_2020,4,"Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural infor- mation processing systems , pages 2017–2025, 2015. 2,3",4
MMDM: Multi-frame and Multi-scale for Image Demoir ´eing,CVPR_2020,5,"Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. 2018. 2,3,4,5",6
Abstract,CVPR_2020,1,". Our goal is to reduce energy and memory consumption of neural networks so they can be deployed on devices with limited resources while preserving accuracy. To achieve this goal, we devised Squeeze U-Net for image segmentation. The design of this architecture is inspired by SqueezeNet [2] and U-Net [3] . We choose the U-Net architecture [3] as a starting point because it can be successfully trained on small data sets which is beneficial for hardware with limited memory and for applications for which large training data sets may not be available. We replace the down and up sampling layers in U-Net with modules similar to the fire modules in SqueezeNet [2]. Our fire modules use point-wise convolutions followed by an inception stage [4] in which pointwise and 3 ×3 convolutions are performed independently then concatenated to form the output. It results in a small model with only 2.6 million parameters. The total number of parameters in our Squeeze U-Net is 1.68 × , 2.59 ×, 11.58 ×, 3.65×, , 16.84 ×, 27.4 × smaller than Mobile Net [5], Deep Lab [6], U-Net [3] , SegNet [7], FCN [8], DeconvNet [9] architectures. To analyze the merit of the Squeeze U-Net architecture, we have implemented both it and U-Net using Tensorflow 1.14 and Python 3.6 with CUDA 10.1.243, and measured execution time of every layer on an NVIDIA K40 GPU. We show that Squeeze U- Net for the contracting path requires 63% of the time for U-Net, for the expanding path 75% of the U-Net time and for the two together 69% of the U-Net time on the CamVid data set. For inference, Squeeze U-Net is 17% faster than U-Net. Next, we describe related work followed by a detailed description of the Squeeze U- Net architecture in Section 3. Section 4 discusses training using the Squeeze U-Net architecture followed by an evaluation in Section 5 and our conclusions in Section 6. 2. Related Work It has become evident that Deep Neural Networks typically are over parametrized in that a variety of compression techniques have been applied on large parameter spaces with no or only minor loss of accuracy. Redundancy in deep learning models results in waste of computation, memory and energy. Shrinking, factorization or compressing pretrained networks are approaches for removing redundancy and obtaining smaller models [11]–",2
Abstract,CVPR_2020,2,. One of the straightforward approaches in model compression is applying singular value decomposition (SVD) to a pretrained CNN model and finding low rank approximations of the parameters [15]. Other approaches are network pruning which takes a pretrained model and replaces parameters which are below a certain threshold with zeros to form sparse matrices. In sparse matrices relative encoding of indices can be used to compress indices to a few bits at the expense of indirection. To reduce the number of parameters and computational effort for CNNs several techniques based on factorizing the convolution kernel has been used. Depthwise separable convolution,2
Abstract,CVPR_2020,3,"separates convolution across channels from Squeeze U-Net: A Memory and Energy Effici ent Image Segmentation Network Nazanin Beheshti Department of Co mputer Science University of Houston Nsbeheshti70@gmail.com Lennart Johnsson Department of Co mputer Science University of Houston johnsson@cs.uh.edu convolution within channels. Depthwise separable convolution is used in Squeez eNet and in the Squeeze U- Net and in e.g. [17][18]. Potential further reduction in model size by reducing the data type size from 32-bits to eight or 16 bits, commonly known as quantization, may be able to reduce the Squeeze U-Net model, but is not investigated here. Quantization in depthwise separable convolution networks using non-linear activation functions within layers may require special attention as shown in [19] for MobileNetV1. Other approaches towards reducing the computation time have focused on specialized hardware for CNNs, such as e.g. [20]–[22]. [22] focus on data reuse for dense uncompressed models thereby making the hardware architecture energy efficient. Contributions : For image segmentation we show that Squeeze U-Net achieves the same accuracy as U-Net on the CamVid dataset [23] with 2.6 million parameters, a 12 ൈ reduction compared to U-Net [3]. In designing Squeeze U- Net we employ the SqueezeNet fire module [2] design in both the U-Net contracting and expansive paths. The fire modules initial depthwise convolution reduce the number of channels and compensates this reduction by an inception stage with two parallel convolutions each having half the number of output channels of the fire module’s output channels. The two parallel convolutions help prevent feature loss and vanishing gradients which may be caused Table 1: The number of parameters for a K ൈK size convolution kernel, C i input channels and C o output channels are a KൈKൈC ୧ൈC ୓. and is given below for a few CNNs. The networks typically use 3 ൈ3 kernels or a combination of 3ൈ3 and 1 ൈ1 kernels. Model Architecture #Params Model Size (MB) Kernel Size Squeeze U-Net 2.59M 32 3ൈ3 2ൈ2 1ൈ1 U-Net 30M 386 3ൈ3 2ൈ2 1ൈ1 SegNet 30M 117 3ൈ3 Deep Lab 20M 83 3ൈ3 – 1 ൈ1 FCN -8s 132M 539 3ൈ3–1 ൈ1– 4ൈ4 7ൈ7–16 ൈ16 DeconvNet 143M 877 1ൈ1 – 3 ൈ3 7ൈ7 Figure 1: (A) shows convolution in the contractin g path in U-Net (B) shows our corres pondin g Squeeze U-Net im plementation usin g fire modules [2] instead of full convolution to reduce the reduce the number of parameters . (C) shows transposed convolution in the expansive path in U-Net (D) shows our Squeeze U-Net implementation corresponding to (C) . In (D) and (B), the fire modu les first squeeze the number of output channels then apply two parallel convolutions with different kernel size to capt ure missing features from the previous layer and concatenate their out puts. by reducing the number of channels [24]. Furthermore, we show that, although the Squeeze U-Net has more layers than U-Net, the inference time for Squeeze U- Net implemented in Tensorflow 1.14 using Python 3.6 and CUDA 10.1.243 on an NVIDIA K40 GPU is 17 % faster than U-Net for the CamVid data set and only requires 66% of the U-Net training time 3. Architecture 3.1 Contracting Path Inspired by SqueezeNet [2], we adopt fire modules for the down sampling (DS) units in the contracting path of Squeeze U-Net. Each fire module in the contracting path consists of one 1 ×1 convolution layer with C୓ᇱ output channels, C୭ᇱ<C ୧ and an inception layer with two parallel convolutions with 3×3 and 1 ×1 kernel size and C୭/2 output channels each. Concatenation of the parallel convolution output channels form the fire module output. It is passed to the next contracting layer and also to the corresponding layer in the expansive path of Squeeze U-Net with long skip connections[25], [26]. For down sampling we use stride 2 convolutions instead of max or average pooling. Striding increase the expressiveness of our network [27]. 3.2 Expansive path For the expansive path in the Squeeze U-Net we also use the SqueezeNet fire modules to reduce the total number of parameters. Table 2: Quantitate comparison of Squeeze U-Net and U-Net regarding model size, number of convolution and multiplication operations. These numbers are obtained using the TensorFlow 1.14 profiler on a saved model. The number of convolutions and multiplication are gathered during inference time. Model Size (MB) #CONV (CamVid) Billion #Mult (CamVid) Million Squeeze U-Net 32 432.71 33.03 U-Net 386.6 1315.60 61.44 Factor of reduction 12.08× 3.04× 1.86× Figure 2: The Squeeze U-Net architecture consists of down sampling units in the contracting U-Net path, and up sampling units i n the expansive U-Net path. Every down sampling (DS) unit consists of two fire modules which extract features. The extracted features are passed down to the next down sampling unit and the corresponding up sampling unit (US) . Every up-sampling unit consist of a tra nsposed fire module, a concatenation unit and two fire modules which in order up samples their input, extrac t features, and concatenate features to construct the output. The main component of the expansive path in Squeeze U-Net is up sampling units (US). In every up-sampling unit, the transposed fire module consists of a 1 ൈ1 transposed convolution with C୓ᇱ output channels , C୭ᇱ൏C ୧ as in the DS fire modules. For the inception stage of the transposed f ire module, the output from the 1 ൈ1 transposed convolution is fed into two parallel transposed convolutions with 2 ൈ2 and 1 ൈ1 kernel size, each with C୭/2 output channels that are concatenated to form the transpose fire module output, as for the DS units. Further, up sampling units also have a stage for concatenating the bypass connections from the corresponding down sampling unit with the output of the transposed fire module thereby merging higher-resolution features from fire modules in the contracting path with lower resolution features in the expansive path. The concatenating unit is followed by two successive fire modules. As shown in Figure 2 and Table 4, there are three up sampling units in the expansive path followed by one 2 ൈ2 transposed convolution. The features from its corresponding layer, the 3 ൈ3 convolution layer before the contracting path, is concatenated with features from the 2ൈ2 transposed convolution and passed to two 3 ൈ3 convolution layers and one 1 ൈ1 convolution layer to generate a HൈWൈclasses tensor for pixel wise segmentation. 4.Training To evaluate Squeeze U-Net, we use the CamVid road scenes dataset [23]. This dataset is small, consisting of 701 Table 3: Comparison of the number of Squeeze U-Net a nd U-Net parameters and MACs in the contracting pat h. Squeeze U-Net achieve s a 12.18 ൈ reduction in the number of parameters and a 3.7 ൈ reduction in MACs for the contracting path. Layer Name Squeeze U-Net U-Net Feature Size (× HW) Reduction Factor (#Params)Reduction Factor (#MACs)Layer #Params #MACs (× HW)Layer #Params#MACs (× HW) Convolutio n ൣ3 ൈ 3 ൈ 64 ൧ ൈ2 38592 77184 ൣ3ൈ3ൈ6 4 ൧ൈ2 38592 77184 64 1ൈ 1ൈ DS1 ൦1 ൈ 1 ൈ 32 3 ൈ 3 ൈ 64 1 ൈ 1 ൈ 64൪ ൈ2 47104 2 3552 ൣ3ൈ3 ൈ 1 2 8 ൧ൈ2 221184 1 10592 128 2ൈ2 4.69 ൈ 4.69 ൈ DS 2 ൦1 ൈ 1 ൈ 48 3 ൈ 3 ൈ 128 1 ൈ 1 ൈ 128൪ൈ2 141312 17664 ൣ3ൈ3 ൈ 2 5 6 ൧ൈ2 884736 110592 256 4ൈ4 6.2ൈ 6.2ൈ DS 3 ൦1 ൈ 1 ൈ 64 3 ൈ 3 ൈ 256 1 ൈ 1 ൈ 256൪ ൈ2 376832 1 1 7 7 6 ൣ3ൈ3 ൈ 5 1 2 ൧ൈ2 3538944 1 1 0 5 9 2 512 8ൈ8 9.39 ൈ 9.39 ൈ DS 4 ൦1 ൈ 1 ൈ 80 3 ൈ 3 ൈ 512 1 ൈ 1 ൈ 512൪ ൈ2 942080 7 360 ሾ3ൈ3ൈ1 0 2 4 ሿൈ214155776 1 1 0 5 9 2 1024 16 ൈ 16 15.2 ൈ 15.2 ൈ Total 1545920 137456 18,839,232 5 1 9 5 5 2 12.18 ൈ 3.7 ൈ Figure 3: Training loss and accuracy per epoch for S queeze U-Ne t and U Net. (a) shows loss per epoch. (b) shows accuracy per epo ch We train Squeeze U-Net until it converges to the sam e accuracy as U - Net. parallel. That would reduce the execution time for the contracting and expanding path of Squeeze U-Net to 55% of U-Net. 5.3 Inference Accuracy We assessed the quality of the Squeeze U-Net model relative to the U-Net model on a set of 120 CamVid images not part of the training set. For the assessment the 120 images were divided into 15 batches of eight images each and for each batch the average of true positive, false positive, false negative pixel classification was recorded as well as the average pixel count for each class in each batch. In the evaluation set of 120 images segmentation and classification was successful for Squeeze U-Net and U-Net for the five classes labeled building, tree sky, car and road. Squeeze U-Net in addition successfully segmented and classified the class sidewalk. The results for the five common classes are shown in Table 7. For true positive pixels the average accuracy for the building, tree, sky, car, and road classes, for U-Net was 86.9% vs Squeeze U-Net’s 78%. For the road and sky classes Squeeze U-net is 2 – 3 % less accurate than U-Net. Sky represent on average about 14% of the pixels and the road about 25% of the pixels in the test set. For the building class Squeeze U-Net is on average about 5% less accurate for the test set than U-Net (78.5% vs 83.3%). The building class represent about 41% of the pixels in the test set. For the tree class with on average 6% of the pixels in the test set Squeeze U-net is about 20% less accurate than U-Net (51.1 % vs 72.6%) and for the car class having about 1% of the pixels Squeeze U-Net is about 13% less accurate than U-Net (71.1% vs 84.5%). Squeeze U-net appears more sensitive to the number of pixels representing the class than U-Net, and to also have difficulties with less well-defined structures like trees. The range of true positive pixel classification accuracy across images in the evaluation set is generally higher for Squeeze U-Net than U- Net for each class. The Min and Max values in Table 7 are based on averages for batches of eight images due to our implementation of the test cases, and hence do not cover extreme cases. For false positives Squeeze U-Net tend to have more false positive pixels relative to the true pixels. For the sky class Squeeze U-net only has on average 1.5 % more false positives, but for the road class it has about 8% (31.3% vs 23.4%) more false positives. For the building class Squeeze U-Net however has about 17% le ss false positives than U-Net (22.1% vs 39.3%) and for the tree class Squeeze U-Net also has less false positives though still high (106% vs 130%). For the car class both Squeeze U-Net and U-Net had a very large number of false positives with Squeeze U-Net performing worse than U-Net. As for true positives the variability in the number of false positives across images Table 7: Comparison of Squeeze U-Net (1) and U-Net (2) regardi ng intersection over union (IoU) and false positive to true posit ive for test set Building Tree Sky Car Road Average (1) (2) (1) (2) (1) (2) (1) (2) (1) (2) (1) (2) Percent True positive pixels Average 78.5% 83.3% 51.1% 72.6% 93.7% 96.9% 71.1% 84.5% 95.6% 97.4% 78.0% 86.9% Max 86.3% 90.1% 73.8% 92.4% 99.4% 99.4% 99.3% 97.8% 98.2% 99.2% 91.4% 95.8% Min 58.8% 73.3% 18.0% 51.7% 76.4% 91.1% 46.0% 49.0% 90.0% 93.0% 57.8% 71.6% Intersection over Union (IoU) Average 0.689 0.639 0.370 0.403 0.842 0.879 0.427 0.628 0.751 0.800 0.616 0.670 Max 0.808 0.734 0.575 0.650 0.928 0.945 0.667 0.812 0.913 0.937 0.778 0.816 Min 0.395 0.443 0.113 0.220 0.676 0.753 0.011 0.020 0.575 0.686 0.354 0.424 False Positive pixels relative to true positive pixels Average 22.1% 39.3% 106.3% 130.1% 12.5% 11.0% 688.2% 354.1% 31.3% 23.4% 172.1% 111.6% Max 83.1% 89.4% 331.0% 280.4% 24.8% 23.1% 8870.8% 4752.1% 62.8% 39.4% 1874.5% 10 36.9% Min 6.0% 23.4% 18.6% 20.1% 6.3% 4.5% 26.2% 10.7% 7.7% 5.6% 13.0% 12.8% Figure 5. The execution times of fire modules in ou r Tensorflow implementation. By parallelizing the execution of the 2nd and third convolution we estimate that Squeeze U- Net should be able to achieve a reduction of about 20%. tend to be higher for Squeeze U-Net than U-Net. For the Intersection over Union measure (IoU) (the ratio of true positive pixels to true pixels plus false positives) on average the Squeeze U-Net measure is le ss than about 3 - 5%, lower than the U-Net measure for the tree, sky and road classes. For the building class the Sq ueeze U-Net measure is about 5% higher than that of U-Net. For the car class the Squeeze U-Net measure is considerably lower than that for U-Net. The variability of the IoU measure across the test images is higher than that for U-Net. Figure 6 shows the classification generated by Squeeze U-Net and U-Net for four CamVid images . 6. Conclusion In this work we presen ted Squeeze U-Net for image segmentation that has 12× fewer parameters, and 3× fewer MACs. Squeeze U-Net training time for the CamVid data set is 69% of that of U-Net and for inference the Squeeze U-Net architecture is 17% faster. We also estimate, that by using parallel convolutions, training and inference times of Squeeze U-Net may achieve 2× reduction in execution time compared to U-Net. Squeeze U-Net use fire modules [2] and transposed fire modules in the contracting and expansive paths of U-Net to reduce model size and generate a memory and power efficient segmentation model. The 12× reduction in memory requirements should result in a significant reduction in energy requirement compared to U- Net and the 3× reduction in MACs similarly should reduce energy dissipation for computation. Energy measurements is part of our future work as is a concurrent implementation of the inception stage in the fire modules. A further understanding of the Squeeze U-Net accuracy and sensitivity to class characteristics and training needs are also part of future work. Acknowledgement The University of Houston Data Science Institute’s Research Computing Center made available NVIDIA K40 GPU nodes free of charge which is gratefully acknowledged. The many helpful discussions with members of the Advanced Computing Research Lab were also very helpful in achieving our results. Especially Suyash Bakshi’s insights on GPU programming and tools were very helpful. Test Image Ground truth Squeeze U-Net U-Net Figure 6: Qualitative assessmentofSqueezeU-NetandU-Netsegmentationonthe CamVid road scenesdataset References",1
Abstract,CVPR_2020,4,"A. Pedram, S. Richardson, M. Horowitz, S. Galal, and S. Kvatinsky, “Dark memory and accelerator-rich system optimization in the dark silicon era,” IEEE Des. Test , vol. 34, no. 2, pp. 39–50, 2016.",2
Abstract,CVPR_2020,5,"T. Sheng, C. Feng, S. Zhuo, X. Zhang, L. Shen, and M. Aleksic, “A quantization- friendly separable convolution for mobilenets,” in 2018 1st Workshop on Energy Efficient Machine Learning and Cognitive Computing for Embedded Applications (EMC2) , 2018, pp. 14–18.",2
Disentangling Physical Dynamics from Unknown Factors for Unsupervised,CVPR_2020,1,"C. Corbière, N. Thome, A. Bar-Hen, M. Cord, and P. Pérez. Addressing failure prediction by learning model conﬁdence. InAdvances in Neural Information Processing Systems (NeurIPS) , pages 2902–2913, 2019. 8",2
Disentangling Physical Dynamics from Unknown Factors for Unsupervised,CVPR_2020,2,"S. A. Eslami, N. Heess, T. Weber, Y . Tassa, D. Szepesvari, G. E. Hinton, et al. Attend, infer, repeat: Fast scene under- standing with generative models. In Advances in Neural In- formation Processing Systems (NeurIPS) , pages 3225–3233, 2016. 2[15] R. Fablet, S. Ouala, and C. Herzet. Bilinear residual neural network for the identiﬁcation and forecasting of geophysical dynamics. In 2018 26th European Signal Processing Con- ference (EUSIPCO) , pages 1477–1481. IEEE, 2018. 2,8",2
Disentangling Physical Dynamics from Unknown Factors for Unsupervised,CVPR_2020,3,"M. Asch, M. Bocquet, and M. Nodet. Data assimilation: methods, algorithms, and applications , volume 11. SIAM, 2016. 2,4",2
Disentangling Physical Dynamics from Unknown Factors for Unsupervised,CVPR_2020,4,"P. Battaglia, R. Pascanu, M. Lai, D. J. Rezende, et al. In- teraction networks for learning about objects, relations and physics. In Advances in neural information processing sys- tems (NeurIPS) , pages 4502–4510, 2016. 2",2
Disentangling Physical Dynamics from Unknown Factors for Unsupervised,CVPR_2020,5,"P. Becker, H. Pandya, G. Gebhardt, C. Zhao, C. J. Taylor, and G. Neumann. Recurrent Kalman networks: Factorized inference in high-dimensional deep feature spaces. In In- ternational Conference on Machine Learning (ICML) , pages 544–552, 2019. 2,5",2
Evaluating Scalable Bayesian Deep Learning,CVPR_2020,1,"Peter Auer, Mark Herbster, and Manfred K Warmuth. Expo- nentially many local minima for single neurons. In Advances in Neural Information Processing Systems (NeurIPS) , pages 316–322, 1996. 5",3
Evaluating Scalable Bayesian Deep Learning,CVPR_2020,2,"David Barber and Christopher M Bishop. Ensemble learning in Bayesian neural networks. Nato ASI Series F Computer and Systems Sciences , 168:215–238, 1998. 5",1
Evaluating Scalable Bayesian Deep Learning,CVPR_2020,3,"Eli Bingham, Jonathan P. Chen, Martin Jankowiak, Fritz Obermeyer, Neeraj Pradhan, Theofanis Karaletsos, Rohit Singh, Paul Szerlip, Paul Horsfall, and Noah D. Goodman. Pyro: Deep Universal Probabilistic Programming. Journal of Machine Learning Research , 2018. 11",2
Evaluating Scalable Bayesian Deep Learning,CVPR_2020,4,"Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty in neural network. InInternational Conference on Machine Learning (ICML) , pages 1613–1622, 2015. 2,5",4
Evaluating Scalable Bayesian Deep Learning,CVPR_2020,5,"Jochen Br ¨ocker. Reliability, sufﬁciency, and the decomposi- tion of proper scores. Quarterly Journal of the Royal Mete- orological Society , 135(643):1512–1519, 2009. 7",2
SampleNet: Differentiable Point Cloud Sampling,CVPR_2020,1,"Ehsan Nezhadarya, Taghavi Ehsan, Bingbing Liu, and Jun Luo. Adaptive Hierarchical Down-Sampling for Point Cloud Classiﬁcation. arXiv preprint arXiv:1904.08506 , 2019. 3",4
SampleNet: Differentiable Point Cloud Sampling,CVPR_2020,2,"Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas J. Guibas. Learning Representations and Genera- tive Models For 3D Point Clouds. Proceedings of the 35th In- ternational Conference on Machine Learning (ICML) , pages 40–49, 2018. 2,7",4
SampleNet: Differentiable Point Cloud Sampling,CVPR_2020,3,"Yasuhiro Aoki, Hunter Goforth, Rangaprasad Arun Srivat- san, and Simon Lucey. PointNetLK: Robust & Efﬁcient Point Cloud Registration using PointNet. Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) , pages 7163–7172, 2019. 2",4
SampleNet: Differentiable Point Cloud Sampling,CVPR_2020,4,"Angel X. Chang, Thomas Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. arXiv preprint arXiv:1512.03012 , 2015. 5",2
SampleNet: Differentiable Point Cloud Sampling,CVPR_2020,5,"Pengwen Chen, Yunmei Chen, and Murali Rao. Metrics De- ﬁned by Bregman Divergences. Communications in Mathe- matical Sciences , 6, 2008. 4",3
Activity-aware Attributes for Zero-Shot Driver Behavior Recognition,CVPR_2020,1,"Martin Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasserstein generative adversarial networks. In Interna- tional Conference on Machine Learning (ICML) , pages 214– 223, 2017. 3",3
Activity-aware Attributes for Zero-Shot Driver Behavior Recognition,CVPR_2020,2,"Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe Hillier, and Andrew Zisserman. A short note about kinetics- 600. arXiv preprint arXiv:1808.01340 , 2018. 2",5
Activity-aware Attributes for Zero-Shot Driver Behavior Recognition,CVPR_2020,3,"Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In Conference on Computer Vision and Pattern Recognition (CVPR) , pages 6299–6308. IEEE, 2017. 4",1
Activity-aware Attributes for Zero-Shot Driver Behavior Recognition,CVPR_2020,4,"C ´eline Craye and Fakhri Karray. Driver distraction detec- tion and recognition using rgb-d sensor. arXiv preprint arXiv:1502.00250 , 2015. 1",1
Activity-aware Attributes for Zero-Shot Driver Behavior Recognition,CVPR_2020,5,"Rafael Felix, Vijay BG Kumar, Ian Reid, and Gustavo Carneiro. Multi-modal cycle-consistent generalized zero- shot learning. In European Conference on Computer Vision (ECCV) , pages 21–37. Springer, 2018. 2,3",4
Joint Graph-based Depth Reﬁnement and Normal Estimation,CVPR_2020,1,"Heiko Hirschm ¨uller. Stereo processing by semiglobal matching and mutual information. IEEE Transac- tions on Pattern Analysis and Machine Intelligence (TPAMI) , 30(2):328–341, Feb 2008. 1,5,6,7",2
Joint Graph-based Depth Reﬁnement and Normal Estimation,CVPR_2020,2,"Moritz Menze and Andreas Geiger. Object scene ﬂow for autonomous vehicles. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Boston, MA, USA , 2015. 5,7",1
Joint Graph-based Depth Reﬁnement and Normal Estimation,CVPR_2020,3,"Jonathan T. Barron and Ben Poole. The fast bilateral solver. In European Conference on Computer Vision (ECCV), Amsterdam, The Netherlands , 2016. 1,2,5",2
Joint Graph-based Depth Reﬁnement and Normal Estimation,CVPR_2020,4,"Michael Bleyer, Christoph Rhemann, and Carsten Rother. PatchMatch stereo - Stereo matching with slanted support windows. In British Machine Vision Conference (BMVC), Dundee, UK , 2011. 1,8",3
Joint Graph-based Depth Reﬁnement and Normal Estimation,CVPR_2020,5,"Antoni Buades, Bartomeu Coll, and Jean-Michel Morel. A review of image denoising algorithms, with a new one. SIMUL , 4:490–530, 2005. 4",3
Long Short-Term Memory Deep-Filter in Remote Photoplethysmography,CVPR_2020,1,"R. Macwan, S. Bobbia, Y . Benezeth, J. Dubois, and A. Man- souri. Periodic variance maximization using generalized eigenvalue decomposition applied to remote photoplethys- mography estimation. In IEEE Conf. on Computer Vision and Pattern Recognition Workshops , 2018.",2
Long Short-Term Memory Deep-Filter in Remote Photoplethysmography,CVPR_2020,2,"Wim Verkruysse, Lars O Svaasand, and J Stuart Nelson. Re- mote plethysmographic imaging using ambient light. Optics express , 16(26):21434–21445, 2008.",3
Long Short-Term Memory Deep-Filter in Remote Photoplethysmography,CVPR_2020,3,"Z. Zhang, J.M. Girard, Y . Wu, X. Zhang, et al. Multimodal spontaneous emotion corpus for human behavior analysis. InIEEE Conf. on Computer Vision and Pattern Recognition , pages 3438–3446, 2016.",2
Long Short-Term Memory Deep-Filter in Remote Photoplethysmography,CVPR_2020,4,"F. Bousefsaf, C. Maaoui, and A. Pruski. Continuous wavelet ﬁltering on webcam photoplethysmographic signals to re- motely assess the instantaneous heart rate. Biomedical Signal Processing and Control , 8(6):568–574, 2013.",2
Long Short-Term Memory Deep-Filter in Remote Photoplethysmography,CVPR_2020,5,"F. Bousefsaf, A. Pruski, and C. Maaoui. 3d convolutional neural networks for remote pulse rate measurement and map- ping from facial video. Applied Sciences , 9(20), 2019.",2
Global Texture Enhancement for Fake Face Detection In the Wild,CVPR_2020,1,"Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 2414–2423, 2016.",3
Global Texture Enhancement for Fake Face Detection In the Wild,CVPR_2020,2,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Process- ing Systems , pages 5767–5777, 2017.",5
Global Texture Enhancement for Fake Face Detection In the Wild,CVPR_2020,3,"Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasser- stein generative adversarial networks. In International Con- ference on Machine Learning , pages 214–223, 2017.",3
Global Texture Enhancement for Fake Face Detection In the Wild,CVPR_2020,4,"David Berthelot, Thomas Schumm, and Luke Metz. Began: Boundary equilibrium generative adversarial networks. arXiv preprint arXiv:1703.10717 , 2017.",3
Global Texture Enhancement for Fake Face Detection In the Wild,CVPR_2020,5,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural image synthesis. arXiv preprint arXiv:1809.11096 , 2018.",3
Learning to Measure the Static Friction Coefﬁcient in Cloth Contact,CVPR_2020,1,"Kiran S Bhat, Christopher D Twigg, Jessica K Hodgins, Pradeep K Khosla, Zoran Popovi ´c, and Steven M Seitz. Esti- mating cloth simulation parameters from video. In Proceed- ings of the 2003 ACM SIGGRAPH/Eurographics symposium on Computer animation , pages 37–51. Eurographics Associ- ation, 2003. 1,3",6
Learning to Measure the Static Friction Coefﬁcient in Cloth Contact,CVPR_2020,2,"David Clyde, Joseph Teran, and Rasmus Tamstorf. Model- ing and data-driven parameter estimation for woven fabrics. InProceedings of the ACM SIGGRAPH/Eurographics Sym- posium on Computer Animation , page 17. ACM, 2017. 3",3
Learning to Measure the Static Friction Coefﬁcient in Cloth Contact,CVPR_2020,3,"Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadarrama, Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko, and Trevor Darrell. Long-term recurrent convolutional net- works for visual recognition and description. In Proceed-ings of the IEEE conference on Computer Vision and Pattern Recognition , pages 2625–2634, 2015. 2,6",7
Learning to Measure the Static Friction Coefﬁcient in Cloth Contact,CVPR_2020,4,"Katherine L Bouman, Bei Xiao, Peter Battaglia, and William T Freeman. Estimating the material properties of fabric from video. In 2013 IEEE International Conference on Computer Vision , pages 1984–1991, 2013. 3",4
Learning to Measure the Static Friction Coefﬁcient in Cloth Contact,CVPR_2020,5,"Xianping Liu, Z Yue, Zhiping Cai, D G. Chetwynd, and Stu- art Smith. Quantifying touch-feel perception: Tribological aspects. Measurement Science & Technology - MEAS SCI TECHNOL , 19, 2008. 2",4
Optimal least-squares solution to the hand-eye calibration problem,CVPR_2020,1,"Mili Shah, Roger D. Eastman, and Tsai Hong. An overview of robot-sensor calibration methods for evaluation of perception systems. In Proceedings of the Workshop on Performance Metrics for Intelligent Systems , PerMIS ’12, pages 15–20, New York, NY , USA, 2012. ACM.",2
Optimal least-squares solution to the hand-eye calibration problem,CVPR_2020,2,"Jack C. K. Chou and M. Kamel. Finding the position and ori- entation of a sensor on a robot manipulator using quaternions. The International Journal of Robotics Research , 10(3):240– 254, 1991.",2
Optimal least-squares solution to the hand-eye calibration problem,CVPR_2020,3,"Peter D. Lax. Linear Algebra and Its Applications . Wiley- Interscience, Hoboken, NJ, second edition, 2007.",2
Optimal least-squares solution to the hand-eye calibration problem,CVPR_2020,4,"Yiu Cheung Shiu and Shaheen Ahmad. Calibration of wrist- mounted robotic sensors by solving homogeneous transform equations of the form ax= xb. Robotics and Automation, IEEE Transactions on , 5(1):16–29, 1989.",1
Optimal least-squares solution to the hand-eye calibration problem,CVPR_2020,5,"Roger Y . Tsai and Reimer K. Lenz. A new technique for fully autonomous and efﬁcient 3d robotics hand-eye calibration. In Proceedings of the 4th International Symposium on Robotics Research , pages 287–297, Cambridge, MA, USA, 1988. MIT Press.",2
S3V AE: Self-Supervised Sequential V AE,CVPR_2020,1,"Yoshua Bengio, Aaron Courville, and Pascal Vincent. Rep- resentation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence , 35(8):1798–1828, 2013. 1",3
S3V AE: Self-Supervised Sequential V AE,CVPR_2020,2,"Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. arXiv preprint arXiv:1802.07687 , 2018. 5",1
S3V AE: Self-Supervised Sequential V AE,CVPR_2020,3,"Emily L Denton et al. Unsupervised learning of disentangled representations from video. In Advances in neural informa- tion processing systems , pages 4414–4423, 2017. 2",1
S3V AE: Self-Supervised Sequential V AE,CVPR_2020,4,"Deng-Ping Fan, Ming-Ming Cheng, Jiang-Jiang Liu, Shang- Hua Gao, Qibin Hou, and Ali Borji. Salient objects in clutter: Bringing salient object detection to the foreground. In Eu- ropean Conference on Computer Vision (ECCV) . Springer, 2018. 1",6
S3V AE: Self-Supervised Sequential V AE,CVPR_2020,5,"Irina Higgins, Lo ¨ıc Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew M Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual con- cepts with a constrained variational framework. In ICLR , 2017. 1,2",8
Content-based Propagation of User Markings for,CVPR_2020,1,"Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien, editors. Semi-supervised learning . The MIT Press, 2006. 3",3
Content-based Propagation of User Markings for,CVPR_2020,2,"Yuri Boykov and Gareth Funka-Lea. Graph cuts and efﬁcient ND image segmentation. International Journal of Computer Vision , 70(2):109–131, 2006. 2",1
Content-based Propagation of User Markings for,CVPR_2020,3,"Yuri Boykov and Marie-Pierre Jolly. Interactive graph cuts for optimal boundary & region segmentation of objects in nd images. In IEEE International Conference on Computer Vision (ICCV) , volume 1, pages 105–112. IEEE, 2001. 2",1
Content-based Propagation of User Markings for,CVPR_2020,4,"Stuart Berg, Dominik Kutra, Thorben Kroeger, Christoph N. Straehle, et al., and Anna Kreshuk. ilastik: Interactive ma- chine learning for (bio) image analysis. Nature Methods , pages 1–7, 2019. 2",4
Content-based Propagation of User Markings for,CVPR_2020,5,"Ignacio Arganda-Carreras, Verena Kaynig, Curtis Rueden, Kevin W Eliceiri, Johannes Schindelin, Albert Cardona, and H Sebastian Seung. Trainable Weka Segmentation: a machine learning tool for microscopy pixel classiﬁcation. Bioinformatics , page 180, 2017. 2",7
Vehicle Re-Identiﬁcation Based on Complementary Features,CVPR_2020,1,"Ondrej Chum, James Philbin, Josef Sivic, Michael Isard, and Andrew Zisserman. Total recall: Automatic query expan- sion with a generative feature model for object retrieval. In 2007 IEEE 11th International Conference on Computer Vi- sion, pages 1–8. IEEE, 2007.",5
Vehicle Re-Identiﬁcation Based on Complementary Features,CVPR_2020,2,"Alexander Hermans, Lucas Beyer, and Bastian Leibe. In de- fense of the triplet loss for person re-identiﬁcation. arXiv preprint arXiv:1703.07737 , 2017.",3
Vehicle Re-Identiﬁcation Based on Complementary Features,CVPR_2020,3,"Xinchen Liu, Wu Liu, Huadong Ma, and Huiyuan Fu. Large- scale vehicle re-identiﬁcation in urban surveillance videos. In2016 IEEE International Conference on Multimedia and Expo (ICME) , pages 1–6. IEEE, 2016.",4
Vehicle Re-Identiﬁcation Based on Complementary Features,CVPR_2020,4,"Yan Bai, Yihang Lou, Feng Gao, Shiqi Wang, Yuwei Wu, and Ling-Yu Duan. Group-sensitive triplet embedding for vehicle reidentiﬁcation. IEEE Transactions on Multimedia , 20(9):2385–2399, 2018.",6
Vehicle Re-Identiﬁcation Based on Complementary Features,CVPR_2020,5,"Ruihang Chu, Yifan Sun, Yadong Li, Zheng Liu, Chi Zhang, and Yichen Wei. Vehicle re-identiﬁcation with viewpoint- aware metric learning. In Proceedings of the IEEE Inter- national Conference on Computer Vision , pages 8282–8291, 2019.",6
Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN,CVPR_2020,1,"Lawrence O Gostin, Laura A Levit, Sharyl J Nass, et al. Be- yond the HIPAA privacy rule: enhancing privacy, improving health through research . National Academies Press, 2009.",4
Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN,CVPR_2020,2,"Bargav Jayaraman, Lingxiao Wang, David Evans, and Quan- quan Gu. Distributed learning without distress: Privacy- preserving empirical risk minimization. In Advances in Neural Information Processing Systems , pages 6343–6354, 2018.",4
Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN,CVPR_2020,3,"Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision , 2016.",3
Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN,CVPR_2020,4,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014.",1
Synthetic Learning: Learn From Distributed Asynchronized Discriminator GAN,CVPR_2020,5,"Jakub Kone ˇcn`y, H Brendan McMahan, Felix X Yu, Peter Richt ´arik, Ananda Theertha Suresh, and Dave Bacon. Fed- erated learning: Strategies for improving communication ef- ﬁciency. arXiv preprint arXiv:1610.05492 , 2016.",6
Structure Preserving Generative Cross-Domain Learning,CVPR_2020,1,"Ruichu Cai, Zijian Li, Pengfei Wei, Jie Qiao, Kun Zhang, and Zhifeng Hao. Learning disentangled semantic represen- tation for domain adaptation. In IJCAI: proceedings of the conference , volume 2019, page 2060. NIH Public Access, 2019. 6",6
Structure Preserving Generative Cross-Domain Learning,CVPR_2020,2,"Mingsheng Long, Yue Cao, Jianmin Wang, and Michael I Jordan. Learning transferable features with deep adaptation networks. arXiv preprint arXiv:1502.02791 , 2015. 1,2",4
Structure Preserving Generative Cross-Domain Learning,CVPR_2020,3,"Alexander M Bronstein, Michael M Bronstein, Ron Kim- mel, Mona Mahmoudi, and Guillermo Sapiro. A gromov-hausdorff framework with diffusion geometry for topologically-robust non-rigid shape matching. Interna- tional Journal of Computer Vision , 89(2-3):266–286, 2010. 2",5
Structure Preserving Generative Cross-Domain Learning,CVPR_2020,4,"Chen-Yu Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsuper- vised domain adaptation. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 10285–10295, 2019. 2",4
Structure Preserving Generative Cross-Domain Learning,CVPR_2020,5,"Arthur Gretton, Alex Smola, Jiayuan Huang, Marcel Schmit- tfull, Karsten Borgwardt, and Bernhard Sch ¨olkopf. Covari- ate shift by kernel mean matching. Dataset shift in machine learning , 3(4):5, 2009. 1",6
Visually Imbalanced Stereo Matching,CVPR_2020,1,"Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In CVPR , 2018.",1
Visually Imbalanced Stereo Matching,CVPR_2020,2,"Heiko Hirschmueller. Stereo processing by semiglobal matching and mutual information. TPAMI , 2008.",2
Visually Imbalanced Stereo Matching,CVPR_2020,3,"Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In CVPR Workshops , 2017.",5
Visually Imbalanced Stereo Matching,CVPR_2020,4,"Saumil S Patel, Michael T Ukwade, Scott B Stevenson, Harold E Bedell, Vanitha Sampath, and Haluk Ogmen. Stereoscopic depth perception from oblique phase dispari- ties. Vision Research , 2003.",6
Visually Imbalanced Stereo Matching,CVPR_2020,5,"Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. InInternational Conference on Medical Image Computing and Computer-assisted Intervention , 2015.",3
Alleviation of Gradient Exploding in GANs: Fake Can Be Real,CVPR_2020,1,"Adji B Dieng, Francisco JR Ruiz, David M Blei, and Michalis K Titsias. Prescribed generative adversarial net- works. arXiv preprint arXiv:1910.04302 , 2019.",4
Alleviation of Gradient Exploding in GANs: Fake Can Be Real,CVPR_2020,2,"Arnab Ghosh, Viveka Kulharia, Vinay P Namboodiri, Philip HS Torr, and Puneet K Dokania. Multi-agent diverse generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8513–8521, 2018.",5
Alleviation of Gradient Exploding in GANs: Fake Can Be Real,CVPR_2020,3,"Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincen- t Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in neural information pro- cessing systems , pages 5767–5777, 2017.",5
Alleviation of Gradient Exploding in GANs: Fake Can Be Real,CVPR_2020,4,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibri- um. In Advances in Neural Information Processing Systems , pages 6626–6637, 2017.",5
Alleviation of Gradient Exploding in GANs: Fake Can Be Real,CVPR_2020,5,"Lars Mescheder, Andreas Geiger, and Sebastian Nowozin. Which training methods for GANs do actually converge? In Proceedings of the 35th International Conference on Ma- chine Learning , pages 3481–3490, 2018.",3
Open Compound Domain Adaptation,CVPR_2020,1,"Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In ICML , 2015. 2,5",1
Open Compound Domain Adaptation,CVPR_2020,2,"Switzerland handwritten digits dataset. https:// github.com/kensanata/numbers . Accessed: 2019- 03-15. 5,8",2
Open Compound Domain Adaptation,CVPR_2020,3,"Andreea Bobu, Eric Tzeng, Judy Hoffman, and Trevor Darrell. Adapting to continuously shifting domains. ICLR Workshop , 2018. 3",4
Open Compound Domain Adaptation,CVPR_2020,4,"Zhangjie Cao, Lijia Ma, Mingsheng Long, and Jianmin Wang. Partial adversarial domain adaptation. In ECCV , 2018. 3",4
Open Compound Domain Adaptation,CVPR_2020,5,"Ziliang Chen, Jingyu Zhuang, Xiaodan Liang, and Liang Lin. Blending-target domain adaptation by adversarial meta- adaptation networks. In CVPR , 2019. 3,6",4
"Noise Modeling, Synthesis and Classiﬁcation for Generic Object Anti-Spooﬁng",CVPR_2020,1,"W. Bao, H. Li, N. Li, and W. Jiang. A liveness detection method for face recognition based on optical ﬂow ﬁeld. In International Conference on Image Analysis and Signal Pro- cessing , 2009. 2",2
"Noise Modeling, Synthesis and Classiﬁcation for Generic Object Anti-Spooﬁng",CVPR_2020,2,"A. Jourabloo, Y . Liu, and X. Liu. Face de-spooﬁng: Anti- spooﬁng via noise modeling. In European Conference on Computer Vision (ECCV) , 2018. 1,2",2
"Noise Modeling, Synthesis and Classiﬁcation for Generic Object Anti-Spooﬁng",CVPR_2020,3,"T. Chugh, K. Cao, and A. K. Jain. Fingerprint spoof buster: Use of minutiae-centered patches. IEEE Transactions on In- formation Forensics and Security (TIFS) , 2018. 2",2
"Noise Modeling, Synthesis and Classiﬁcation for Generic Object Anti-Spooﬁng",CVPR_2020,4,"L. Feng, L. Po, Y . Li, X. Xu, F. Yuan, T. C.-H. Cheung, and K. Cheung. Integration of image quality and motion cues for face anti-spooﬁng: A neural network approach. Journal of Visual Communication and Image Representation , 2016. 2",2
"Noise Modeling, Synthesis and Classiﬁcation for Generic Object Anti-Spooﬁng",CVPR_2020,5,"Q. Song, R. Xiong, D. Liu, Z. Xiong, F. Wu, and W. Gao. Fast image super-resolution via local adaptive gradient ﬁeld sharpening transform. IEEE Transactions on Image Process- ing, 2018. 3",2
Unsupervised Learning from Video with Deep Neural Embeddings,CVPR_2020,1,"Vision and Pattern Recognition , pages 6299–6308, 2017. [5]Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recogni- tion.arXiv preprint arXiv:1812.03982 , 2018. [6]Chuang Gan, Boqing Gong, Kun Liu, Hao Su, and",1
Unsupervised Learning from Video with Deep Neural Embeddings,CVPR_2020,2,2D CNNs and ImageNet? Proceedings of the IEEE,1
Unsupervised Learning from Video with Deep Neural Embeddings,CVPR_2020,3,"Transmission (3DPVT’06) , pages 1–8. IEEE, 2006. [2]Philip Bachman, R Devon Hjelm, and William Buch- walter. Learning representations by maximizing mutual information across views. arXiv preprint arXiv:1906.00910 , 2019. [3]Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised learning of visual features. In Proceedings of the Euro- pean Conference on Computer Vision (ECCV) , pages",2
Unsupervised Learning from Video with Deep Neural Embeddings,CVPR_2020,4,Inproceedings of the IEEE Conference on Computer,2
Unsupervised Learning from Video with Deep Neural Embeddings,CVPR_2020,5,"Amir Akbarzadeh, J-M Frahm, Philippos Mordohai,",4
A Training Method for Image Compression Networks to,CVPR_2020,1,"Jooyoung Lee, Seunghyun Cho, and Seung-Kwon Beack. Context-adaptive entropy model for end-to-end optimized image compression. In the 7th Int. Conf. on Learning Repre- sentations , May 2019. 1",3
A Training Method for Image Compression Networks to,CVPR_2020,2,"Johannes Ball ´e, Valero Laparra, and Eero P. Simoncelli. End-to-end optimized image compression. In the 5th Int. Conf. on Learning Representations , 2017. 1",3
A Training Method for Image Compression Networks to,CVPR_2020,3,"Jooyoung Lee, Seunghyun Cho, and Munchurl Kim. An end- to-end joint learning scheme of image compression and qual- ity enhancement with improved entropy minimization. arXiv preprint arXiv:1912.12817 , 2019. 1,2,3",3
A Training Method for Image Compression Networks to,CVPR_2020,4,"David Minnen, Johannes Ball ´e, and George Toderici. Joint autoregressive and hierarchical priors for learned image compression. In Advances in Neural Information Process- ing Systems , May 2018. 1",3
A Training Method for Image Compression Networks to,CVPR_2020,5,"Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision , 2016. 1",3
Triple-GAN: Progressive Face Aging with Triple Translation Loss,CVPR_2020,1,"Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco An- dreetto, and Hartwig Adam. Mobilenets: Efﬁcient convolu- tional neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861 , 2017.",8
Triple-GAN: Progressive Face Aging with Triple Translation Loss,CVPR_2020,2,Megvii Inc. Face++ research toolkit. www. faceplusplus.com .,2
Triple-GAN: Progressive Face Aging with Triple Translation Loss,CVPR_2020,3,"Yunfan Liu, Qi Li, and Zhenan Sun. Attribute enhanced face aging with wavelet-based generative adversarial networks. arXiv preprint arXiv:1809.06647 , 2018.[16] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE Interna- tional Conference on Computer Vision , pages 2794–2802, 2017.",3
Triple-GAN: Progressive Face Aging with Triple Translation Loss,CVPR_2020,4,"S Mukaida. Facial image synthesis using age manipula- tion based on statistical feature extraction. VIIP2002, Spain , 2002.",2
Triple-GAN: Progressive Face Aging with Triple Translation Loss,CVPR_2020,5,"Sveinn Palsson, Eirikur Agustsson, Radu Timofte, and Luc Van Gool. Generative adversarial style transfer networks for face aging. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops , pages 2084–2092, 2018.",4
Enhanced Blind Face Restoration with Multi-Exemplar Images,CVPR_2020,1,"Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high ﬁdelity natural image synthesis. ICLR , 2019. 6",3
Enhanced Blind Face Restoration with Multi-Exemplar Images,CVPR_2020,2,"Patrizio Campisi and Karen Egiazarian. Blind image decon- volution: theory and applications . CRC press, 2016. 1",1
Enhanced Blind Face Restoration with Multi-Exemplar Images,CVPR_2020,3,"Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In ICCV , 2017. 2,3,4",1
Enhanced Blind Face Restoration with Multi-Exemplar Images,CVPR_2020,4,"Guilin Liu, Fitsum A Reda, Kevin J Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for ir- regular holes using partial convolutions. In ECCV , 2018. 5",6
Enhanced Blind Face Restoration with Multi-Exemplar Images,CVPR_2020,5,"Takeru Miyato, Toshiki Kataoka, Masanori Koyama, and Yuichi Yoshida. Spectral normalization for generative ad- versarial networks. ICLR, 2018. 6",4
Thermal Image Super-Resolution Challenge - PBVS 2020,CVPR_2020,1,"Franc ¸ois Chollet. Xception: Deep learning with depthwise separable convolutions. In Proceedings of the IEEE Confer- ence on Computer Vision and Pattern Recognition (CVPR) , pages 1251–1258, 2017. 4",2
Thermal Image Super-Resolution Challenge - PBVS 2020,CVPR_2020,2,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June 2016. 6",4
Thermal Image Super-Resolution Challenge - PBVS 2020,CVPR_2020,3,"Xiaoyi Jia, Xiangmin Xu, Bolun Cai, and Kailing Guo. Sin- gle image super-resolution using multi-scale convolutional neural network. In Paciﬁc Rim Conference on Multimedia , pages 149–157. Springer, 2017. 6[4] Priya Kansal and Sabari Nathan. Eyenet: Attention based convolutional encoder-decoder network for eye region seg- mentation. arXiv preprint arXiv:1910.03274 , 2019. 6",4
Thermal Image Super-Resolution Challenge - PBVS 2020,CVPR_2020,4,"Biao Li, Jiabin Liu, Bo Wang, Zhiquan Qi, and Yong Shi. s-lwsr: Super lightweight super-resolution network. arXiv preprint arXiv:1909.10774 , 2019. 2,4",5
Thermal Image Super-Resolution Challenge - PBVS 2020,CVPR_2020,5,"Biao Li, Jiabin Liu, Bo Wang, Zhiquan Qi, and Yong Shi. s-lwsr: Super lightweight super-resolution network. arXiv preprint arXiv:1909.10774 , 2019. 2,4",5
Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identiﬁcation,CVPR_2020,1,"Dangwei Li, Xiaotang Chen, Zhang Zhang, and Kaiqi Huang. Learning deep context-aware features over body and latent parts for person re-identiﬁcation. In CVPR , 2017. 1",4
Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identiﬁcation,CVPR_2020,2,"Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In ICLR , 2015. 4",3
Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identiﬁcation,CVPR_2020,3,"Song Bai, Xiang Bai, and Qi Tian. Scalable person re- identiﬁcation on supervised smoothed manifold. In CVPR , 2017. 2",3
Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identiﬁcation,CVPR_2020,4,"Song Bai, Peng Tang, Philip H. S. Torr, and Longin Jan Late- cki. Re-ranking via metric fusion for object retrieval and person re-identiﬁcation. In CVPR , 2019. 2",3
Learning Multi-Granular Hypergraphs for Video-Based Person Re-Identiﬁcation,CVPR_2020,5,"Song Bai, Feihu Zhang, and Philip H. S. Torr. Hy- pergraph convolution and hypergraph attention. CoRR , abs/1901.08150, 2019. 3",3
Augment Your Batch: Improving Generalization Through Instance Repetition,CVPR_2020,1,"J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09 , 2009. 1,5",2
Augment Your Batch: Improving Generalization Through Instance Repetition,CVPR_2020,2,"Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. On large- batch training for deep learning: Generalization gap and sharp minima. In ICLR , 2017. 1,3[15] Shankar Krishnan, Ying Xiao, and Rif. A. Saurous. Neu- mann optimizer: A practical optimization algorithm for deep neural networks. In International Conference on Learning Representations , 2018. 1,2",5
Augment Your Batch: Improving Generalization Through Instance Repetition,CVPR_2020,3,"Antreas Antoniou, Amos Storkey, and Harrison Edwards. Data augmentation generative adversarial networks. arXiv preprint arXiv:1711.04340 , 2017. 2",3
Augment Your Batch: Improving Generalization Through Instance Repetition,CVPR_2020,4,"Tal Ben-Nun and Torsten Hoeﬂer. Demystifying parallel and distributed deep learning: An in-depth concurrency analysis. ACM Comput. Surv. , 52(4), Aug. 2019. 1",1
Augment Your Batch: Improving Generalization Through Instance Repetition,CVPR_2020,5,"R. D. Blumofe and C. E. Leiserson. Scheduling multi- threaded computations by work stealing. Journal of the ACM , 46(5):720–748, 1999. 4",2
Mapillary Street-Level Sequences: A Dataset for Lifelong P lace Recognition,CVPR_2020,1,"R. Arandjelovic, P . Gronat, A. Torii, T. Pajdla, and J. Sivic . NetVLAD: CNN architecture for weakly supervised place reco g- nition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 5297–5307, 2016. 1,2,3,6",2
Mapillary Street-Level Sequences: A Dataset for Lifelong P lace Recognition,CVPR_2020,2,"R. Arandjelovic, P . Gronat, A. Torii, T. Pajdla, and J. Sivic . NetVLAD: CNN Architecture for Weakly Supervised Place Recognition. IEEE Transactions on Pattern Analysis Machine Intelligence , 40(06):1437–1451, jun 2018. ISSN 1939-3539. doi: 10.1109/TPAMI .2017.2711011. 6",2
Mapillary Street-Level Sequences: A Dataset for Lifelong P lace Recognition,CVPR_2020,3,"D. G ´alvez-L ´opez and J. D. Tardos. Bags of binary words for fast place recognition in image sequences. IEEE Transactions on Robotics , 28(5):1188–1197, 2012. 2",2
Mapillary Street-Level Sequences: A Dataset for Lifelong P lace Recognition,CVPR_2020,4,"R. Arandjelovic and A. Zisserman. All About VLAD. 2013 IEEE Conference on Computer Vision and Pattern Recognition , pages 1578–1585, 2013. 2",2
Mapillary Street-Level Sequences: A Dataset for Lifelong P lace Recognition,CVPR_2020,5,"N. Carlevaris-Bianco, A. K. Ushani, and R. M. Eustice. University of Michigan North Campus long-term vision and lidar dataset. International Journal of Robotics Research , 35(9): 1023–1035, 2015. 2,3",2
A Lighting-Invariant Point Processor for Shading,CVPR_2020,1,"Berthold KP Horn. Shape from shading: A method for ob- taining the shape of a smooth opaque object from one view. Technical Report AITR-232, MIT Artiﬁcial Intelligence Lab- oratory, 1970. 1",2
A Lighting-Invariant Point Processor for Shading,CVPR_2020,2,"Ronen Basri and David W Jacobs. Lambertian reﬂectance and linear subspaces. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI) , (2):218–233, 2003. 2",1
A Lighting-Invariant Point Processor for Shading,CVPR_2020,3,"Benjamin Kunsberg and Steven W Zucker. How shading constrains surface patches without knowledge of light sources. SIAM Journal on Imaging Sciences , 7(2):641–668, 2014. 2, 3,4",1
A Lighting-Invariant Point Processor for Shading,CVPR_2020,4,"A Yuille and D Snow. Shape and albedo from multiple im- ages using integrability. In Computer Vision and Pattern Recognition (CVPR) , pages 158–164, 1997. 7",1
A Lighting-Invariant Point Processor for Shading,CVPR_2020,5,"Jonathan T. Barron and Jitendra Malik. Shape, illumination, and reﬂectance from shading. IEEE Transactions on Pat- 101 tern Analysis and Machine Intelligence (TPAMI) , 37(8):1670– 1687, 2015. 2",2
CARS: Continuous Evolution for Efﬁcient Neural Architecture Search,CVPR_2020,1,"Francesco Paolo Casale, Jonathan Gordon, and Nicolo Fusi. Probabilistic neural architecture search. arXiv , 2019.",3
CARS: Continuous Evolution for Efﬁcient Neural Architecture Search,CVPR_2020,2,"Kai Han, Yunhe Wang, Han Shu, Chuanjian Liu, Chunjing Xu, and Chang Xu. Attribute aware pooling for pedestrian attribute recognition. IJCAI , 2019.",6
CARS: Continuous Evolution for Efﬁcient Neural Architecture Search,CVPR_2020,3,"Shaohui Lin, Rongrong Ji, Chao Chen, Dacheng Tao, and Jiebo Luo. Holistic cnn compression via low-rank decompo- sition with knowledge transfer. T-PAMI , 2019.",5
CARS: Continuous Evolution for Efﬁcient Neural Architecture Search,CVPR_2020,4,"Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng- Lin Liu. Practical block-wise neural network architecture generation. CVPR , 2018.",5
CARS: Continuous Evolution for Efﬁcient Neural Architecture Search,CVPR_2020,5,"Yanqi Zhou, Siavash Ebrahimi, Sercan Ö Arık, Haonan Yu, Hairong Liu, and Greg Diamos. Resource-efﬁcient neural architect. arXiv , 2018.",6
Deformable Siamese Attention Networks for Visual Object Tracking,CVPR_2020,1,"David Held, Sebastian Thrun, and Silvio Savarese. Learning to track at 100 fps with deep regression networks. In ECCV , 2016.",3
Deformable Siamese Attention Networks for Visual Object Tracking,CVPR_2020,2,"Jo ˜ao F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking with kernelized correlation ﬁl- ters. IEEE transactions on pattern analysis and machine in- telligence , 37(3):583–596, 2014.",4
Deformable Siamese Attention Networks for Visual Object Tracking,CVPR_2020,3,"M Kristan, A Leonardis, J Matas, M Felsberg, R Pﬂugfelder, LˇCehovin, T V oj ´ır, G H ¨ager, A Luke ˇziˇc, G Fern ´andez, et al. The visual object tracking vot2016 challenge results. Lec- ture Notes in Computer Science (including subseries Lecture Notes in Artiﬁcial Intelligence and Lecture Notes in Bioin- formatics) , 9914:777–823, 2016.",11
Deformable Siamese Attention Networks for Visual Object Tracking,CVPR_2020,4,"Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, and Vincent Vanhoucke. Youtube-boundingboxes: A large high-precision human-annotated data set for object detection in video. In CVPR , 2017.",5
Deformable Siamese Attention Networks for Visual Object Tracking,CVPR_2020,5,"Qiang Wang, Li Zhang, Luca Bertinetto, Weiming Hu, and Philip HS Torr. Fast online object tracking and segmentation: A unifying approach. In CVPR , 2019.",5
Ternary MobileNets via Per-Layer Hybrid Filter Banks,CVPR_2020,1,"Yoni Choukroun, Eli Kravchik, and Pavel Kisilev. Low- bit quantization of neural networks for efﬁcient inference. CoRR , abs/1902.06822, 2019. 8",3
Ternary MobileNets via Per-Layer Hybrid Filter Banks,CVPR_2020,2,"Alireza Aghasi, Afshin Abdi, Nam Nguyen, and Justin Romberg. Net-trim: Convex pruning of deep neural net- works with performance guarantee. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA , pages 3180–3189, 2017. 8",4
Ternary MobileNets via Per-Layer Hybrid Filter Banks,CVPR_2020,3,"Hande Alemdar, Nicholas Caldwell, Vincent Leroy, Adrien Prost-Boucle, and Fr ´ed´eric P ´etrot. Ternary neural net- works for resource-efﬁcient AI applications. CoRR , abs/1609.00222, 2016. 1",5
Ternary MobileNets via Per-Layer Hybrid Filter Banks,CVPR_2020,4,"R. Andri, L. Cavigelli, D. Rossi, and L. Benini. Yodann: An architecture for ultralow power binary-weight cnn accel- eration. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 37(1):48–60, Jan 2018. 1,4",2
Ternary MobileNets via Per-Layer Hybrid Filter Banks,CVPR_2020,5,"Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Post-training 4-bit quantization of convolution networks for rapid-deployment. CoRR , abs/1810.05723, 2018. 8",4
"Correlating Edge, Pose with Parsing",CVPR_2020,1,"A. Buades, B. Coll, and J. M. Morel. A non-local algorithm for image denoising. In IEEE Computer Society Conference on Computer Vision & Pattern Recognition , 2005.",2
"Correlating Edge, Pose with Parsing",CVPR_2020,2,"Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision , pages 801–818, 2018.",5
"Correlating Edge, Pose with Parsing",CVPR_2020,3,"Ke Gong, Yiming Gao, Xiaodan Liang, Xiaohui Shen, Meng Wang, and Liang Lin. Graphonomy: Universal human pars- ing via graph transfer learning. In Proceedings of the IEEEConference on Computer Vision and Pattern Recognition , pages 7450–7459, 2019.",6
"Correlating Edge, Pose with Parsing",CVPR_2020,4,"Xiaodan Liang, Gong Ke, Xiaohui Shen, and Lin Liang. Look into person: Joint body parsing and pose estimation network and a new benchmark. IEEE Transactions on Pat- tern Analysis and Machine Intelligence , PP(99):1–1, 2018.",4
"Correlating Edge, Pose with Parsing",CVPR_2020,5,"Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Pro- ceedings of the IEEE conference on computer vision and pat- tern recognition , pages 3431–3440, 2015.",3
Fast Hardware-Aware Neural Architecture Search,CVPR_2020,1,"Matthew D.Zeiler and Rob Fergus. Visualizing and under- standing convolutional networks. ECCV , 2014.",1
Fast Hardware-Aware Neural Architecture Search,CVPR_2020,2,"Intel. Intel distribution of openvino toolkit, 2018 r5 build.https://software.intel.com/en-us/ openvino-toolkit , 2018.",2
Fast Hardware-Aware Neural Architecture Search,CVPR_2020,3,"Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh- moginov, and Liang-Chieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. CVPR, 2018.",5
Fast Hardware-Aware Neural Architecture Search,CVPR_2020,4,"Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient convolutional neural net- work for mobile devices. CVPR, 2018.",4
Fast Hardware-Aware Neural Architecture Search,CVPR_2020,5,"Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V . Le. Learning transferable architectures for scalable image recognition. In CVPR , 2018.",4
Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians,CVPR_2020,1,"3×3, 2563×3, 2561× 1, 2SoftMaxVisibleVisible",4
Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians,CVPR_2020,2,PRM Module𝑠𝑅𝑃𝑀(∙) (b) 𝑐𝑜𝑠𝑖𝑛𝑒 𝑠𝑖𝑚𝑖𝑙𝑎𝑟𝑖𝑡𝑦2nd channel< =,2
Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians,CVPR_2020,3,Figure 6. (a) Illustration of the motivation of the PRM module. (b),2
Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians,CVPR_2020,4,"The proposed PRM module. pedestrians, we design a part-based relation module (PRM) as shown in Fig. 6(b). For a current pedestrian candi- date, the PRM module will favor its counterparts with sim- ilar visible parts and assign them large adaptive weights in aggregating features. For the example in Fig. 6(a), we want to use the embedding features of upper-body to mea- sure the semantic similarity between bkt tandbkt+τ t+τ, since both their upper parts are visible. To this end, given a pair ofbkt tandbki i, the PRM module ﬁrst applies a seg- mentation subnetwork Nsegtoxkt tto predict the visible maskvkt=Nseg(xkt t)for the current pedestrian candidate, wherevkt∈[0,1]7×7×1. Next, the adaptive weight wki iis computed using an improved semantic similarity function sPRM(·), which is deﬁned in terms of vkt: sPRM(ekt t,eki i) =1 |V|/summationdisplay p∈Vekt t(p)·eki i(p) |ekt t(p)||eki i(p)|, (10) whereV={p|vkt(p)/greaterorequalslantmin{0.5,γ}}andγis a thresh- old which adaptively determined by vkt. For background, the values in vkttend to be zero. In order to retain enough pixels for computing the semantic similarity for background proposals, γis set to a value such that at least 20% pixels in embedding features are retained. The percentile 20% is chosen according to the deﬁnition of heavy occlusion in ex- isting pedestrian dataset: a pedestrian is considered to be heavily occluded if only 20%−65% of its body is visible.",2
Temporal-Context Enhanced Detection of Heavily Occluded Pedestrians,CVPR_2020,5,2.4 Discussion,2
Few-Shot Open-Set Recognition using Meta-Learning,CVPR_2020,1,"Abhijit Bendale and Terrance E. Boult. Towards open set deep networks. Proceedings of the IEEE Computer Soci- ety Conference on Computer Vision and Pattern Recognition , 2016-Decem:1563–1572, 2016. 1,2,6,7",1
Few-Shot Open-Set Recognition using Meta-Learning,CVPR_2020,2,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. 1",6
Few-Shot Open-Set Recognition using Meta-Learning,CVPR_2020,3,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model- Agnostic Meta-Learning for Fast Adaptation of Deep Net- works. 2017. 1,3",3
Few-Shot Open-Set Recognition using Meta-Learning,CVPR_2020,4,"Chelsea Finn, Kelvin Xu, and Sergey Levine. Probabilistic Model-Agnostic Meta-Learning. (NeurIPS), 2018. 3",3
Few-Shot Open-Set Recognition using Meta-Learning,CVPR_2020,5,"Zongyuan Ge, Sergey Demyanov, and Rahil Garnavi. Gen- erative OpenMax for Multi-Class Open Set Classiﬁcation. 2019. 1,2,6,7",3
HigherHRNet: Scale-Aware Representation Learning for,CVPR_2020,1,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic image segmen- tation with deep convolutional nets and fully connected crfs. 2015. 3",5
HigherHRNet: Scale-Aware Representation Learning for,CVPR_2020,2,"Bowen Cheng, Liang-Chieh Chen, Yunchao Wei, Yukun Zhu, Zilong Huang, Jinjun Xiong, Thomas S Huang, Wen- Mei Hwu, and Honghui Shi. Spgnet: Semantic prediction guidance for scene parsing. In ICCV , 2019. 3",9
HigherHRNet: Scale-Aware Representation Learning for,CVPR_2020,3,"Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In CVPR , 2020. 3",7
HigherHRNet: Scale-Aware Representation Learning for,CVPR_2020,4,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014. 5",1
HigherHRNet: Scale-Aware Representation Learning for,CVPR_2020,5,"Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE TPAMI , 2017. 3",3
Eff-UNet: A Novel Architecture for Semantic Segmentation in Unstructured,CVPR_2020,1,"Gabriel J. Brostow, Julien Fauqueur, and Roberto Cipolla. Semantic object classes in video: A high-deﬁnition ground truth database. Pattern Recogn. Lett. , 30(2):88–97, Jan. 2009. 3",2
Eff-UNet: A Novel Architecture for Semantic Segmentation in Unstructured,CVPR_2020,2,"Ashutosh Mishra, Sudhir Kumar, Tarun Kalluri, Girish Varma, Anbumani Subramanian, Manmohan Chandrakar, and C V Jawahar. Semantic segmentation datasets for re- source constrained training. 7th National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG) , October 2019. 2,3,6",7
Eff-UNet: A Novel Architecture for Semantic Segmentation in Unstructured,CVPR_2020,3,"Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE Transactions on Pattern Anal- ysis and Machine Intelligence , 39:2481–2495, 2016. 3",3
Eff-UNet: A Novel Architecture for Semantic Segmentation in Unstructured,CVPR_2020,4,"B. Baheti, S. Gajre, and S. Talbar. Semantic scene under- standing in unstructured environment with deep convolu- tional neural network. In TENCON 2019 - 2019 IEEE Re- gion 10 Conference (TENCON) , pages 790–795, Oct 2019. 2",2
Eff-UNet: A Novel Architecture for Semantic Segmentation in Unstructured,CVPR_2020,5,"Gabriel J. Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and recognition using struc- ture from motion point clouds. In Computer Vision – ECCV 2008 , pages 44–57, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg. 2",2
Less is More: Sample Selection and Label Conditioning,CVPR_2020,1,"D. Bisla, A. Choromanska, R. S. Berman, J. A. Stein, and D. Polsky. Towards automated melanoma detection with deep learning: Data puriﬁcation and augmentation. In IEEEConference on Computer Vision and Pattern Recognition Workshops , 2019. 1",2
Less is More: Sample Selection and Label Conditioning,CVPR_2020,2,"Y . Ji, X. Li, G. Zhang, D. Lin, and H. Chen. Automatic skin lesion segmentation by feature aggregation convolu- tional neural network. Technical report, 2018. 2",2
Less is More: Sample Selection and Label Conditioning,CVPR_2020,3,"D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014. 4",2
Less is More: Sample Selection and Label Conditioning,CVPR_2020,4,"M. L. McHugh. Interrater reliability: the kappa statistic. Bio- chemia medica: Biochemia medica , 22(3):276–282, 2012. 3",2
Less is More: Sample Selection and Label Conditioning,CVPR_2020,5,International Skin Imaging Collaboration: Melanoma Project.https://isic-archive.com .3,2
Modeling Biological Immunity to Adversarial Examples,CVPR_2020,1,"Guy Isely, Christopher Hillar, and Fritz Sommer. Decipher- ing subsampled data: adaptive compressive sampling as a principle of brain communication. In Advances in neural in- formation processing systems , pages 910–918, 2010.",3
Modeling Biological Immunity to Adversarial Examples,CVPR_2020,2,"G ¨unther Palm. Neural associative memories and sparse cod- ing. Neural Networks , 37:165–171, 2013.",2
Modeling Biological Immunity to Adversarial Examples,CVPR_2020,3,"Yijing Watkins, Austin Thresher, David Mascarenas, and Garrett T Kenyon. Sparse coding enables the reconstruction of high-ﬁdelity images and video from retinal spike trains. In Proceedings of the International Conference on Neuromor- phic Systems , page 8. ACM, 2018.",4
Modeling Biological Immunity to Adversarial Examples,CVPR_2020,4,"Weilin Xu, David Evans, and Yanjun Qi. Feature squeez- ing: Detecting adversarial examples in deep neural networks. arXiv preprint arXiv:1704.01155 , 2017.[48] Zhenglong Zhou and Chaz Firestone. Humans can decipher adversarial images. Nature communications , 10(1):1334, 2019. 4675",3
Modeling Biological Immunity to Adversarial Examples,CVPR_2020,5,"Joseph J Atick and A Norman Redlich. What does the retina know about natural scenes? Neural computation , 4(2):196– 210, 1992.",1
Local Deep Implicit Functions for 3D Shape,CVPR_2020,1,"Bruce G Baumgart. A polyhedron representation for com- puter vision. In Proceedings of the May 19-22, 1975, na- tional computer conference and exposition , pages 589–596. ACM, 1975. 2",1
Local Deep Implicit Functions for 3D Shape,CVPR_2020,2,"Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano- lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], Stanford University — Princeton University — Toyota Tech- nological Institute at Chicago, 2015. 4",2
Local Deep Implicit Functions for 3D Shape,CVPR_2020,3,"Kyle Genova, Forrester Cole, Daniel Vlasic, Aaron Sarna, William T Freeman, and Thomas Funkhouser. Learning shape templates with structured implicit functions. In Pro- ceedings of the IEEE International Conference on Computer Vision , pages 7154–7164, 2019. 2,3,4,5,6,7,8[14] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh r-cnn. In Proceedings of the IEEE International Conference on Computer Vision , pages 9785–9795, 2019. 2",6
Local Deep Implicit Functions for 3D Shape,CVPR_2020,4,"Shigeru Muraki. V olumetric shape description of range data using blobby model. ACM SIGGRAPH computer graphics , 25(4):227–235, 1991. 2",2
Local Deep Implicit Functions for 3D Shape,CVPR_2020,5,"Zhiqin Chen and Hao Zhang. Learning implicit ﬁelds for generative shape modeling. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 5939–5948, 2019. 1,2",1
Using Sinusoidally-Modulated Noise as a Surrogate for Slow-Wave Sleep to,CVPR_2020,1,"Filipp Akopyan, Jun Sawada, Andrew Cassidy, Rodrigo Alvarez-Icaza, John Arthur, Paul Merolla, Nabil Imam, Yu- taka Nakamura, Pallab Datta, Gi-Joon Nam, et al. Truenorth: Design and tool ﬂow of a 65 mw 1 million neuron pro- grammable neurosynaptic chip. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems , 34(10):1537–1557, 2015.",11
Using Sinusoidally-Modulated Noise as a Surrogate for Slow-Wave Sleep to,CVPR_2020,2,"K. Boahen. A neuromorph’s prospectus. Computing in Sci- ence Engineering , 19(2):14–28, Mar. 2017.",2
Using Sinusoidally-Modulated Noise as a Surrogate for Slow-Wave Sleep to,CVPR_2020,3,"Mike Davies, Narayan Srinivasa, Tsung-Han Lin, Gautham Chinya, Yongqiang Cao, Sri Harsha Choday, Georgios Di- mou, Prasad Joshi, Nabil Imam, Shweta Jain, et al. Loihi: A neuromorphic manycore processor with on-chip learning. IEEE Micro , 38(1):82–99, 2018.",11
Using Sinusoidally-Modulated Noise as a Surrogate for Slow-Wave Sleep to,CVPR_2020,4,"Simon Friedmann, Johannes Schemmel, Andreas Gr ¨ubl, An- dreas Hartel, Matthias Hock, and Karlheinz Meier. Demon- strating hybrid learning in a ﬂexible neuromorphic hardware system. IEEE transactions on biomedical circuits and sys- tems, 11(1):128–142, 2017. Figure 9: Sinusoidally-modulated slow-wave sleep with the noise amplitude set to 0. Figure 10: Sinusoidally-modulated slow-wave sleep with the noise amplitude set to 1. Figure 11: Cifar-10 Input. (a) Before training (b) Some training (c) More training Figure 12: Reconstructions with sleep. (a) Before training (b) Some training (c) More training Figure 13: Top 64 most active dictionary elements with Sleeps.",6
Using Sinusoidally-Modulated Noise as a Surrogate for Slow-Wave Sleep to,CVPR_2020,5,"Garrett T Kenyon. Extreme synergy: Spatiotemporal cor- relations enable rapid image reconstruction from computer- generated spike trains. Journal of vision , 10(3):21–21, 2010.",1
Distilling Knowledge from Graph Convolutional Networks,CVPR_2020,1,"Hanting Chen, Yunhe Wang, Chang Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chunjing Xu, Chao Xu, and Qi Tian. Data-free learning of student networks. arXiv preprint arXiv:1904.01186 , 2019.",9
Distilling Knowledge from Graph Convolutional Networks,CVPR_2020,2,"Xiaoxi He, Zimu Zhou, and Lothar Thiele. Multi-task zip- ping via layer-wise neuron sharing. In Advances in Neural Information Processing Systems , pages 6016–6026, 2018.",3
Distilling Knowledge from Graph Convolutional Networks,CVPR_2020,3,"Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In Advances in neural informa- tion processing systems , pages 5099–5108, 2017.",4
Distilling Knowledge from Graph Convolutional Networks,CVPR_2020,4,"Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph at- tention networks. arXiv preprint arXiv:1710.10903 , 2017.",6
Distilling Knowledge from Graph Convolutional Networks,CVPR_2020,5,"Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 1263– 1272. JMLR. org, 2017.",5
Exemplar Normalization for Learning Deep Representation,CVPR_2020,1,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR , 2009.",6
Exemplar Normalization for Learning Deep Representation,CVPR_2020,2,"Sheng Guo, Weilin Huang, Haozhi Zhang, Chenfan Zhuang, Dengke Dong, Matthew R Scott, and Dinglong Huang. Cur- riculumnet: Weakly supervised learning from large-scale web images. In ECCV , 2018.",7
Exemplar Normalization for Learning Deep Representation,CVPR_2020,3,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. I- dentity mappings in deep residual networks. In ECCV , 2016.",4
Exemplar Normalization for Learning Deep Representation,CVPR_2020,4,"Wenqi Shao, Shitao Tang, Xingang Pan, Ping Tan, Xiao- gang Wang, and Ping Luo. Channel equilibrium networks for learning deep representation. arXiv:2003.00214 , 2020.",6
Exemplar Normalization for Learning Deep Representation,CVPR_2020,5,"Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450 , 2016.",3
SpeedNet: Learning the Speediness in Videos,CVPR_2020,1,"Vahid Kiani and Hamid Reza Pourreza. An effective slow- motion detection approach for compressed soccer videos. ISRN Machine Vision , 2012, 2012.",1
SpeedNet: Learning the Speediness in Videos,CVPR_2020,2,"Hsin-Ying Lee, Jia-Bin Huang, Maneesh Singh, and Ming- Hsuan Yang. Unsupervised representation learning by sort- ing sequences. pages 667–676, 10 2017.",4
SpeedNet: Learning the Speediness in Videos,CVPR_2020,3,"Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,Tim Green, Trevor Back, Paul Natsev, et al. The kinetics hu- man action video dataset. arXiv preprint arXiv:1705.06950 , 2017.",11
SpeedNet: Learning the Speediness in Videos,CVPR_2020,4,"Sami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Apostol (Paul) Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. Youtube- 8m: A large-scale video classiﬁcation benchmark. In arXiv:1609.08675 , 2016.",7
SpeedNet: Learning the Speediness in Videos,CVPR_2020,5,"Eric P Bennett and Leonard McMillan. Computational time- lapse video. In ACM Transactions on Graphics (TOG) , vol- ume 26, page 102. ACM, 2007.",1
GAMIN: Generative Adversarial Multiple Imputation Network,CVPR_2020,1,"Jinsung Yoon, James Jordon, and Mihaela Van Der Schaar. Gain: Missing data imputation using generative adversarial nets. arXiv preprint arXiv:1806.02920 , 2018.",3
GAMIN: Generative Adversarial Multiple Imputation Network,CVPR_2020,2,"Rebecca R Andridge and Roderick JA Little. A review of hot deck imputation for survey non-response. International statistical review , 78(1):40–64, 2010.",1
GAMIN: Generative Adversarial Multiple Imputation Network,CVPR_2020,3,"Roderick JA Little and Donald B Rubin. Statistical analysis with missing data , volume 793. John Wiley & Sons, 2019.",1
GAMIN: Generative Adversarial Multiple Imputation Network,CVPR_2020,4,"Melissa J Azur, Elizabeth A Stuart, Constantine Frangakis, and Philip J Leaf. Multiple imputation by chained equations: what is it and how does it work? International journal of methods in psychiatric research , 20(1):40–49, 2011.",4
GAMIN: Generative Adversarial Multiple Imputation Network,CVPR_2020,5,"Daniel J Stekhoven and Peter B ¨uhlmann. Missforest—non- parametric missing value imputation for mixed-type data. Bioinformatics , 28(1):112–118, 2011.",1
Hierarchical Regression Network for Spectral Reconstruction from RGB Images,CVPR_2020,1,"Orest Kupyn, V olodymyr Budzan, Mykola Mykhailych, Dmytro Mishkin, and Ji ˇr´ı Matas. Deblurgan: Blind mo- tion deblurring using conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 8183–8192, 2018.",5
Hierarchical Regression Network for Spectral Reconstruction from RGB Images,CVPR_2020,2,"Daniela Poli and Thierry Toutin. Review of developments in geometric modelling for high resolution satellite pushb- room sensors. The Photogrammetric Record , 27(137):58– 73, 2012.",1
Hierarchical Regression Network for Spectral Reconstruction from RGB Images,CVPR_2020,3,"Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 3291–3300, 2018.",4
Hierarchical Regression Network for Spectral Reconstruction from RGB Images,CVPR_2020,4,"Yuzhi Zhao, Lai-Man Po, Tiantian Zhang, Zongbang Liao, Xiang Shi, Yujia Zhang, Weifeng Ou, Pengfei Xian, JingjingXiong, Chang Zhou, et al. Saliency map-aided generative ad- versarial network for raw to rgb mapping. In 2019 IEEE/CVF International Conference on Computer Vision Workshop (IC- CVW) , pages 3449–3457. IEEE, 2019.",11
Hierarchical Regression Network for Spectral Reconstruction from RGB Images,CVPR_2020,5,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil- ian Q Weinberger. Densely connected convolutional net- works. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4700–4708, 2017.",4
Adapting JPEG XS gains and priorities to tasks and contents,CVPR_2020,1,"ISO, Information technology — JPEG XS low-latency lightweight image coding system — Part 5: Reference soft- ware , vol. 2020. Jan. 2020. 2,4[15] “intopix tico-xs — jpeg-xs ip-cores and sdks (iso/iec 21122).” https://www.intopix.com/jpeg-xs . Accessed: 2020-02-18. 2",3
Adapting JPEG XS gains and priorities to tasks and contents,CVPR_2020,2,"ISO, Information technology — JPEG XS low-latency lightweight image coding system — Part 1: Core coding sys- tem, vol. 2019. May 2019. 1,2",3
Adapting JPEG XS gains and priorities to tasks and contents,CVPR_2020,3,"A. Descampe, J. Keinert, T. Richter, S. F ¨oßel, and G. Rou- vroy, “Jpeg xs, a new standard for visually lossless low- latency lightweight image compression,” in Applications of Digital Image Processing XL (A. G. Tescher, ed.), vol. 10396, pp. 68 – 79, International Society for Optics and Photonics, SPIE, 2017. 1,2",2
Adapting JPEG XS gains and priorities to tasks and contents,CVPR_2020,4,"Y . Jiang and M. S. Pattichis, “Jpeg image compression using quantization table optimization based on perceptual image quality assessment,” in 2011 Conference Record of the Forty Fifth Asilomar Conference on Signals, Systems and Comput- ers (ASILOMAR) , pp. 225–229, Nov 2011. 1",2
Adapting JPEG XS gains and priorities to tasks and contents,CVPR_2020,5,"M. Hopkins, M. Mitzenmacher, and S. Wagner-Carena, “Simulated annealing for jpeg quantization,” in 2018 Data Compression Conference , pp. 412–412, March 2018. 1",2
PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks,CVPR_2020,1,"Matan Atzmon, Haggai Maron, and Yaron Lipman. Point con- volutional neural networks by extension operators. arXiv preprint arXiv:1803.10091 , 2018. 2,6",3
PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks,CVPR_2020,2,"Marie-Julie Rakotosaona, Vittorio La Barbera, Paul Guerrero, Niloy J Mitra, and Maks Ovsjanikov. Pointcleannet: Learning to denoise and remove outliers from dense point clouds. In Computer Graphics Forum . Wiley Online Library, 2019. 3",5
PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks,CVPR_2020,3,"Pedro Hermosilla, Tobias Ritschel, Pere-Pau V ´azquez, `Alvar Vinacua, and Timo Ropinski. Monte carlo convolution for learning on non-uniformly sampled point clouds. In SIGGRAPH Asia 2018 Technical Papers , page 235. ACM, 2018. 1,2,3",5
PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks,CVPR_2020,4,"Li Yi, Vladimir G Kim, Duygu Ceylan, I Shen, Mengyan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas Guibas, et al. A scalable active framework for region annotation in 3d shape col- lections. ACM Transactions on Graphics (TOG) , 35(6):210, 2016. 6[50] Hengshuang Zhao, Li Jiang, Chi-Wing Fu, and Jiaya Jia. Pointweb: Enhancing local neighborhood features for point cloud processing. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 5565–5573, 2019. 2,6 5598",11
PointASNL: Robust Point Clouds Processing using Nonlocal Neural Networks,CVPR_2020,5,"Charu C Aggarwal. Outlier analysis. In Data mining , pages 237–263. Springer, 2015. 3",1
Instance Segmentation of Biological Images Using Harmonic Embeddings,CVPR_2020,1,"A. Bieniek and A. Moga. An efﬁcient watershed algo- rithm based on connected components. Pattern recognition , 33(6):907–916, 2000. 5",2
Instance Segmentation of Biological Images Using Harmonic Embeddings,CVPR_2020,2,"A. Fathi, Z. Wojna, V . Rathod, P. Wang, H. O. Song, S. Guadarrama, and K. P. Murphy. Semantic instance segmentation via deep metric learning. arXiv preprint arXiv:1703.10277 , 2017. 2,3",2
Instance Segmentation of Biological Images Using Harmonic Embeddings,CVPR_2020,3,"K. He, G. Gkioxari, P. Dollar, and R. Girshick. Mask r-cnn. InThe IEEE International Conference on Computer Vision (ICCV) , Oct 2017. 2,6,7,8",2
Instance Segmentation of Biological Images Using Harmonic Embeddings,CVPR_2020,4,"B. Romera-Paredes and P. H. S. Torr. Recurrent instance segmentation. In European Conference on Computer Vision , pages 312–329. Springer, 2016. 2,5",2
Instance Segmentation of Biological Images Using Harmonic Embeddings,CVPR_2020,5,"PyTorch tensors and dynamic neural networks in python with strong gpu acceleration. http://pytorch.org . Ac- cessed: 2018-11-15. 5,7 3850",1
Afﬁnity Graph Supervision for Visual Recognition,CVPR_2020,1,"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár. Focal loss for dense object detection. CVPR , 2017. 3[20] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. ECCV , 2014. 6",5
Afﬁnity Graph Supervision for Visual Recognition,CVPR_2020,2,"Chu Wang, Babak Samari, Vladimir G.Kim, Siddhartha Chaudhuri, and Kaleem Siddiqi. Afﬁnity graph supervision for visual recognition. arXiv preprint arXiv:2003.09049 , 2020.https://arxiv.org/abs/2003.09049 (vis- ited: 2020-03-27). 3,4,7",5
Afﬁnity Graph Supervision for Visual Recognition,CVPR_2020,3,RMSprop optimizer. http://www.cs.toronto.edu/ ~tijmen/csc321/slides/lecture_slides_ lec6.pdf . Accessed: 2019-11-11. 2,2
Afﬁnity Graph Supervision for Visual Recognition,CVPR_2020,4,"Michaël Defferrard, Xavier Bresson, and Pierre Van- dergheynst. Convolutional neural networks on graphs with fast localized spectral ﬁltering. NIPS , 2016. 1,2",3
Afﬁnity Graph Supervision for Visual Recognition,CVPR_2020,5,"Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. pages 1025–1035, 2017. 2",3
High-Resolution Dual-Stage Multi-Level Feature Aggregation for Single Image,CVPR_2020,1,"Seungjun Nah, Sungyong Baik, Seokil Hong, Gyeongsik Moon, Sanghyun Son, Radu Timofte, and Kyoung Mu Lee. Ntire 2019 challenge on video deblurring and super- resolution: Dataset and study. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops , June 2019. 2, 5, 7, 8",7
High-Resolution Dual-Stage Multi-Level Feature Aggregation for Single Image,CVPR_2020,2,"Seungjun Nah, Sanghyun Son, Radu Timofte, and Ky- oung Mu Lee. Ntire 2020 challenge on image and video deblurring. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition (CVPR) Workshops , June 2020. 2, 5, 6, 7",4
High-Resolution Dual-Stage Multi-Level Feature Aggregation for Single Image,CVPR_2020,3,"Hyeonjun Sim and Munchurl Kim. A deep motion deblur- ring network based on per-pixel adaptive kernels with resid- ual down-up and up-down modules. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) Workshops , 2019. 1, 2, 6, 7, 8",1
High-Resolution Dual-Stage Multi-Level Feature Aggregation for Single Image,CVPR_2020,4,Vinod Nair and Geoffrey E Hinton. Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings of the International Conference on Machine Learning (ICML) . 6,1
High-Resolution Dual-Stage Multi-Level Feature Aggregation for Single Image,CVPR_2020,5,"Yochai Blau, Roey Mechrez, Radu Timofte, Tomer Michaeli, and Lihi Zelnik-Manor. The 2018 pirm challenge on percep- tual image super-resolution. In Proceedings of the European Conference on Computer Vision (ECCV) . 6",5
Flow Contrastive Estimation of Energy-Based Models,CVPR_2020,1,"Martin Arjovsky, Soumith Chintala, and Bottou. Wasserstein gan. arXiv preprint arXiv:1701.07875 , 2017. 7",3
Flow Contrastive Estimation of Energy-Based Models,CVPR_2020,2,"Paul Baltescu and Phil Blunsom. Pragmatic neural lan- guage modelling in machine translation. arXiv preprint arXiv:1412.7119 , 2014. 2",1
Flow Contrastive Estimation of Energy-Based Models,CVPR_2020,3,"Jens Behrmann, David Duvenaud, and J ¨orn-Henrik Ja- cobsen. Invertible residual networks. arXiv preprint arXiv:1811.00995 , 2018. 1,4",3
Flow Contrastive Estimation of Energy-Based Models,CVPR_2020,4,"Avishek Joey Bose, Huan Ling, and Yanshuai Cao. Adversarial contrastive estimation. arXiv preprint arXiv:1805.03642 , 2018. 2",3
Flow Contrastive Estimation of Energy-Based Models,CVPR_2020,5,"Zihang Dai, Amjad Almahairi, Philip Bachman, Ed- uard Hovy, and Aaron Courville. Calibrating energy- based generative adversarial networks. arXiv preprint arXiv:1702.01691 , 2017. 2,8",5
Imparting Fairness to Pre-Trained Biased Representations,CVPR_2020,1,"Stephen Boyd and Lieven Vandenberghe. Convex Optimiza- tion. Cambridge University Press, 2004.",1
Imparting Fairness to Pre-Trained Biased Representations,CVPR_2020,2,"Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, and Graham Neubig. Controllable invariance through adversarial feature learning. In Advances in Neural Information Processing Systems , 2017. 1,2,6,7",5
Imparting Fairness to Pre-Trained Biased Representations,CVPR_2020,3,"Martin Bertran, Natalia Martinez, Afroditi Papadaki, Qiang Qiu, Miguel Rodrigues, Galen Reeves, and Guillermo Sapiro. Adversarially learned representations for information obfus- cation and inference. In International Conference on Machine Learning , 2019. 2",7
Imparting Fairness to Pre-Trained Biased Representations,CVPR_2020,4,"DP Bertsekas. Nonlinear programming 2nd edn (belmont, ma: Athena scientiﬁc). 1999. 4",2
Imparting Fairness to Pre-Trained Biased Representations,CVPR_2020,5,"Alex Beutel, Jilin Chen, Zhe Zhao, and Ed H Chi. Data deci- sions and theoretical implications when adversarially learning fair representations. arXiv preprint arXiv:1707.00075 , 2017. 2",4
Erasing Integrated Learning : A Simple yet Effective Approach for Weakly,CVPR_2020,1,"Xuanyi Dong, Deyu Meng, Fan Ma, and Yi Yang. A dual- network progressive approach to weakly supervised object detection. In Proceedings of the 25th ACM international conference on Multimedia , pages 279–287. ACM, 2017.",4
Erasing Integrated Learning : A Simple yet Effective Approach for Weakly,CVPR_2020,2,"Qibin Hou, PengTao Jiang, Yunchao Wei, and Ming-Ming Cheng. Self-erasing network for integral object attention. In Advances in Neural Information Processing Systems , pages 549–559, 2018.",4
Erasing Integrated Learning : A Simple yet Effective Approach for Weakly,CVPR_2020,3,"Xiaoyan Li, Meina Kan, Shiguang Shan, and Xilin Chen. Weakly supervised object detection with segmentation col- laboration. In The IEEE International Conference on Com- puter Vision (ICCV) , October 2019.",4
Erasing Integrated Learning : A Simple yet Effective Approach for Weakly,CVPR_2020,4,"Dongyu She, Jufeng Yang, Ming-Ming Cheng, Yu-Kun Lai, Paul L Rosin, and Liang Wang. Wscnet: Weakly supervised coupled networks for visual sentiment classiﬁcation and de- tection. IEEE Transactions on Multimedia , 2019.",6
Erasing Integrated Learning : A Simple yet Effective Approach for Weakly,CVPR_2020,5,"Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception archi- tecture for computer vision. In Proceedings of the IEEE con- ference on computer vision and pattern recognition , pages 2818–2826, 2016.",5
Adaptive Fractional Dilated Convolution Network,CVPR_2020,1,"Long Mai, Hailin Jin, and Feng Liu. Composition-preserving deep photo aesthetics assessment. In 2016 IEEE Confer- ence on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV , USA, June 27-30, 2016 , pages 497– 506, 2016.",3
Adaptive Fractional Dilated Convolution Network,CVPR_2020,2,"Katharina Schwarz, Patrick Wieschollek, and Hendrik P. A. Lensch. Will people like your image? learning the aesthetic space. In 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018, Lake Tahoe, NV , USA, March 12-15, 2018 , pages 2048–2057, 2018.",3
Adaptive Fractional Dilated Convolution Network,CVPR_2020,3,"Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolu- tion, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence , 40(4):834–848, 2018.",5
Adaptive Fractional Dilated Convolution Network,CVPR_2020,4,"Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In Proceedings of the IEEE international confer- ence on computer vision , pages 764–773, 2017.",7
Adaptive Fractional Dilated Convolution Network,CVPR_2020,5,"Ritendra Datta, Dhiraj Joshi, Jia Li, and James Ze Wang. Studying aesthetics in photographic images using a compu- tational approach. In Computer Vision - ECCV 2006, 9th Eu- ropean Conference on Computer Vision, Graz, Austria, May 7-13, 2006, Proceedings, Part III , pages 288–301, 2006.",4
Robust Object Detection under Occlusion with,CVPR_2020,1,"Z. Zhang J. Zhu L. Xie J. Wang, C. Xie and A. Yuille. De- tecting semantic parts on partially occluded objects. British Machine Vision Conference , 2017. 3,6[16] Yin Li Jian Sun and Sing Bing Kang. Symmetric stereo matching for occlusion handling. IEEE Conference on Com- puter Vision and Pattern Recognition , 2018. 3",2
Robust Object Detection under Occlusion with,CVPR_2020,2,"Adam Kortylewski, Qing Liu, Huiyu Wang, Zhishuai Zhang, and Alan Yuille. Combining compositional models and deep networks for robust object classiﬁcation under occlusion. arXiv preprint arXiv:1905.11826 , 2019. 1,2,6",5
Robust Object Detection under Occlusion with,CVPR_2020,3,"Adam Kortylewski, Qing Liu, Huiyu Wang, Zhishuai Zhang, and Alan Yuille. Combining compositional models and deep networks for robust object classiﬁcation under occlusion. arXiv preprint arXiv:1905.11826 , 2019. 1,2,6",5
Robust Object Detection under Occlusion with,CVPR_2020,4,"D. Arthur and S. Vassilvitskii. k-means++: The advantages of careful seeding. In Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms , 2007. 5",2
Robust Object Detection under Occlusion with,CVPR_2020,5,"Elie Bienenstock and Stuart Geman. Compositionality in neural systems. In The Handbook of Brain Theory and Neu- ral Networks , pages 223–226. 1998. 1",1
LatentFusion: End-to-End Differentiable Reconstruction and Rendering,CVPR_2020,1,"Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian Richardt, and Yong-Liang Yang. HoloGAN: Unsupervised learning of 3D representations from natural images. In Inter- national Conference on Computer Vision (ICCV) , 2019. 2, 3,4",5
LatentFusion: End-to-End Differentiable Reconstruction and Rendering,CVPR_2020,2,"Nicolas Ballas, Li Yao, Chris Pal, and Aaron Courville. Delving deeper into convolutional networks for learning video representations. arXiv preprint arXiv:1511.06432 , 2015. 4",4
LatentFusion: End-to-End Differentiable Reconstruction and Rendering,CVPR_2020,3,"Petr Beckmann and Andre Spizzichino. The scattering of electromagnetic waves from rough surfaces. Norwood, MA, Artech House, Inc. , 1987. 5",1
LatentFusion: End-to-End Differentiable Reconstruction and Rendering,CVPR_2020,4,"Blender Online Community. Blender - a 3D modelling and rendering package . Blender Foundation, 2019. 5",2
LatentFusion: End-to-End Differentiable Reconstruction and Rendering,CVPR_2020,5,"Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Mano- lis Savva, Shuran Song, Hao Su, Jianxiong Xiao, Li Yi, and Fisher Yu. ShapeNet: An Information-Rich 3D Model Repository. Technical Report arXiv:1512.03012 [cs.GR], 2015. 2,5,7",2
Bringing Old Photos Back to Life,CVPR_2020,1,"F. Stanco, G. Ramponi, and A. De Polo, “Towards the au- tomated restoration of old photographic prints: a survey,” in The IEEE Region 8 EUROCON 2003. Computer as a Tool. , vol. 2. IEEE, 2003, pp. 370–374. 1,2",2
Bringing Old Photos Back to Life,CVPR_2020,2,"K. Zhang, W. Zuo, S. Gu, and L. Zhang, “Learning deep cnn denoiser prior for image restoration,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni- tion, 2017, pp. 3929–3938. 1,2",2
Bringing Old Photos Back to Life,CVPR_2020,3,"K. Zhang, W. Zuo, Y . Chen, D. Meng, and L. Zhang, “Be- yond a gaussian denoiser: Residual learning of deep cnn for image denoising,” IEEE Transactions on Image Processing , vol. 26, no. 7, pp. 3142–3155, 2017. 1,2",2
Bringing Old Photos Back to Life,CVPR_2020,4,"V . Bruni and D. Vitulano, “A generalized model for scratch detection,” IEEE transactions on image processing , vol. 13, no. 1, pp. 44–50, 2004. 1,2",2
Bringing Old Photos Back to Life,CVPR_2020,5,"R.-C. Chang, Y .-L. Sie, S.-M. Chou, and T. K. Shih, “Photo defect detection for image inpainting,” in Seventh IEEE In- ternational Symposium on Multimedia (ISM’05) . IEEE, 2005, pp. 5–pp. 1,2",2
Learning to Evaluate Perception Models using Planner-Centric Metrics,CVPR_2020,1,"Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel Urtasun. Monocular 3d object de- tection for autonomous driving. In CVPR , June 2016. 2",6
Learning to Evaluate Perception Models using Planner-Centric Metrics,CVPR_2020,2,"Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Std: Sparse-to-dense 3d object detector for point cloud. InICCV , October 2019. 2",5
Learning to Evaluate Perception Models using Planner-Centric Metrics,CVPR_2020,3,"Brady Zhou, Philipp Kr ¨ahenb ¨uhl, and Vladlen Koltun. Does computer vision matter for action? Science Robotics , 4(30), 2019. 2",3
Learning to Evaluate Perception Models using Planner-Centric Metrics,CVPR_2020,4,Nvidia drive constellation. https://www. nvidia.com/en-us/self-driving-cars/ drive-constellation/ . Accessed: 2019-10-14. 2,2
Learning to Evaluate Perception Models using Planner-Centric Metrics,CVPR_2020,5,"Waymo open dataset: An autonomous driving dataset, 2019. 2,6",3
Real-time Tracking with Stabilized Frame,CVPR_2020,1,"Luca Bertinetto, Jack Valmadre, Joao F Henriques, Andrea Vedaldi, and Philip HS Torr. Fully-convolutional siamese networks for object tracking. In European conference on computer vision , pages 850–865. Springer, 2016.",5
Real-time Tracking with Stabilized Frame,CVPR_2020,2,"Bo Li, Junjie Yan, Wei Wu, Zheng Zhu, and Xiaolin Hu. High performance visual tracking with siamese region pro- posal network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 8971– 8980, 2018.",5
Real-time Tracking with Stabilized Frame,CVPR_2020,3,"Axel Sauer, Elie Aljalbout, and Sami Haddadin. Tracking holistic object representations, 2019.",3
Real-time Tracking with Stabilized Frame,CVPR_2020,4,"Jianren Wang, Yihui He, Xiaobo Wang, Xinjia Yu, and Xia Chen. Prediction-tracking-segmentation. arXiv preprint arXiv:1904.03280 , 2019.",5
Real-time Tracking with Stabilized Frame,CVPR_2020,5,"Shuaicheng Liu, Lu Yuan, Ping Tan, and Jian Sun. Bundled camera paths for video stabilization. Acm Transactions on Graphics , 32(4):1–10, 2013.",4
CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection,CVPR_2020,1,"Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV , 2017.",7
CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection,CVPR_2020,2,"Lichao Huang, Yi Yang, Yafeng Deng, and Yinan Yu. Dense- box: Unifying landmark localization with end to end object detection. arXiv preprint arXiv:1509.04874 , 2015.",4
CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection,CVPR_2020,3,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural net- works. In NIPS , 2012.",3
CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection,CVPR_2020,4,"Yuntao Chen, Chenxia Han, Naiyan Wang, and Zhaoxiang Zhang. Revisiting feature alignment for one-stage object de- tection. arXiv: Computer Vision and Pattern Recognition , 2019.",4
CentripetalNet: Pursuing High-quality Keypoint Pairs for Object Detection,CVPR_2020,5,"Hei Law and Jia Deng. Cornernet: Detecting objects as paired keypoints. In ECCV , 2018.",1
ManiGAN: Text-Guided Image Manipulation,CVPR_2020,1,"Afﬁne transformation has been widely implemented in conditional normalisation techniques [5, 7, 11, 21, 24, 26] to incorporate additional information [7, 11, 21], or to avoid information loss caused by normalisation [24]. Differently from these methods, our afﬁne combination module is de- signed to fuse text and image cross-modality representa- tions to enable effective manipulation, and is only placed at speciﬁc positions instead of all normalisation layers.",9
ManiGAN: Text-Guided Image Manipulation,CVPR_2020,2,Generative Adversarial Networks for Image,2
ManiGAN: Text-Guided Image Manipulation,CVPR_2020,3,"A bird with black eye rings and a black bill , with a red crown and a red belly .",1
ManiGAN: Text-Guided Image Manipulation,CVPR_2020,4,"to high-level semantics [41], to meet a user’s preferences, which has numerous potential applications in video games, image editing, and computer-aided design. Recently, with the development of deep learning and deep generative models, automatic image manipulation has made remark- able progress, including image inpainting [12, 25], image colourisation [40], style transfer [9, 14], and domain or at- tribute translation [13, 16].",5
ManiGAN: Text-Guided Image Manipulation,CVPR_2020,5,"All the above works mainly focus on speciﬁc problems, and few studies [6, 23] concentrate on more general and user-friendly image manipulation by using natural language descriptions. More precisely, the task aims to semantically edit parts of an image according to the given text provided by a user, while preserving other contents that are not de- scribed in the text. However, current state-of-the-art text-guided image manipulation methods are only able to pro- duce low-quality images (see Fig. 1: ﬁrst row), far from satisfactory, and even fail to effectively manipulate complex scenes (see Fig. 1: second row).",2
Interactive Image Segmentation with First Click Attention,CVPR_2020,1,"Sabarinath Mahadevan, Paul V oigtlaender, and Bastian Leibe. Iteratively trained interactive segmentation. In Brit. Mach. Vis. Conf. , 2018. 2,6,8",3
Interactive Image Segmentation with First Click Attention,CVPR_2020,2,"Yang Hu, Andrea Soltoggio, Russell Lock, and Steve Carter. A fully convolutional two-stream fusion network for inter- active image segmentation. Neural Networks , 109:31–42, 2019. 2,6",4
Interactive Image Segmentation with First Click Attention,CVPR_2020,3,"Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,Andreas Kopf, Edward Yang, Zachary DeVito, Martin Rai- son, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS , pages 8024–8035, 2019. 6",21
Interactive Image Segmentation with First Click Attention,CVPR_2020,4,"Vladimir Vezhnevets and Vadim Konouchine. Growcut: In- teractive multi-label nd image segmentation by cellular au- tomata. proc. of Graphicon , 1(4):150–156, 2005. 7",1
Interactive Image Segmentation with First Click Attention,CVPR_2020,5,"Huan Ling, Jun Gao, Amlan Kar, Wenzheng Chen, and Sanja Fidler. Fast interactive object annotation with curve-gcn. In IEEE Conf. Comput. Vis. Pattern Recog. , pages 5257–5266, 2019. 2",5
Guided Frequency Separation Network for Real-World Super-Resolution,CVPR_2020,1,"Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceed- ings of the IEEE International Conference on Computer Vi- sion, pages 1501–1510, 2017.",1
Guided Frequency Separation Network for Real-World Super-Resolution,CVPR_2020,2,"Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc Van Gool. Dslr-quality photos on mobile devices with deep convolutional networks. In Proceedings of the IEEE International Conference on Computer Vision , pages 3277–3285, 2017.",5
Guided Frequency Separation Network for Real-World Super-Resolution,CVPR_2020,3,"Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming- Hsuan Yang. Deep laplacian pyramid networks for fast and accurate super-resolution. In Proceedings of the IEEE con-ference on computer vision and pattern recognition , pages 624–632, 2017.",4
Guided Frequency Separation Network for Real-World Super-Resolution,CVPR_2020,4,"Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and Kyoung Mu Lee. Enhanced deep residual networks for single image super-resolution. In Proceedings of the IEEE confer- ence on computer vision and pattern recognition workshops , pages 136–144, 2017.",5
Guided Frequency Separation Network for Real-World Super-Resolution,CVPR_2020,5,"Pejman Rasti, Tonis Uiboupin, Sergio Escalera, and Gholam- reza Anbarjafari. Convolutional neural network super resolu- tion for face recognition in surveillance monitoring. In Inter- national conference on articulated motion and deformable objects , pages 175–184. Springer, 2016.",4
Hierarchical Feature Embedding for Attribute Recognition,CVPR_2020,1,"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense object detection. In Pro- ceedings of the IEEE international conference on computer vision , pages 2980–2988, 2017. 2",5
Hierarchical Feature Embedding for Attribute Recognition,CVPR_2020,2,"Abrar H Abdulnabi, Gang Wang, Jiwen Lu, and Kui Jia. Multi-task cnn model for attribute prediction. IEEE Trans- actions on Multimedia , 17(11):1949–1959, 2015. 2",4
Hierarchical Feature Embedding for Attribute Recognition,CVPR_2020,3,"Ejaz Ahmed, Michael Jones, and Tim K Marks. An improved deep learning architecture for person re-identiﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 3908–3916, 2015. 2",3
Hierarchical Feature Embedding for Attribute Recognition,CVPR_2020,4,"Lubomir Dimitrov Bourdev. Pose-aligned networks for deep attribute modeling, July 26 2016. US Patent 9,400,925. 8",2
Hierarchical Feature Embedding for Attribute Recognition,CVPR_2020,5,"Tianlong Chen, Shaojin Ding, Jingyi Xie, Ye Yuan, Wuyang Chen, Yang Yang, Zhou Ren, and Zhangyang Wang. Abd- net: Attentive but diverse person re-identiﬁcation. arXiv preprint arXiv:1908.01114 , 2019. 2",8
Unsupervised Intra-domain Adaptation for Semantic Segmentation,CVPR_2020,1,"Minghao Chen, Hongyang Xue, and Deng Cai. Do- main adaptation for semantic segmentation with maximum squares loss. In ICCV , pages 2090–2099, 2019. 2",3
Unsupervised Intra-domain Adaptation for Semantic Segmentation,CVPR_2020,2,"Christos Sakaridis, Dengxin Dai, Simon Hecker, and Luc Van Gool. Model adaptation with synthetic and real data for semantic dense foggy scene understanding. In ECCV , pages 687–704, 2018. 2",4
Unsupervised Intra-domain Adaptation for Semantic Segmentation,CVPR_2020,3,"Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Ki- hyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic seg- mentation. In CVPR , pages 7472–7481, 2018. 1,2,3,5,6, 7",6
Unsupervised Intra-domain Adaptation for Semantic Segmentation,CVPR_2020,4,"Yi-Hsuan Tsai, Kihyuk Sohn, Samuel Schulter, and Manmo- han Chandraker. Domain adaptation for structured output via discriminative patch representations. In ICCV , pages 1456– 1465, 2019. 1,2",4
Unsupervised Intra-domain Adaptation for Semantic Segmentation,CVPR_2020,5,"Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. InNeurIPS , pages 137–144, 2007. 7",4
Learning Better Lossless Compression Using Lossy Compression,CVPR_2020,1,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR , pages 770–778, 2016. 4,11",4
Learning Better Lossless Compression Using Lossy Compression,CVPR_2020,2,Workshop and Challenge on Learned Image Compression. https://www.compression.cc/challenge/ .6,1
Learning Better Lossless Compression Using Lossy Compression,CVPR_2020,3,"Eirikur Agustsson, Fabian Mentzer, Michael Tschannen, Lukas Cavigelli, Radu Timofte, Luca Benini, and Luc Van Gool. Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations. In NIPS , 2017. 1",7
Learning Better Lossless Compression Using Lossy Compression,CVPR_2020,4,"Eirikur Agustsson and Radu Timofte. NTIRE 2017 Chal- lenge on Single Image Super-Resolution: Dataset and Study. InCVPR Workshops , 2017. 6",1
Learning Better Lossless Compression Using Lossy Compression,CVPR_2020,5,"Eirikur Agustsson, Michael Tschannen, Fabian Mentzer, Radu Timofte, and Luc Van Gool. Generative Adversar- ial Networks for Extreme Learned Image Compression. In ICCV , 2019. 1",5
Blindly Assess Image Quality in the Wild Guided by A,CVPR_2020,1,Proposed Method,2
Blindly Assess Image Quality in the Wild Guided by A,CVPR_2020,2,IQA model ﬁrst and then present details of the three sub- networks in the following.,1
Blindly Assess Image Quality in the Wild Guided by A,CVPR_2020,3,1. Self­Adaptive IQA Model,2
Blindly Assess Image Quality in the Wild Guided by A,CVPR_2020,4,"Traditional deep learning based quality prediction mod- els receive an input image and directly map it to a quality score, the procedure can be described as follows: ϕ(x,θ) =q, (1) whereϕdenotes the network model, xis the input image, θ represents the weight parameters. Note that once the train- ing stage completes, weight parameters are ﬁxed for all test",1
Blindly Assess Image Quality in the Wild Guided by A,CVPR_2020,5,ResNet-50,2
Vehicle Re-Identiﬁcation in Multi-Camera scenarios based on Ensembling Deep,CVPR_2020,1,"Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classiﬁcation with deep convolutional neural net- works. In Advances in neural information processing sys- tems, pages 1097–1105, 2012.",3
Vehicle Re-Identiﬁcation in Multi-Camera scenarios based on Ensembling Deep,CVPR_2020,2,"Junaid Ahmed Ansari, Sarthak Sharma, Anshuman Majum- dar, J Krishna Murthy, and K Madhava Krishna. The earth ain’t ﬂat: Monocular reconstruction of vehicles on steep and graded roads from a moving camera. In IEEE/RSJ Interna- tional Conference on Intelligent Robots and Systems (IROS) , pages 8404–8410, 2018.",5
Vehicle Re-Identiﬁcation in Multi-Camera scenarios based on Ensembling Deep,CVPR_2020,3,"Relja Arandjelovi ´c and Andrew Zisserman. Three things ev- eryone should know to improve object retrieval. In IEEE Conference on Computer Vision and Pattern Recognition , pages 2911–2918, 2012.",1
Vehicle Re-Identiﬁcation in Multi-Camera scenarios based on Ensembling Deep,CVPR_2020,4,"Song Bai and Xiang Bai. Sparse contextual activation for ef- ﬁcient visual re-ranking. IEEE Transactions on Image Pro- cessing , 25(3):1056–1069, 2016.",1
Vehicle Re-Identiﬁcation in Multi-Camera scenarios based on Ensembling Deep,CVPR_2020,5,"Ondrej Chum, James Philbin, Josef Sivic, Michael Isard, and Andrew Zisserman. Total recall: Automatic query expan- sion with a generative feature model for object retrieval. In IEEE International Conference on Computer Vision , pages 1–8, 2007.",5
Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning,CVPR_2020,1,"Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certi- ﬁed adversarial robustness via randomized smoothing. arXiv preprint arXiv:1902.02918 , 2019. 8",3
Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning,CVPR_2020,2,"Alexey Dosovitskiy, Philipp Fischer, Jost Tobias Springen- berg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with exemplar convolutional neural networks. IEEE transactions on pattern analysis and machine intelligence , 38(9):1734–1747, 2015. 1,2",5
Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning,CVPR_2020,3,"Daniel Kang, Yi Sun, Dan Hendrycks, Tom Brown, and Jacob Steinhardt. Testing robustness against unforeseen adversaries. arXiv preprint arXiv:1908.08016 , 2019. 5,6",5
Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning,CVPR_2020,4,"Mehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision , pages 69–84. Springer, 2016. 1,2,3",1
Adversarial Robustness: From Self-Supervised Pre-Training to Fine-Tuning,CVPR_2020,5,"Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsu- pervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728 , 2018. 1,2,3",3
Global-Local Bidirectional Reasoning for Unsupervised Representation,CVPR_2020,1,"(“vote”) in gray as references. Besides, we show the supervised baselines of our models.",2
Global-Local Bidirectional Reasoning for Unsupervised Representation,CVPR_2020,2,Method #Points Supervised Acc.,2
Global-Local Bidirectional Reasoning for Unsupervised Representation,CVPR_2020,3,PointNet [ 37] 1k ✓ 89.2,2
Global-Local Bidirectional Reasoning for Unsupervised Representation,CVPR_2020,4,PointNet++ [ 38] 1k ✓ 90.5,2
Global-Local Bidirectional Reasoning for Unsupervised Representation,CVPR_2020,5,PointNet++ [ 38] (vote) 1k ✓ 90.7,2
Joint Filtering of Intensity Images and Neuromorphic Events for,CVPR_2020,1,"Wensheng Cheng, Hao Luo, Wen Yang, Lei Yu, Shoushun Chen, and Wei Li. DET: A high-resolution DVS dataset for lane extraction. In Proc. of Conference on Computer Vision and Pattern Recognition (CVPR) workshops , 2019. 2",6
Joint Filtering of Intensity Images and Neuromorphic Events for,CVPR_2020,2,"Patrick Llull, Xuejun Liao, Xin Yuan, Jianbo Yang, David Kittle, Lawrence Carin, Guillermo Sapiro, and David J Brady. Coded aperture compressive temporal imaging. Op- tics Express , 21(9):10526–10545, 2013. 2",8
Joint Filtering of Intensity Images and Neuromorphic Events for,CVPR_2020,3,"Mohammed Mutlaq Almatraﬁ and Keigo Hirakawa. Davis camera optical ﬂow. IEEE Transactions on Computational Imaging , 2019. 1",1
Joint Filtering of Intensity Images and Neuromorphic Events for,CVPR_2020,4,"Richard G Baraniuk, Thomas Goldstein, Aswin C Sankara- narayanan, Christoph Studer, Ashok Veeraraghavan, and Michael B Wakin. Compressive video sensing: algorithms, architectures, and applications. Signal Processing Magazine , 34(1):52–66, 2017. 2",6
Joint Filtering of Intensity Images and Neuromorphic Events for,CVPR_2020,5,"Souptik Barua, Yoshitaka Miyatani, and Ashok Veeraragha- van. Direct face detection and video reconstruction from event cameras. In Proc. of Winter Conference on Applica- tions of Computer Vision (WACV) , pages 1–9, 2016. 7,8",3
Revisiting the Evaluation of Uncertainty Estimation and Its Application to,CVPR_2020,1,"Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution examples in neural net- works. arXiv preprint arXiv:1610.02136 , 2016. 1,2,3,5, 7",1
Revisiting the Evaluation of Uncertainty Estimation and Its Application to,CVPR_2020,2,"Jay Heo, Hae Beom Lee, Saehoon Kim, Juho Lee, Kwang Joon Kim, Eunho Yang, and Sung Ju Hwang. Uncertainty-aware attention for reliable interpretation and prediction. In Advances in Neural Information Processing Systems , pages 917–926, 2018. 1,2,3",7
Revisiting the Evaluation of Uncertainty Estimation and Its Application to,CVPR_2020,3,"Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil- ian Q Weinberger. Densely connected convolutional net- works. In Proceedings of the IEEE conference on computer vision and pattern recognition , pages 4700–4708, 2017. 7",4
Revisiting the Evaluation of Uncertainty Estimation and Its Application to,CVPR_2020,4,"J Eric Bickel and Seong Dae Kim. Veriﬁcation of the weather channel probability of precipitation forecasts. Monthly Weather Review , 136(12):4867–4881, 2008. 6",1
Revisiting the Evaluation of Uncertainty Estimation and Its Application to,CVPR_2020,5,"Jochen Br ¨ocker and Leonard A Smith. Increasing the re- liability of reliability diagrams. Weather and forecasting , 22(3):651–661, 2007. 6",1
Explaining Failure: Investigation of Surprise and Expectation in CNNs,CVPR_2020,1,"Thomas Hartley, Kirill Sidorov, Christopher Willis, and David Marshall. Gradient weighted superpixels for inter- pretability in CNNs. In BMVC 2019: Workshop on Inter- pretable and Explainable Machine Vision , September 2019. 5",4
Explaining Failure: Investigation of Surprise and Expectation in CNNs,CVPR_2020,2,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 770–778, June 2016. 3",4
Explaining Failure: Investigation of Surprise and Expectation in CNNs,CVPR_2020,3,"Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. “Why should I trust you?”: Explaining the predictions of any classiﬁer. In 22nd ACM SIGKDD International Con- ference on Knowledge Discovery and Data Mining , pages 1135–1144, 2016. 1,2,5",3
Explaining Failure: Investigation of Surprise and Expectation in CNNs,CVPR_2020,4,"Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal- lenge. International Journal of Computer Vision (IJCV) , 115(3):211–252, 2015. 1,3",11
Explaining Failure: Investigation of Surprise and Expectation in CNNs,CVPR_2020,5,"Ramprasaath R. Selvaraju, Prithvijit Chattopadhyay, Mo- hamed Elhoseiny, Tilak Sharma, Dhruv Batra, Devi Parikh, and Stefan Lee. Choose your neuron: Incorporating domain knowledge through neuron-importance. In The European Conference on Computer Vision (ECCV) , pages 540–556, September 2018. 2",2
Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization,CVPR_2020,1,"Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. Technical report, Cite- seer, 2009. 5,7",3
Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization,CVPR_2020,2,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, and Aleksander Madry. Adversarially robust gener- alization requires more data. In Advances in Neural Infor- mation Processing Systems , pages 5014–5026, 2018. 1,2,3, 7",5
Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization,CVPR_2020,3,"Anish Athalye, Nicholas Carlini, and David Wagner. Ob- fuscated gradients give a false sense of security: Circum- venting defenses to adversarial examples. arXiv preprint arXiv:1802.00420 , 2018. 1,6",3
Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization,CVPR_2020,4,"Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial attacks: Reliable attacks against black-box machine learning models. arXiv preprint arXiv:1712.04248 , 2017. 5",3
Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization,CVPR_2020,5,"Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP) , pages 39–57. IEEE, 2017. 1,5",1
Multi-view Neural Human Rendering,CVPR_2020,1,"Marc Habermann, Weipeng Xu, Michael Zollhoefer, Ger- ard Pons-Moll, and Christian Theobalt. Livecap: Real-time human performance capture from monocular video. ACM Transactions on Graphics (TOG) , 38(2):14, 2019.",5
Multi-view Neural Human Rendering,CVPR_2020,2,"Richard A Newcombe, Shahram Izadi, Otmar Hilliges, David Molyneaux, David Kim, Andrew J Davison, Push- meet Kohli, Jamie Shotton, Steve Hodges, and Andrew W Fitzgibbon. Kinectfusion: Real-time dense surface mapping and tracking. In ISMAR , volume 11, pages 127–136, 2011.",10
Multi-view Neural Human Rendering,CVPR_2020,3,"Manuel M Oliveira. Image-based modeling and rendering techniques: A survey. Rita, 9(2):37–66, 2002.",1
Multi-view Neural Human Rendering,CVPR_2020,4,"Justus Thies, Michael Zollh ¨ofer, Christian Theobalt, Marc Stamminger, and Matthias Nießner. Ignor: Image-guided neural object rendering. arXiv preprint arXiv:1811.10720 , 2018.",5
Multi-view Neural Human Rendering,CVPR_2020,5,"Vincent Sitzmann, Justus Thies, Felix Heide, Matthias Nießner, Gordon Wetzstein, and Michael Zollhofer. Deep- voxels: Learning persistent 3d feature embeddings. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 2437–2446, 2019.",6
NTIRE 2020 Challenge on NonHomogeneous Dehazing,CVPR_2020,1,"C. Ancuti, C. O. Ancuti, M. Sbert, and R. Timofte. DENSE- HAZE: A benchmark for image dehazing with dense-haze and haze-free images. IEEE ICIP , 2019. 2,3",2
NTIRE 2020 Challenge on NonHomogeneous Dehazing,CVPR_2020,2,"C. O. Ancuti, C. Ancuti, C. De Vleeschouwer, and R. Tim- ofte. O-HAZE: a dehazing benchmark with real hazy and haze-free outdoor images. IEEE CVPR, NTIRE Workshop , 2018. 2,3,12",2
NTIRE 2020 Challenge on NonHomogeneous Dehazing,CVPR_2020,3,"Boaz Arad, Radu Timofte, Yi-Tun Lin, Graham Finlayson, Ohad Ben-Shahar, et al. NTIRE 2020 challenge on spectral reconstruction from an rgb image. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Work- shops , June 2020. 2",6
NTIRE 2020 Challenge on NonHomogeneous Dehazing,CVPR_2020,4,"B. Cai, X. Xu, K. Jia, C. Qing, and D. Tao. Dehazenet: An end-to-end system for single image haze removal. IEEE Transactions on Image Processing , 2016. 1",2
NTIRE 2020 Challenge on NonHomogeneous Dehazing,CVPR_2020,5,"Raanan Fattal. Single image dehazing. SIGGRAPH , 2008. 1",2
Fast Video Object Segmentation with Temporal Aggregation Network and,CVPR_2020,1,"Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for seman- tic image segmentation. arXiv preprint arXiv:1706.05587 , 2017. 1,2",4
Fast Video Object Segmentation with Temporal Aggregation Network and,CVPR_2020,2,"Qi Chu, Wanli Ouyang, Hongsheng Li, Xiaogang Wang, Bin Liu, and Nenghai Yu. Online multi-object tracking using cnn-based single object tracker with spatial-temporal atten- tion mechanism. In Proceedings of the IEEE International Conference on Computer Vision , pages 4836–4845, 2017. 2",6
Fast Video Object Segmentation with Temporal Aggregation Network and,CVPR_2020,3,"Linchao Bao, Baoyuan Wu, and Wei Liu. Cnn in mrf: Video object segmentation via inference in a cnn-based higher- order spatio-temporal mrf. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition , pages 5977–5986, 2018. 1,2,3",3
Fast Video Object Segmentation with Temporal Aggregation Network and,CVPR_2020,4,"Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking without bells and whistles. In Proceedings of the IEEE International Conference on Computer Vision . IEEE, 2019. 2,4",3
Fast Video Object Segmentation with Temporal Aggregation Network and,CVPR_2020,5,"Erik Bochinski, V olker Eiselein, and Thomas Sikora. High- speed tracking-by-detection without using image informa- tion. In 2017 14th IEEE International Conference on Ad- vanced Video and Signal Based Surveillance (AVSS) , pages 1–6. IEEE, 2017. 2",3
Meta-DermDiagnosis: Few-Shot Skin Disease Identiﬁcation using,CVPR_2020,1,Gregory R. Koch. Siamese neural networks for one-shot im- age recognition. 2015.,2
Meta-DermDiagnosis: Few-Shot Skin Disease Identiﬁcation using,CVPR_2020,2,"M. Z. Alom, C. Yakopcic, T. M. Taha, and V . K. Asari. Nu- clei segmentation with recurrent residual convolutional neu- ral networks based u-net (r2u-net). In NAECON 2018 - IEEE National Aerospace and Electronics Conference , pages 228– 233, July 2018.",2
Meta-DermDiagnosis: Few-Shot Skin Disease Identiﬁcation using,CVPR_2020,3,"D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y . Bengio. End-to-end attention-based large vocabulary speech recognition. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) , pages 4945–4949, March 2016.",2
Meta-DermDiagnosis: Few-Shot Skin Disease Identiﬁcation using,CVPR_2020,4,"Sihong Chen, Kai Ma, and Yefeng Zheng. Med3d: Transfer learning for 3d medical image analysis. ArXiv , abs/1904.00625, 2019.",3
Meta-DermDiagnosis: Few-Shot Skin Disease Identiﬁcation using,CVPR_2020,5,"Noel Codella, Veronica Rotemberg, Philipp Tschandl, M Emre Celebi, Stephen Dusza, David Gutman, Brian Helba, Aadi Kalloo, Konstantinos Liopyris, Michael Marchetti, et al. Skin lesion analysis toward melanoma detection 2018: A challenge hosted by the interna- tional skin imaging collaboration (isic). arXiv preprint arXiv:1902.03368 , 2019.",11
Learning to See Through Obstructions,CVPR_2020,1,"Yu Li and Michael S Brown. Single image layer separation using relative smoothness. In CVPR , 2014. 2",1
Learning to See Through Obstructions,CVPR_2020,2,"Jean-Baptiste Alayrac, Joao Carreira, and Andrew Zisser- man. The visual centrifuge: Model-free layered video repre- sentations. In CVPR , 2019. 1,2,6,7,8",3
Learning to See Through Obstructions,CVPR_2020,3,"Nikolaos Arvanitopoulos, Radhakrishna Achanta, and Sabine Susstrunk. Single image reﬂection suppression. In CVPR , 2017. 1,2",3
Learning to See Through Obstructions,CVPR_2020,4,"Efrat Be’Ery and Arie Yeredor. Blind separation of superim- posed shifted images using parameterized joint diagonaliza- tion. TIP, 17(3):340–353, 2008. 1",1
Learning to See Through Obstructions,CVPR_2020,5,"Sean Bell, Kavita Bala, and Noah Snavely. Intrinsic images in the wild. ACM TOG , 33(4):159, 2014. 2",3
End-to-End Model-Free Reinforcement Learning,CVPR_2020,1,"Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316 , 2016. 1",11
End-to-End Model-Free Reinforcement Learning,CVPR_2020,2,"Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance for direct percep- tion in autonomous driving. In Proceedings of the IEEE International Conference on Computer Vision , pages 2722– 2730, 2015. 1,2",4
End-to-End Model-Free Reinforcement Learning,CVPR_2020,3,"Will Dabney, Georg Ostrovski, David Silver, and R ´emi Munos. Implicit quantile networks for distributional rein- forcement learning. arXiv preprint arXiv:1806.06923 , 2018. 3,4,5,6",4
End-to-End Model-Free Reinforcement Learning,CVPR_2020,4,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition, 2015. 2,4,5",4
End-to-End Model-Free Reinforcement Learning,CVPR_2020,5,"V olodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on ma- chine learning , pages 1928–1937, 2016. 1,2,4",8
Low-rate Image Compression with Super-resolution Learning,CVPR_2020,1,"Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim- ing He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 7794–7803, 2018. 2",4
Low-rate Image Compression with Super-resolution Learning,CVPR_2020,2,"Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multi- scale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Sys- tems & Computers, 2003 , volume 2, pages 1398–1402. Ieee, 2003. 3",3
Low-rate Image Compression with Super-resolution Learning,CVPR_2020,3,"Feng Jiang, Wen Tao, Shaohui Liu, Jie Ren, Xun Guo, and Debin Zhao. An end-to-end compression framework based on convolutional neural networks. IEEE Transactions on Circuits and Systems for Video Technology , 28(10):3007– 3018, 2017. 2",6
Low-rate Image Compression with Super-resolution Learning,CVPR_2020,4,"Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 , 2014. 3",1
Low-rate Image Compression with Super-resolution Learning,CVPR_2020,5,"Yue Li, Dong Liu, Houqiang Li, Li Li, Zhu Li, and Feng Wu. Learning a convolutional neural network for image compact-resolution. IEEE Transactions on Image Process- ing, 28(3):1092–1107, 2019. 1,2,3",6
"Select, Supplement and Focus for RGB-D Saliency Detection",CVPR_2020,1,"Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. Human attention in visual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding , 163:90–100, 2017.",5
"Select, Supplement and Focus for RGB-D Saliency Detection",CVPR_2020,2,"Jinming Su, Jia Li, Changqun Xia, and Yonghong Tian. Selectivity or invariance: Boundary-aware salient object detection. arXiv preprint arXiv:1812.10066 , 2018.",4
"Select, Supplement and Focus for RGB-D Saliency Detection",CVPR_2020,3,"Zhe Wu, Li Su, and Qingming Huang. Cascaded partial decoder for fast and accurate salient object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3907–3916, 2019.",3
"Select, Supplement and Focus for RGB-D Saliency Detection",CVPR_2020,4,"Pingping Zhang, Dong Wang, Huchuan Lu, Hongyu Wang, and Xiang Ruan. Amulet: Aggregating multi-level convo- lutional features for salient object detection. In Proceedings of the IEEE International Conference on Computer Vision , pages 202–211, 2017.",5
"Select, Supplement and Focus for RGB-D Saliency Detection",CVPR_2020,5,"An effective graph and depth layer based rgb-d image foreground object extraction method. Computational visual media , (4):85–91.",1
Deﬂating Dataset Bias Using Synthetic Data Augmentation,CVPR_2020,1,"Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars Mescheder, An- dreas Geiger, and Carsten Rother. Augmented reality meets com- puter vision: Efﬁcient data generation for urban driving scenes. In- ternational Journal of Computer Vision , 126(9):961–972, 2018. 2",5
Deﬂating Dataset Bias Using Synthetic Data Augmentation,CVPR_2020,2,"Bharath Bhushan Damodaran, Benjamin Kellenberger, R ´emi Fla- mary, Devis Tuia, and Nicolas Courty. Deepjdot: Deep joint distribu- tion optimal transport for unsupervised domain adaptation. In Pro- ceedings of the European Conference on Computer Vision (ECCV) , pages 447–463, 2018. 2",5
Deﬂating Dataset Bias Using Synthetic Data Augmentation,CVPR_2020,3,"Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh V ora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. arXiv preprint arXiv:1903.11027 , 2019. 8",10
Deﬂating Dataset Bias Using Synthetic Data Augmentation,CVPR_2020,4,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei- Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition , pages 248–255. Ieee, 2009. 1,4",6
Deﬂating Dataset Bias Using Synthetic Data Augmentation,CVPR_2020,5,"Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. arXiv preprint arXiv:1711.03938 , 2017. 8",5
Noise Robust Generative Adversarial Networks,CVPR_2020,1,"Michael Elad and Michal Aharon. Image denoising via sparse and redundant representations over learned dictionar- ies.IEEE Trans. Image Process. , 15(12):3736–3745, 2006.",1
Noise Robust Generative Adversarial Networks,CVPR_2020,2,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS , 2014.",8
Noise Robust Generative Adversarial Networks,CVPR_2020,3,"Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, G ¨unter Klambauer, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a Nash equilibrium. In NIPS , 2017.",6
Noise Robust Generative Adversarial Networks,CVPR_2020,4,"Ishan Jindal, Matthew Nokleby, and Xuewen Chen. Learning deep networks from noisy labels with dropout regularization. InICDM , 2016.",3
Noise Robust Generative Adversarial Networks,CVPR_2020,5,"Takuhiro Kaneko, Yoshitaka Ushiku, and Tatsuya Harada. Class-distinct and class-mutual image generation with GANs. In BMVC , 2019.",3
GrappaNet: Combining Parallel Imaging with Deep Learning for Multi-Coil,CVPR_2020,1,"Mark A. Griswold, Peter M. Jakob, Robin M. Heide-mann, Mathias Nittka, Vladimir Jellus, Jianmin Wang, Berthold Kiefer, and Axel Haase. Generalized auto- calibrating partially parallel acquisitions (GRAPPA). Magnetic Resonance in Medicine , 47(6):1202–1210, 2002. 1,2",2
GrappaNet: Combining Parallel Imaging with Deep Learning for Multi-Coil,CVPR_2020,2,"Yoseob Han and Jong Chul Ye. Framing U-Net via deep convolutional framelets: Application to sparse- view CT. IEEE Transactions on Medical Imaging , 37 (6), 2018. 3",1
GrappaNet: Combining Parallel Imaging with Deep Learning for Multi-Coil,CVPR_2020,3,"Florian Knoll, Kerstin Hammernik, Chi Zhang, S. Möller, Thomas Pock, Daniel K. Sodickson, and Mehmet Akçakaya. Deep learning methods for paral- lel magnetic resonance image reconstruction. CoRR , abs/1904.01112, 2019. 3",4
GrappaNet: Combining Parallel Imaging with Deep Learning for Multi-Coil,CVPR_2020,4,"Kyong Hwan Jin, Michael T. McCann, Emmanuel Froustey, and Michael Unser. Deep convolutional neu- ral network for inverse problems in imaging. CoRR , abs/1611.03679, 2016. URL http://arxiv.org/ abs/1611.03679 .3 14321",2
GrappaNet: Combining Parallel Imaging with Deep Learning for Multi-Coil,CVPR_2020,5,"Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012. 6",1
L2UWE: A Framework for the Efﬁcient Enhancement of Low-Light Underwater,CVPR_2020,1,"Tali Treibitz and Yoav Y Schechner. Polarization: Beneﬁcial for visibility enhancement? In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pages 525–532. IEEE, 2009. 2",1
L2UWE: A Framework for the Efﬁcient Enhancement of Low-Light Underwater,CVPR_2020,2,"H.-Y . Yang, P.-Y . Chen, C.-C. Huang, Y .-Z. Zhuang, and Y .- H. Shiau. Low complexity underwater image enhancement based on dark channel prior. In Second International Con- ference on Innovations in Bio-inspired Computing and Ap- plications (IBICA) , pages 17–20. IEEE, 2011. 2",2
L2UWE: A Framework for the Efﬁcient Enhancement of Low-Light Underwater,CVPR_2020,3,"K. He, J. Sun, and X. Tang. Guided image ﬁltering. IEEE transactions on pattern analysis & machine intelligence , 35(6):1397–1409, 2013. 6",2
L2UWE: A Framework for the Efﬁcient Enhancement of Low-Light Underwater,CVPR_2020,4,"Codruta Orniana Ancuti and Cosmin Ancuti. Single image dehazing by multi-scale fusion. IEEE Transactions on Image Processing , 22(8):3271–3282, 2013. 1,2,4,6",1
L2UWE: A Framework for the Efﬁcient Enhancement of Low-Light Underwater,CVPR_2020,5,"W. Ren, S. Liu, H. Zhang, J. Pan, X. Cao, and M.-H. Yang. Single image dehazing via multi-scale convolutional neural networks. In European conference on computer vision , pages 154–169. Springer, 2016. 2",2
Automated Depth Video Monitoring For Fall Reduction : A Case Study,CVPR_2020,1,"D. Venema, A. Skinner, R. Nailon, D. Conley, R. High, and K. J. Jones. Patient and system factors associated with unas- sisted and injurious falls: An observational study. BMC Geriatrics , 9(1):348, 2019. 1",2
Automated Depth Video Monitoring For Fall Reduction : A Case Study,CVPR_2020,2,"P. Bauer, J. Brown Kramer, B. Rush, and L. Sabalka. Mod- eling bed exit likelihood in a camera-based automated video monitoring application. In 2017 IEEE International Confer- ence on Electro Information Technology (EIT) , pages 56–61, 2017. 1",2
Automated Depth Video Monitoring For Fall Reduction : A Case Study,CVPR_2020,3,"D. Boswell, J. Ramsey, M. Smith, and B. Wagers. The cost- effectiveness of a patient-sitter program in an acute care hos- pital: A test of the impact of sitters on the incidence of falls and patient satisfaction. Quality Management in Healthcare , 10(1):10–16, 2001. 2",2
Automated Depth Video Monitoring For Fall Reduction : A Case Study,CVPR_2020,4,"E.D. Bouldin, E.M. Andresen, N.E. Dunton, M. Simon, T.M. Waters, M. Liu, M.J. Daniels, L.C. Mion, and R.I. Shorr. Falls among adult patients hospitalized in the united states: prevalence and trends. Journal of Patient Safety , 9(1):13–17, 2013. 1",2
Automated Depth Video Monitoring For Fall Reduction : A Case Study,CVPR_2020,5,"K. Bradley. Remote video monitoring: A novel approach in fall prevention. Journal of Continuing Education in Nursing , 9(11):29–35, 2016. 2",2
Adaptive Loss-aware Quantization for Multi-bit Networks,CVPR_2020,1,"Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El- Yaniv, and Yoshua Bengio. Binarized neural networks: Train- ing deep neural networks with weights and activations con- strained to +1 or -1. arXiv preprint arXiv:1602.02830 , 2016.",5
Adaptive Loss-aware Quantization for Multi-bit Networks,CVPR_2020,2,"Julian Faraone, Nicholas J. Fraser, Michaela Blott, and Philip Heng Wai Leong. SYQ: learning symmetric quantization for efﬁcient deep neural networks. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition , pages 4300–4309, 2018.",2
Adaptive Loss-aware Quantization for Multi-bit Networks,CVPR_2020,3,"Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El- Yaniv, and Yoshua Bengio. Quantized neural networks: Train- ing neural networks with low precision weights and activa- tions. Journal of Machine Learning Research , 18(187):1–30, 2017.",5
Adaptive Loss-aware Quantization for Multi-bit Networks,CVPR_2020,4,"Sangil Jung, Changyong Son, Seohyung Lee, Jinwoo Son, Jae- Joon Han, Youngjun Kwak, Sung Ju Hwang, and Changkyu Choi. Learning to quantize deep networks by optimizing quantization intervals with task loss. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition , pages 4350–4359, 2019.",8
Adaptive Loss-aware Quantization for Multi-bit Networks,CVPR_2020,5,"Fengfu Li, Bo Zhang, and Bin Liu. Ternary weight networks. InProceedings of Advances in Neural Information Processing Systems , 2016.",3
Learningnanoscale motionpatterns of vesiclesin livingcells,CVPR_2020,1,"T. Dertinger, R. Colyer, G. Iyer, S. Weiss, and J. Enderlein. Fast, background-free, 3D super-resolution optical ﬂuctua- tion imaging (soﬁ). Proceedings of the National Academy of Sciences,106(52):22287–22292, 2009. 2",2
Learningnanoscale motionpatterns of vesiclesin livingcells,CVPR_2020,2,"K.AgarwalandR.Macháň. Multiplesignalclassiﬁcational- gorithm for super-resolution ﬂuorescence microscopy. Na- tureCommunications ,7:13752,2016. 2,3,4",2
Learningnanoscale motionpatterns of vesiclesin livingcells,CVPR_2020,3,"H. Al-Obaidi, B. Nasseri, and A. T. Florence. Dynamics of microparticles inside lipid vesicles: movement in conﬁned spaces.JournalofDrugTargeting ,18(10):821–830, 2010. 3",2
Learningnanoscale motionpatterns of vesiclesin livingcells,CVPR_2020,4,"E. Alexander, Q. Guo, S. Koppal, S. Gortler, and T. Zickler. Focal ﬂow: Measuring distance and velocity with defocus anddiﬀerentialmotion.In EuropeanConferenceonComputer Vision,pages667–682, 2016. 2",2
Learningnanoscale motionpatterns of vesiclesin livingcells,CVPR_2020,5,"F. Baradel, N. Neverova, J. Mille, G. Mori, and C. Wolf. Co- phy: Counterfactual learning of physical dynamics. arXiv preprintarXiv:1909.12000 ,2019.2",2
Self-Supervised Scene De-occlusion,CVPR_2020,1,"Kai Chen, Jiaqi Wang, Shuo Yang, Xingcheng Zhang, Yuan- jun Xiong, Chen Change Loy, and Dahua Lin. Optimizing video object detection via a scale-time lattice. In CVPR , June 2018.",7
Self-Supervised Scene De-occlusion,CVPR_2020,2,"Ziwei Liu, Xiaoxiao Li, Ping Luo, Chen-Change Loy, and Xiaoou Tang. Semantic image segmentation via deep parsing network. In ICCV , 2015.",5
Self-Supervised Scene De-occlusion,CVPR_2020,3,"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, and Piotr Doll ´ar. Microsoft coco: Common objects in context. In ECCV , 2014.",7
Self-Supervised Scene De-occlusion,CVPR_2020,4,"Hongwei Lin, Zihao Wang, Panpan Feng, Xingjiang Lu, and Jinhui Yu. A computational model of topological and geo- metric recovery for visual curve completion. Computational Visual Media , 2(4):329–342, 2016.",5
Self-Supervised Scene De-occlusion,CVPR_2020,5,"Gaetano Kanizsa. Organization in vision: Essays on Gestalt perception . Praeger Publishers, 1979.",2
Discovering Human Interactions with Novel Objects via Zero-Shot Learning,CVPR_2020,1,"Tanmay Gupta, Alexander Schwing, and Derek Hoiem. No- frills human-object interaction detection: Factorization, lay- out encodings, and training techniques. In The IEEE Inter- national Conference on Computer Vision (ICCV) , 2019. 2, 3,7",3
Discovering Human Interactions with Novel Objects via Zero-Shot Learning,CVPR_2020,2,"Georgia Gkioxari, Ross Girshick, Piotr Dollr, and Kaiming He. Detecting and recognizing human-object interactions. In CVPR , 2018. 1,2,3,6,7,8",4
Discovering Human Interactions with Novel Objects via Zero-Shot Learning,CVPR_2020,3,"T. Lin, P. Dollr, R. Girshick, K. He, B. Hariharan, and S. Belongie. Feature pyramid networks for object detection. In CVPR , 2017. 1,2,3,5,6",2
Discovering Human Interactions with Novel Objects via Zero-Shot Learning,CVPR_2020,4,"Ankan Bansal, Sai Saketh Rambhatla, Abhinav Shrivastava, and Rama Chellappa. Detecting human-object interactions via functional generalization. 2020. 1,2",4
Discovering Human Interactions with Novel Objects via Zero-Shot Learning,CVPR_2020,5,"Ankan Bansal, Karan Sikka, Gaurav Sharma, Rama Chel- lappa, and Ajay Divakaran. Zero-shot object detection. In ECCV , 2018. 2,3,5,6,7",5
ECA-Net: Efﬁcient Channel Attention for Deep Convolutional Neural Networks,CVPR_2020,1,"Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B. Girshick. Mask R-CNN. In ICCV , pages 2980–2988, 2017.",4
ECA-Net: Efﬁcient Channel Attention for Deep Convolutional Neural Networks,CVPR_2020,2,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In ECCV , 2016.",4
ECA-Net: Efﬁcient Channel Attention for Deep Convolutional Neural Networks,CVPR_2020,3,"Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea Vedaldi. Gather-excite: Exploiting feature context in convo- lutional neural networks. In NeurIPS , 2018.",5
ECA-Net: Efﬁcient Channel Attention for Deep Convolutional Neural Networks,CVPR_2020,4,"Tsung-Yi Lin, Piotr Doll ´ar, Ross B. Girshick, Kaiming He, Bharath Hariharan, and Serge J. Belongie. Feature pyramid networks for object detection. In CVPR , pages 936–944, 2017.",3
ECA-Net: Efﬁcient Channel Attention for Deep Convolutional Neural Networks,CVPR_2020,5,"Tsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll ´ar. Focal loss for dense object detection. In ICCV , 2017.",3
Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data,CVPR_2020,1,"David Acuna, Amlan Kar, and Sanja Fidler. Devil is in the edges: Learning semantic boundaries from noisy annota- tions. In CVPR , 2019. 2",3
Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data,CVPR_2020,2,"Minmin Chen, Kilian Q Weinberger, Zhixiang Xu, and Fei Sha. Marginalizing stacked linear denoising autoencoders. Journal of Machine Learning Research , 16(1):3849–3875, 2015. 6,8",4
Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data,CVPR_2020,3,"Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR , 2012. 1",3
Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data,CVPR_2020,4,"Gabriela Csurka. Domain adaptation for visual applications: A comprehensive survey. arXiv preprint arXiv:1702.05374 , 2017. 2",2
Neural Data Server: A Large-Scale Search Engine for Transfer Learning Data,CVPR_2020,5,"Sam Gross, Marc’Aurelio Ranzato, and Arthur Szlam. Hard mixtures of experts for large scale weakly supervised vision. InCVPR , pages 6865–6873, 2017. 4",3
Meta-Learning for Few-Shot Land Cover Classiﬁcation,CVPR_2020,1,"Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning (ICML) , pages 1126–1135, 2017.",3
Meta-Learning for Few-Shot Land Cover Classiﬁcation,CVPR_2020,2,"B. Liu, X. Yu, A. Yu, P. Zhang, G. Wan, and R. Wang. Deep few-shot learning for hyperspectral image classiﬁcation. IEEE Transactions on Geoscience and Remote Sensing , 57(4):2290–2304, 2019.",2
Meta-Learning for Few-Shot Land Cover Classiﬁcation,CVPR_2020,3,"D. Alajaji and H. Alhichri. Few shot scene classiﬁcation in remote sensing using meta-agnostic machine. In 2020 6th Conference on Data Science and Machine Learning Applications (CDMA) , pages 77–80, 2020.",2
Meta-Learning for Few-Shot Land Cover Classiﬁcation,CVPR_2020,4,"Antreas Antoniou, Harrison Edwards, and Amos Storkey. How to train your MAML. arXiv preprint arXiv:1810.09502 , 2018.",3
Meta-Learning for Few-Shot Land Cover Classiﬁcation,CVPR_2020,5,"Yoshua Bengio, Samy Bengio, and Jocelyn Cloutier. Learning a synaptic learning rule . Universit ´e de Montr ´eal, D ´epartement d’informatique et de recherche . . . , 1990.",3
Grid-GCN for Fast and Scalable Point Cloud Learning,CVPR_2020,1,"I. Armeni, A. Sax, A. R. Zamir, and S. Savarese. Joint 2D- 3D-Semantic Data for Indoor Scene Understanding. ArXiv e-prints , Feb. 2017. 2,6,8",2
Grid-GCN for Fast and Scalable Point Cloud Learning,CVPR_2020,2,"Matan Atzmon, Haggai Maron, and Yaron Lipman. Point convolutional neural networks by extension operators. arXiv preprint arXiv:1803.10091 , 2018. 2,7",3
Grid-GCN for Fast and Scalable Point Cloud Learning,CVPR_2020,3,"Yizhak Ben-Shabat, Michael Lindenbaum, and Anath Fis- cher. 3dmfv: Three-dimensional point cloud classiﬁcation in real-time using convolutional neural networks. IEEE Robotics and Automation Letters , 3(4):3145–3152, 2018. 7",3
Grid-GCN for Fast and Scalable Point Cloud Learning,CVPR_2020,4,"Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Generative and discriminative voxel mod- eling with convolutional neural networks. arXiv preprint arXiv:1608.04236 , 2016. 2",4
Grid-GCN for Fast and Scalable Point Cloud Learning,CVPR_2020,5,"Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pages 3075–3084, 2019. 7",3
SILA: An Incremental Learning Approach for,CVPR_2020,1,"F. Bartoli, G. Lisanti, L. Ballan, and A. Del Bimbo, “Context-aware trajectory prediction,” in 2018 24th Interna- tional Conference on Pattern Recognition (ICPR) , pp. 1941– 1946, IEEE, 2018.",2
SILA: An Incremental Learning Approach for,CVPR_2020,2,"A. Mohamed, K. Qian, M. Elhoseiny, and C. Claudel, “Social-stgcnn: A social spatio-temporal graph convolu- tional neural network for human trajectory prediction,” arXiv preprint arXiv:2002.11927 , 2020.",2
SILA: An Incremental Learning Approach for,CVPR_2020,3,"Y . F. Chen, M. Liu, and J. P. How, “Augmented dictionary learning for motion prediction,” in 2016 IEEE International Conference on Robotics and Automation (ICRA) , pp. 2527– 2534, IEEE, 2016.[14] E. Snelson and Z. Ghahramani, “Sparse gaussian processes using pseudo-inputs,” in Advances in neural information processing systems , pp. 1257–1264, 2006.",2
SILA: An Incremental Learning Approach for,CVPR_2020,4,"F. Schneemann and P. Heinemann, “Context-based detection of pedestrian crossing intention for autonomous driving in urban environments,” in 2016 IEEE/RSJ International Con- ference on Intelligent Robots and Systems (IROS) , pp. 2243– 2248, IEEE, 2016.",2
SILA: An Incremental Learning Approach for,CVPR_2020,5,"J. F. P. Kooij, N. Schneider, F. Flohr, and D. M. Gavrila, “Context-based pedestrian path prediction,” in European Conference on Computer Vision , pp. 618–633, Springer, 2014.",2
Hierarchically Robust Representation Learning,CVPR_2020,1,"Robert S. Chen, Brendan Lucier, Yaron Singer, and Vasilis Syrgkanis. Robust optimization for non- convex objectives. In NeurIPS , pages 4708–4717, 2017.",2
Hierarchically Robust Representation Learning,CVPR_2020,2,"Corinna Cortes and Vladimir Vapnik. Support-vector networks. Machine Learning , 20(3):273–297, 1995.",1
Hierarchically Robust Representation Learning,CVPR_2020,3,"John C. Duchi, Shai Shalev-Shwartz, Yoram Singer, and Tushar Chandra. Efﬁcient projections onto the l1- ball for learning in high dimensions. In ICML , pages 272–279, 2008.",2
Hierarchically Robust Representation Learning,CVPR_2020,4,Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.,2
Hierarchically Robust Representation Learning,CVPR_2020,5,"Qi Qian, Rong Jin, Shenghuo Zhu, and Yuanqing Lin. Fine-grained visual categorization via multi-stage metric learning. In CVPR , pages 3716–3724, 2015.",4
FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation,CVPR_2020,1,"Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alexei A. Efros, and Sergey Levine. Few-shot segmentation propaga- tion with guided networks. In ICLR Workshop , 2018. 1,3, 5,6",4
FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation,CVPR_2020,2,"Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In TPAMI , 2017. 3",3
FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation,CVPR_2020,3,"Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation. InBMVC , 2017. 1,3,5,6",5
FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation,CVPR_2020,4,"Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR , 2015. 5",5
FSS-1000: A 1000-Class Dataset for Few-Shot Segmentation,CVPR_2020,5,"Diederik P. Kingma and Jimmy Lei Ba. Adam: A Method for Stochastic Optimization. In ICLR , 2015. 5",2
Select to Better Learn: Fast and Accurate Deep Learning using Data Selection,CVPR_2020,1,"Christos Boutsidis, Michael W Mahoney, and Petros Drineas. An improved approximation algorithm for the col- umn subset selection problem. In Proceedings of the twen- tieth annual ACM-SIAM symposium on Discrete algorithms , pages 968–977. SIAM, 2009.",3
Select to Better Learn: Fast and Accurate Deep Learning using Data Selection,CVPR_2020,2,"Tony F Chan and Per Christian Hansen. Some applications of the rank revealing qr factorization. SIAM Journal on Sci- entiﬁc and Statistical Computing , 13(3):727–741, 1992.",1
Select to Better Learn: Fast and Accurate Deep Learning using Data Selection,CVPR_2020,3,"Amit Deshpande, Luis Rademacher, Santosh Vempala, and Grant Wang. Matrix approximation and projective cluster- ing via volume sampling. In Proceedings of the seventeenth annual ACM-SIAM symposium on Discrete algorithm , pages 1117–1126. Society for Industrial and Applied Mathematics, 2006.",4
Select to Better Learn: Fast and Accurate Deep Learning using Data Selection,CVPR_2020,4,"Saurabh Paul, Malik Magdon-Ismail, and Petros Drineas. Column selection via adaptive sampling. In Advances in neu- ral information processing systems , pages 406–414, 2015.",3
Select to Better Learn: Fast and Accurate Deep Learning using Data Selection,CVPR_2020,5,"Christopher M Bishop. Pattern recognition and machine learning . springer, 2006.",1
Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via,CVPR_2020,1,"Liang-Chieh Chen, Jonathan T Barron, George Papandreou, Kevin Murphy, and Alan L Yuille. Semantic image seg- mentation with task-speciﬁc edge detection using cnns and a discriminatively trained domain transform. In Proc. of the IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) , pages 4545–4554, 2016. 3",5
Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via,CVPR_2020,2,"Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 3146–3154, 2019. 3,5,7",7
Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via,CVPR_2020,3,"Wei Liu, Andrew Rabinovich, and Alexander C Berg. Parsenet: Looking wider to see better. In CoRR . Citeseer, 2015. 3,6",3
Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via,CVPR_2020,4,"Hassan Alhaija, Siva Mustikovela, Lars Mescheder, An- dreas Geiger, and Carsten Rother. Augmented reality meets computer vision: Efﬁcient data generation for urban driving scenes. International Journal of Computer Vision (IJCV) , 2018. 1,3",5
Cars Can’t Fly up in the Sky: Improving Urban-Scene Segmentation via,CVPR_2020,5,"Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE Transactions on Pattern Anal- ysis and Machine Intelligence (TPAMI) , 39(12):2481–2495, 2017. 1,3",3
Cross-modal Deep Face Normals with Deactivable Skip Connections,CVPR_2020,1,"Pablo Garrido, Levi Valgaerts, Chenglei Wu, and Christian Theobalt. Reconstructing detailed dynamic face geometry from monocular video. ACM Transactions on Graphics , 2013. 2",4
Cross-modal Deep Face Normals with Deactivable Skip Connections,CVPR_2020,2,"The bjut-3d large-scale chinese face database. 3,5",2
Cross-modal Deep Face Normals with Deactivable Skip Connections,CVPR_2020,3,"Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, and Marcus Magnor. Tex2shape: Detailed full human body geometry from a single image. In Proceedings of the IEEE International Conference on Computer Vision , 2019. 3",4
Cross-modal Deep Face Normals with Deactivable Skip Connections,CVPR_2020,4,"Andrew D. Bagdanov, Alberto Del Bimbo, and Iacopo Masi. The ﬂorence 2d/3d hybrid face dataset. In ACM Workshop on Human Gesture and Behavior Understanding , 2011. 2,5, 6,8",2
Cross-modal Deep Face Normals with Deactivable Skip Connections,CVPR_2020,5,"Aayush Bansal, Bryan Russell, and Abhinav Gupta. Marr revisited: 2d-3d alignment via surface normal prediction. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , 2016. 3,5,6",3
Photoplethysmography based stratiﬁcation of blood pressure using multi,CVPR_2020,1,"M. Abadi, P. Barham, and J. Chen. Tensorﬂow: A system for large-scale machine learning. 2016. 5",2
Photoplethysmography based stratiﬁcation of blood pressure using multi,CVPR_2020,2,"J. Allen. Photoplethysmography and its application in clini- cal physiological measurement. Physiol. Meas. , 28(3):1–22, 2007. 1",2
Photoplethysmography based stratiﬁcation of blood pressure using multi,CVPR_2020,3,"K. W. Chan, k. hung, and Y . T. Zhang. Noninvasive and cufﬂess measurements of blood pressure for telemedicine. In Proceedings of the 23rd Annual International Conference of the IEEE Engineering in Medicine and Biology Society , pages 3592–3593, 2001. 1",2
Photoplethysmography based stratiﬁcation of blood pressure using multi,CVPR_2020,4,"A. V . Chobanian, G. L. Bakris, H. R. Black, W. C. Cushman, L. A. Green, J. L. Izzo, and D. W. Jones. Seventh report of the joint national committee on prevention, detection, eval- uation, and treatment of high blood pressure. Hypertension , 289(19):1206–1252, 2003. 3",2
Photoplethysmography based stratiﬁcation of blood pressure using multi,CVPR_2020,5,"J. Dey, A. Gaurav, and V . N. Tiwari. Instabp: Cuff-less blood pressure monitoring on smartphone using single ppg sensor. 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society , pages 5002– 5005, 2018. 2,6",2
Maintaining Discrimination and Fairness in Class Incremental Learning,CVPR_2020,1,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems , pages 2672–2680, 2014. 2",8
Maintaining Discrimination and Fairness in Class Incremental Learning,CVPR_2020,2,"Jathushan Rajasegaran, Munawar Hayat, Salman Khan, Fa- had Shahbaz Khan, and Ling Shao. Random path selection for incremental learning. ArXiv , abs/1906.01120, 2019. 7",5
Maintaining Discrimination and Fairness in Class Incremental Learning,CVPR_2020,3,"Yue Wu, Yinpeng Chen, Lijuan Wang, Yuancheng Ye, Zicheng Liu, Yandong Guo, Zhengyou Zhang, and Yun Fu. Incremental classiﬁer learning with generative adversarial networks. ArXiv , abs/1802.00853, 2018. 1,2",8
Maintaining Discrimination and Fairness in Class Incremental Learning,CVPR_2020,4,"Rahaf Aljundi, Francesca Babiloni, Mohamed Elhoseiny, Marcus Rohrbach, and Tinne Tuytelaars. Memory aware synapses: Learning what (not) to forget. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 139–154, 2018. 1,2",5
Maintaining Discrimination and Fairness in Class Incremental Learning,CVPR_2020,5,"Mart ´ın Arjovsky, Soumith Chintala, and L ´eon Bottou. Wasserstein gan. ArXiv , abs/1701.07875, 2017. 5",3
Speech2Action: Cross-modal Supervision for Action Recognition,CVPR_2020,1,"Rohit Girdhar, Du Tran, Lorenzo Torresani, and Deva Ra- manan. Distinit: Learning video representations without a single labeled video. ICCV , 2019. 7",4
Speech2Action: Cross-modal Supervision for Action Recognition,CVPR_2020,2,"Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl V on- drick, Josh McDermott, and Antonio Torralba. The sound of pixels. In Proceedings of the European Conference on Computer Vision (ECCV) , pages 570–586, 2018. 2",6
Speech2Action: Cross-modal Supervision for Action Recognition,CVPR_2020,3,"David R Winer and R Michael Young. Automated screen- play annotation for extracting storytelling knowledge. In Thirteenth Artiﬁcial Intelligence and Interactive Digital En- tertainment Conference , 2017. 3",1
Speech2Action: Cross-modal Supervision for Action Recognition,CVPR_2020,4,"Bruno Korbar, Du Tran, and Lorenzo Torresani. Cooperative learning of audio and video models from self-supervised syn- chronization. In Advances in Neural Information Processing Systems , pages 7763–7774, 2018. 2,7",3
Speech2Action: Cross-modal Supervision for Action Recognition,CVPR_2020,5,"Relja Arandjelovic and Andrew Zisserman. Look, listen and learn. In Proceedings of the IEEE International Conference on Computer Vision , pages 609–617, 2017. 2",1
Learning to Detect Important People in Unlabelled Images for,CVPR_2020,1,"Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient semi-supervised learning method for deep neural networks. InInternational Conference on Machine Learning Workshop on Challenges in Representation Learning , 2013.",2
Learning to Detect Important People in Unlabelled Images for,CVPR_2020,2,"Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. In Advances in neural information processing systems , 2017.",1
Learning to Detect Important People in Unlabelled Images for,CVPR_2020,3,"Alexander C Berg, Tamara L Berg, Hal Daume, Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Mensch, Margaret Mitchell, Aneesh Sood, Karl Stratos, et al. Understanding and predicting importance in images. In Computer Vision and Pattern Recognition , 2012.",11
Learning to Detect Important People in Unlabelled Images for,CVPR_2020,4,"Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. Transactions on Neural Networks , 20(3):542–542, 2009.",3
Learning to Detect Important People in Unlabelled Images for,CVPR_2020,5,"Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej Chum. Label propagation for deep semi-supervised learning. InComputer Vision and Pattern Recognition , 2019.",4
Lossy Compression with Distortion Constrained Optimization,CVPR_2020,1,"Nick Johnston, Damien Vincent, David Minnen, Michele Covell, Saurabh Singh, Troy Chinen, Sung Jin Hwang, Joel Shor, and George Toderici. Improved lossy image com- pression with priming and spatially adaptive bit rates for re- current networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4385– 4393, 2018. 1",9
Lossy Compression with Distortion Constrained Optimization,CVPR_2020,2,"Oren Rippel and Lubomir Bourdev. Real-time adaptive im- age compression. In Proceedings of the 34th International Conference on Machine Learning-Volume 70 , pages 2922– 2930. JMLR. org, 2017. 2",1
Lossy Compression with Distortion Constrained Optimization,CVPR_2020,3,"Jorge Pessoa, Helena Aidos, Pedro Tom ´as, and M ´ario AT Figueiredo. End-to-end learning of video compression using spatio-temporal autoencoders. 2018. 1",4
Lossy Compression with Distortion Constrained Optimization,CVPR_2020,4,"Alexander A Alemi, Ben Poole, Ian Fischer, Joshua V Dil- lon, Rif A Saurous, and Kevin Murphy. Fixing a bro- ken ELBO, 2018. In International Conference on Machine Learning , 2018. 1,2",6
Lossy Compression with Distortion Constrained Optimization,CVPR_2020,5,"Yoshua Bengio, Nicholas L ´eonard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neu- rons for conditional computation. CoRR , abs/1308.3432, 2013. 3",3
Any-Shot Sequential Anomaly Detection in Surveillance Videos,CVPR_2020,1,"Mich `ele Basseville, Igor V Nikiforov, et al. Detection of abrupt changes: theory and application , volume 104. pren- tice Hall Englewood Cliffs, 1993.",3
Any-Shot Sequential Anomaly Detection in Surveillance Videos,CVPR_2020,2,"Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR) , 41(3):1–58, 2009.",3
Any-Shot Sequential Anomaly Detection in Surveillance Videos,CVPR_2020,3,"Rizwan Chaudhry, Avinash Ravichandran, Gregory Hager, and Ren ´e Vidal. Histograms of oriented optical ﬂow and binet-cauchy kernels on nonlinear dynamical systems for the recognition of human actions. In 2009 IEEE Conference on Computer Vision and Pattern Recognition , pages 1932– 1939. IEEE, 2009.",4
Any-Shot Sequential Anomaly Detection in Surveillance Videos,CVPR_2020,4,"Rensso Victor Hugo Mora Colque, Carlos Caetano, Matheus Toledo Lustosa de Andrade, and William Robson Schwartz. Histograms of optical ﬂow orientation and magnitude and entropy to detect anomalous events in videos. IEEE Trans- actions on Circuits and Systems for Video Technology , 27(3):673–682, 2016.",4
Any-Shot Sequential Anomaly Detection in Surveillance Videos,CVPR_2020,5,"Yang Cong, Junsong Yuan, and Ji Liu. Sparse reconstruction cost for abnormal event detection. In CVPR 2011 , pages 3449–3456. IEEE, 2011.",3
Interactive Multi-Label CNN Learning with Partial Labels,CVPR_2020,1,"A. Kuznetsova, H. Rom, N. Alldrin, J. Uijlings, I. Krasin, J. Pont-Tuset, S. Kamali, S. Popov, M. Malloci, A. Kolesnikov, T. Duerig, and V . Ferrari., “The open images dataset v4: Uniﬁed image classiﬁcation, object detection, and visual relationship detection at scale,” International Journal of Computer Vision , 2016. 1,5",2
Interactive Multi-Label CNN Learning with Partial Labels,CVPR_2020,2,"Y . Guo and S. Gu, “Multi-label classiﬁcation using condi- tional dependency networks,” International Joint Conference on Artiﬁcial Intelligence , 2011. 1,2",2
Interactive Multi-Label CNN Learning with Partial Labels,CVPR_2020,3,"D. Huynh and E. Elhamifar, “Fine-grained generalized zero- shot learning via dense attribute-based attention,” IEEE Con- ference on Computer Vision and Pattern Recognition , 2020. 1",2
Interactive Multi-Label CNN Learning with Partial Labels,CVPR_2020,4,"Y . Gong, Y . Jia, T. Leung, A. Toshev, and S. Ioffe, “Deep convolutional ranking for multilabel image annotation,” In- ternational Conference on Learning Representations , 2013. 1",2
Interactive Multi-Label CNN Learning with Partial Labels,CVPR_2020,5,"M. Guillaumin, T. Mensink, J. Verbeek, and C. Schmid, “Tagprop: Discriminative metric learning in nearest neigh- bor models for image auto-annotation,” International Con- ference on Computer Vision , 2009. 1",2
An Investigation into the Stochasticity of Batch Whitening,CVPR_2020,1,"Lei Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. CoRR , abs/1607.06450, 2016. 1,2,5",3
An Investigation into the Stochasticity of Batch Whitening,CVPR_2020,2,"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR , 2016. 1,2,6,7",4
An Investigation into the Stochasticity of Batch Whitening,CVPR_2020,3,"Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal co- variate shift. In ICML , 2015. 1,2,3,5,6",1
An Investigation into the Stochasticity of Batch Whitening,CVPR_2020,4,"Sanjeev Arora, Zhiyuan Li, and Kaifeng Lyu. Theoretical analysis of auto rate-tuning by batch normalization. In ICLR , 2019. 2",3
An Investigation into the Stochasticity of Batch Whitening,CVPR_2020,5,"Andrei Atanov, Arsenii Ashukha, Dmitry Molchanov, Kirill Neklyudov, and Dmitry Vetrov. Uncertainty estimation via stochastic batch normalization. In ICLR Workshop , 2018. 2, 3",5
Weakly-supervised Domain Adaptation via GAN and Mesh Model,CVPR_2020,1,"Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Augmented skeleton space transfer for depth-based hand pose estimation. In CVPR , 2018. 1,2,6",3
Weakly-supervised Domain Adaptation via GAN and Mesh Model,CVPR_2020,2,"Seungryul Baek, Kwang In Kim, and Tae-Kyun Kim. Pushing the envelope for RGB-based dense 3D hand pose estimation via neural rendering. In CVPR , 2019. 4,5,7",3
Weakly-supervised Domain Adaptation via GAN and Mesh Model,CVPR_2020,3,"Christian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, and Thomas Brox. FreiHand: A dataset for markerless capture of hand pose and shape from single RGB image. In ICCV , 2019. 1,2 6131",6
Weakly-supervised Domain Adaptation via GAN and Mesh Model,CVPR_2020,4,"Seungryul Baek, Zhiyuan Shi, Masato Kawade, and Tae-Kyun Kim. Kinematic-layout-aware random forests for depth-based action recognition. In BMVC , 2017. 3",4
Weakly-supervised Domain Adaptation via GAN and Mesh Model,CVPR_2020,5,"Binod Bhattarai, Seungryul Baek, Rumeysa Bodur, and Tae-Kyun Kim. Sampling strategies for GAN synthetic data. InICASSP , 2020. 2",4
MLCVNet: Multi-Level Context VoteNet for 3D Object Detection,CVPR_2020,1,"Ross Girshick. Fast R-CNN. In Proceedings of the IEEE international conference on computer vision , pages 1440– 1448, 2015. 2",2
MLCVNet: Multi-Level Context VoteNet for 3D Object Detection,CVPR_2020,2,"Yangyan Li, Angela Dai, Leonidas Guibas, and Matthias Nießner. Database-assisted object retrieval for real-time 3D reconstruction. In Computer Graphics Forum , volume 34, pages 435–446. Wiley Online Library, 2015. 3",4
MLCVNet: Multi-Level Context VoteNet for 3D Object Detection,CVPR_2020,3,"Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep Hough voting for 3D object detection in point clouds. arXiv preprint arXiv:1904.09664 , 2019. 1,2,3,4,6",4
MLCVNet: Multi-Level Context VoteNet for 3D Object Detection,CVPR_2020,4,"Shuran Song and Jianxiong Xiao. Deep sliding shapes for amodal 3D object detection in RGB-D images. In Proceed- ings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 808–816, 2016. 1,6",1
MLCVNet: Multi-Level Context VoteNet for 3D Object Detection,CVPR_2020,5,"John McCormac, Ronald Clark, Michael Bloesch, Andrew Davison, and Stefan Leutenegger. Fusion++: V olumetric object-level SLAM. In 2018 International Conference on 3D Vision (3DV) , pages 32–41. IEEE, 2018. 1",5
TRPLP – Trifocal Relative Pose from Lines at Points,CVPR_2020,1,"Adrien Bartoli and Peter Sturm. Structure-from-motion using lines: Representation, triangulation, and bundle ad- justment. Computer vision and image understanding , 100(3):416–441, 2005.",1
TRPLP – Trifocal Relative Pose from Lines at Points,CVPR_2020,2,"Ricardo Fabbri, Peter J. Giblin, and Benjamin B. Kimia. Camera pose estimation using ﬁrst-order curve differential geometry. In Proceedings of the IEEE European Confer- ence in Computer Vision , Lecture Notes in Computer Sci- ence. Springer, 2012.",2
TRPLP – Trifocal Relative Pose from Lines at Points,CVPR_2020,3,"Jonathan D. Hauenstein and Margaret H. Regan. Adaptive strategies for solving parameterized systems using homotopy continuation. Appl. Math. Comput. , 332:19–34, 2018.",2
TRPLP – Trifocal Relative Pose from Lines at Points,CVPR_2020,4,"Q.-T. Luong. Matrice Fondamentale et Calibration Visuelle sur l’Environnement-Vers une plus grande autonomie des systemes robotiques . PhD thesis, Universit ´e de Paris-Sud, Centre d’Orsay, 1992.",2
TRPLP – Trifocal Relative Pose from Lines at Points,CVPR_2020,5,"James Mathews. Multi-focal tensors as invariant differential forms. arXiv e-prints , page arXiv:1610.04294, Oct 2016.",2
Extensions and limitations of randomized smoothing for robustness guarantees,CVPR_2020,1,"Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, and Aleksander Madry. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705 , 2019.",8
Extensions and limitations of randomized smoothing for robustness guarantees,CVPR_2020,2,"Marc Khoury and Dylan Hadﬁeld-Menell. On the geometry of adversarial examples. arXiv preprint arXiv:1811.00525 , 2018.",1
Extensions and limitations of randomized smoothing for robustness guarantees,CVPR_2020,3,"Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Citeseer, 2009.",2
Extensions and limitations of randomized smoothing for robustness guarantees,CVPR_2020,4,"Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certiﬁed robustness to ad- versarial examples with differential privacy. In 2019 IEEESymposium on Security and Privacy (SP) , pages 656–672. IEEE, 2019.",5
Extensions and limitations of randomized smoothing for robustness guarantees,CVPR_2020,5,"Yusuke Tsuzuku, Issei Sato, and Masashi Sugiyama. Lipschitz-margin training: Scalable certiﬁcation of pertur- bation invariance for deep neural networks. In Advances in Neural Information Processing Systems , pages 6541–6550, 2018.",3
Wavelet Synthesis Net for Disparity Estimation to Synthesize DSLR Calibre,CVPR_2020,1,"H. Lin, C. Chen, S. Bing Kang, and J. Yu, “Depth recovery from light ﬁeld using focal stack symmetry,” in Proceedings of the IEEE International Conference on Computer Vision , pp. 3451–3459, 2015.",2
Wavelet Synthesis Net for Disparity Estimation to Synthesize DSLR Calibre,CVPR_2020,2,"D. Eigen, C. Puhrsch, and R. Fergus, “Depth map pre- diction from a single image using a multi-scale deep net- work,” in Advances in neural information processing sys- tems, pp. 2366–2374, 2014.",2
Wavelet Synthesis Net for Disparity Estimation to Synthesize DSLR Calibre,CVPR_2020,3,"Z. Liang, Y . Feng, Y . Guo, H. Liu, W. Chen, L. Qiao, L. Zhou, and J. Zhang, “Learning for disparity estimation through feature constancy,” 2018.",2
Wavelet Synthesis Net for Disparity Estimation to Synthesize DSLR Calibre,CVPR_2020,4,"H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia, “Pyramid scene parsing network,” in Proceedings of the IEEE conference on computer vision and pattern recognition , pp. 2881–2890, 2017.",2
Wavelet Synthesis Net for Disparity Estimation to Synthesize DSLR Calibre,CVPR_2020,5,"T.-W. Hui, X. Tang, and C. Change Loy, “Liteﬂownet: A lightweight convolutional neural network for optical ﬂow es- timation,” in Proceedings of the IEEE Conference on Com- puter Vision and Pattern Recognition , pp. 8981–8989, 2018.",2
Spatio-Temporal Graph for Video Captioning with Knowledge Distillation,CVPR_2020,1,"Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedan- tam, Saurabh Gupta, Piotr Doll ´ar, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325 , 2015.",7
Spatio-Temporal Graph for Video Captioning with Knowledge Distillation,CVPR_2020,2,"Pallabi Ghosh, Yi Yao, Larry S Davis, and Ajay Divakaran. Stacked spatio-temporal graph convolutional networks for action segmentation. arXiv preprint arXiv:1811.10575 , 2018.",4
Spatio-Temporal Graph for Video Captioning with Knowledge Distillation,CVPR_2020,3,"Ross Girshick. Fast r-cnn. In Proceedings of the IEEE inter- national conference on computer vision , pages 1440–1448, 2015.",2
Spatio-Temporal Graph for Video Captioning with Knowledge Distillation,CVPR_2020,4,"Sergio Guadarrama, Niveda Krishnamoorthy, Girish Malkar- nenkar, Subhashini Venugopalan, Raymond Mooney, Trevor Darrell, and Kate Saenko. Youtube2text: Recognizing and describing arbitrary activities using semantic hierarchies and zero-shot recognition. In Proceedings of the IEEE inter- national conference on computer vision , pages 2712–2719, 2013.[14] Saurabh Gupta, Judy Hoffman, and Jitendra Malik. Cross modal distillation for supervision transfer. In Proceedings of the IEEE conference on computer vision and pattern recog- nition , pages 2827–2836, 2016.",7
Spatio-Temporal Graph for Video Captioning with Knowledge Distillation,CVPR_2020,5,"Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross Gir- shick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision , pages 2961–2969, 2017.",4
TBT: T argeted Neural Network Attack with B it Trojan,CVPR_2020,1,"S. Angizi, Z. He, A. S. Rakin, and D. Fan. Cmp-pim: an energy-efﬁcient comparator-based processing-in-memory neural network accelerator. In Proceedings of the 55th An- nual Design Automation Conference , pages 1–6, 2018. 3",2
TBT: T argeted Neural Network Attack with B it Trojan,CVPR_2020,2,"A. N. Bhagoji, D. Cullina, C. Sitawarin, and P. Mittal. En- hancing robustness of machine learning systems via data transformations. 2018 52nd Annual Conference on Informa- tion Sciences and Systems (CISS) , pages 1–5, 2018. 1",2
TBT: T argeted Neural Network Attack with B it Trojan,CVPR_2020,3,"B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Ed- wards, T. Lee, I. Molloy, and B. Srivastava. Detecting back- door attacks on deep neural networks by activation cluster- ing. arXiv preprint arXiv:1811.03728 , 2018. 2,7,8",2
TBT: T argeted Neural Network Attack with B it Trojan,CVPR_2020,4,"J. Clements and Y . Lao. Hardware trojan attacks on neural networks. arXiv preprint arXiv:1806.05768 , 2018. 2",2
TBT: T argeted Neural Network Attack with B it Trojan,CVPR_2020,5,"L. Cojocar, K. Razavi, C. Giuffrida, and H. Bos. Exploit- ing correcting codes: On the effectiveness of ecc memory against rowhammer attacks. S&P’19 , 2019. 3,8",2
Separating Particulate Matter from a Single Microscopic Image,CVPR_2020,1,"J. Boulanger, C. Kervrann, P. Bouthemy, P. Elbau, J.-B. Sibarita, and J. Salamero. Patch-based non-local functional for denoising ﬂuorescence microscopy image sequences. IEEE Trans. Med. Imag. , 2010. 2",2
Separating Particulate Matter from a Single Microscopic Image,CVPR_2020,2,"A. Buades, B. Coll, and J. M. Morel. A review of image de- noising methods, with a new one. SIAM J. Multiscale Model. Simul. , 2005. 2",2
Separating Particulate Matter from a Single Microscopic Image,CVPR_2020,3,"D. Geman and Chengda Yang. Nonlinear image recovery with half-quadratic regularization. IEEE Tran IP , 1995. 4",2
Separating Particulate Matter from a Single Microscopic Image,CVPR_2020,4,"Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Ma- neesh Kumar Singh, and Ming-Hsuan Yang. Diverse image- to-image translation via disentangled representations. In Eu- ropean Conference on Computer Vision , 2018. 2",5
Separating Particulate Matter from a Single Microscopic Image,CVPR_2020,5,"S. Cakir, D. C. Kahraman, R. Cetin-Atalay, and A. E. Cetin. Contrast enhancement of microscopy images using image phase information. IEEE Access , 6, 2018. 2",2
What does Plate Glass Reveal about Camera Calibration?,CVPR_2020,1,"Antonio Criminisi, Ian Reid, and Andrew Zisserman. Single view metrology. International Journal of Computer Vision , 40(2):123–148, 2000. 1",3
What does Plate Glass Reveal about Camera Calibration?,CVPR_2020,2,"Francisco Abad, Emilio Camahort, and Roberto Vivó. Cam- era calibration using two concentric circles. In International Conference Image Analysis and Recognition , pages 688– 696, 2004. 3",3
What does Plate Glass Reveal about Camera Calibration?,CVPR_2020,3,"Nikolaos Arvanitopoulos, Radhakrishna Achanta, and Sabine Süsstrunk. Single image reﬂection suppression. In Proc. of Computer Vision and Pattern Recognition , pages 1752–1760, 2017. 2",3
What does Plate Glass Reveal about Camera Calibration?,CVPR_2020,4,"Max Born and Emil Wolf. Principles of optics: electromag- netic theory of propagation, interference and diffraction of light. Elsevier , 2013. 3",1
What does Plate Glass Reveal about Camera Calibration?,CVPR_2020,5,"Angel Chang, Angela Dai, Thomas Allen Funkhouser, Ma- ciej Halber, Matthias Niebner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGB-D data in indoor environments. In Proc. of Interna- tional Conference on 3D Vision , pages 667–676. Institute of Electrical and Electronics Engineers Inc., 2018. 8",9
BEDSR-Net: A Deep Shadow Removal Network from a Single Document Image,CVPR_2020,1,"Blender Online Community. Blender - a 3D modelling and rendering package . Blender Foundation, 2018.",2
BEDSR-Net: A Deep Shadow Removal Network from a Single Document Image,CVPR_2020,2,"Xiaowei Hu, Yitong Jiang, Chi-Wing Fu, and Pheng-Ann Heng. Mask-ShadowGAN: Learning to remove shadows from unpaired data. In Proceedings of IEEE International Conference on Computer Vision (ICCV) , pages 2472–2481, 2019.",4
BEDSR-Net: A Deep Shadow Removal Network from a Single Document Image,CVPR_2020,3,"Xiaowei Hu, Lei Zhu, Chi-Wing Fu, Jing Qin, and Pheng- Ann Heng. Direction-aware spatial context features for shadow detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 7454–7462, 2018.",5
BEDSR-Net: A Deep Shadow Removal Network from a Single Document Image,CVPR_2020,4,"Shijian Lu, Ben M. Chen, and Chi Chung Ko. Perspec- tive rectiﬁcation of document images using fuzzy set and morphological operations. Image and Vision Computing , 23(5):541–553, May 2005.",2
BEDSR-Net: A Deep Shadow Removal Network from a Single Document Image,CVPR_2020,5,"Steve Bako, Soheil Darabi, Eli Shechtman, Jue Wang, Kalyan Sunkavalli, and Pradeep Sen. Removing shadows from images of documents. In Proceedings of Asian Confer- ence on Computer Vision (ACCV) , pages 173–183, 2016.",6
Explainable Object-induced Action Decision for Autonomous Vehicles,CVPR_2020,1,"Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classiﬁcation. In Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 3156–3164, 2017.",8
Explainable Object-induced Action Decision for Autonomous Vehicles,CVPR_2020,2,"Jinkyu Kim and John Canny. Interpretable learning for self- driving cars by visualizing causal attention. In Proceed- ings of the IEEE international conference on computer vision (ICCV) , pages 2942–2950, 2017.",1
Explainable Object-induced Action Decision for Autonomous Vehicles,CVPR_2020,3,"Huazhe Xu, Yang Gao, Fisher Yu, and Trevor Darrell. End- to-end learning of driving models from large-scale video datasets. In Proceedings of the IEEE conference on com- puter vision and pattern recognition (CVPR) , pages 2174– 2182, 2017.",4
Explainable Object-induced Action Decision for Autonomous Vehicles,CVPR_2020,4,"Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. Textual explanations for self-driving ve- hicles. In Proceedings of the European conference on com- puter vision (ECCV) , pages 563–578, 2018.",5
Explainable Object-induced Action Decision for Autonomous Vehicles,CVPR_2020,5,"Dequan Wang, Coline Devin, Qi-Zhi Cai, Philipp Kr¨ahenb ¨uhl, and Trevor Darrell. Monocular plan view networks for autonomous driving. arXiv preprint arXiv:1905.06937 , 2019.",5
Detection and Classiﬁcation of Pollen Grain Microscope Images,CVPR_2020,1,"Principles and methods for automated palynology. New Phy- tol, (1996):1–8, 2014. 1",1
Detection and Classiﬁcation of Pollen Grain Microscope Images,CVPR_2020,2,"Pollen and spore monitoring in the world. Clinical and Translational Allergy , 8(1):1–5, 2018. 1",1
Detection and Classiﬁcation of Pollen Grain Microscope Images,CVPR_2020,3,"C. M. Bishop. Pattern Recognition and Machine learning . Springer, 2006. 4",2
Detection and Classiﬁcation of Pollen Grain Microscope Images,CVPR_2020,4,"N. A Chinchor and B. Sundheim. Message understanding conference (muc) tests of discourse processing. In Proc. AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation , pages 21–26, 1995. 5",2
Detection and Classiﬁcation of Pollen Grain Microscope Images,CVPR_2020,5,"C. Cortes and V . Vapnik. Support-vector networks. Machine Learning , 20(3):273–297, 1995. 5[6] N. Dalal and B. Triggs. Histograms of oriented gradi- ents for human detection. In 2005 IEEE Computer Soci- ety Conference on Computer Vision and Pattern Recognition (CVPR’05) , volume 1, pages 886–893. IEEE, 2005. 4",2
Role of Spatial Context in Adversarial Robustness for Object Detection,CVPR_2020,1,"Santosh K Divvala, Derek Hoiem, James H Hays, Alexei A Efros, and Martial Hebert. An empirical study of context in object detection. In Computer Vision and Pattern Recogni- tion, 2009. CVPR 2009. IEEE Conference on , pages 1271– 1278. IEEE, 2009. 1,3",5
Role of Spatial Context in Adversarial Robustness for Object Detection,CVPR_2020,2,"Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recog- nition (CVPR) , 2012. 5",3
Role of Spatial Context in Adversarial Robustness for Object Detection,CVPR_2020,3,"Geremy Heitz and Daphne Koller. Learning spatial context: Using stuff to ﬁnd things. In European conference on com- puter vision , pages 30–43. Springer, 2008. 3",1
Role of Spatial Context in Adversarial Robustness for Object Detection,CVPR_2020,4,"Mark Lee and Zico Kolter. On physical adversarial patches for object detection. arXiv preprint arXiv:1906.11897 , 2019. 2[16] Xin Li and Fuxin Li. Adversarial examples detection in deep networks with convolutional ﬁlter statistics. In Proceedings of the IEEE International Conference on Computer Vision , pages 5764–5772, 2017. 7",1
Role of Spatial Context in Adversarial Robustness for Object Detection,CVPR_2020,5,"Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European con- ference on computer vision , pages 21–37. Springer, 2016. 1",7
Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection,CVPR_2020,1,"Pedro Antonio Guti ´errez, Maria Perez-Ortiz, Javier Sanchez- Monedero, Francisco Fernandez-Navarro, and Cesar Hervas- Martinez. Ordinal regression methods: survey and experi- mental study. IEEE Trans. Knowl. Data Eng. , 28(1):127– 146, 2016.",5
Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection,CVPR_2020,2,"Peter McCullagh. Regression models for ordinal data. J. Royal Statistical Society: Series B (Methodological) , 42(2):109–127, 1980.",2
Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection,CVPR_2020,3,"Trong Nguyen Nguyen and Jean Meunier. Anomaly detec- tion in video sequence with appearance-motion correspon- dence. In Proc. IEEE Int. Conf. Comp. Vis. , 2019. 12181",1
Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection,CVPR_2020,4,"Karen Simonyan and Andrew Zisserman. Very deep convo- lutional networks for large-scale image recognition. In Proc. Int. Conf. Learn. Representations , 2015.",1
Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection,CVPR_2020,5,"Ying Zhang, Huchuan Lu, Lihe Zhang, Xiang Ruan, and Shun Sakai. Video anomaly detection based on locality sensitive hashing ﬁlters. Pattern Recognition , 59:302–311, 2016.",5
What Can Be Transferred: Unsupervised Domain Adaptation for,CVPR_2020,1,"Hans E. Atlason, Askell Love, Sigurur Sigursson, Vilmundur Gudnason, and Lotta Maria Ellingsen. Unsupervised brain lesion segmentation from mri using a convolutional autoen- coder. In Medical Imaging: Image Processing , 2018. 2",2
What Can Be Transferred: Unsupervised Domain Adaptation for,CVPR_2020,2,"Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine Learning , 79:151–175, May 2010. 6",6
What Can Be Transferred: Unsupervised Domain Adaptation for,CVPR_2020,3,"Alex Alemi, Ian Fischer, Josh Dillon, and Kevin Murphy. Deep variational information bottleneck. In ICLR , 2017. 2, 4",4
What Can Be Transferred: Unsupervised Domain Adaptation for,CVPR_2020,4,"Caroline Baillard, Pierre Hellier, and Christian Barillot. Seg- mentation of brain 3d mr images using level sets and dense registration. Medical Image Analysis , Oct 2001. 2",3
What Can Be Transferred: Unsupervised Domain Adaptation for,CVPR_2020,5,"Christoph Baur, Benedikt Wiestler, Shadi Albarqouni, and Nassir Navab. Fusing unsupervised and supervised deep learning for white matter lesion segmentation. In Proceed- ings of The 2nd International Conference on Medical Imag- ing with Deep Learning , volume 102, pages 63–72, Jul 2019. 2",4
Cross-Modal Pattern-Propagation for RGB-T Tracking,CVPR_2020,1,"Xiangyuan Lan, Mang Ye, Shengping Zhang, Huiyu Zhou, and Pong C Yuen. Modality-correlation-aware sparse repre- sentation for rgb-infrared object tracking. Pattern Recogni- tion Letters , 2018.",5
Cross-Modal Pattern-Propagation for RGB-T Tracking,CVPR_2020,2,"Chenglong Li, Xiaohao Wu, Nan Zhao, Xiaochun Cao, and Jin Tang. Fusing two-stream convolutional neural networks for rgb-t object tracking. Neurocomputing , 281:78–85, 2018.",5
Cross-Modal Pattern-Propagation for RGB-T Tracking,CVPR_2020,3,"Zhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, and Jian Yang. Pattern-afﬁnitive propagation across depth, surface normal and semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 4106–4115, 2019.",6
Cross-Modal Pattern-Propagation for RGB-T Tracking,CVPR_2020,4,"Zheng Zhu, Wei Wu, Wei Zou, and Junjie Yan. End-to- end ﬂow correlation tracking with spatial-temporal attention. InProceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 548–557, 2018. 7073",4
Cross-Modal Pattern-Propagation for RGB-T Tracking,CVPR_2020,5,"Sean Bell, C Lawrence Zitnick, Kavita Bala, and Ross Gir- shick. Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks. In Proceed- ings of the IEEE conference on computer vision and pattern recognition , pages 2874–2883, 2016.",4
Incremental Learning In Online Scenario,CVPR_2020,1,"Michael McCloskey and Neal J Cohen. Catastrophic inter- ference in connectionist networks: The sequential learning problem. In Psychology of Learning and Motivation , vol- ume 24, pages 109–165. Elsevier, 1989.",1
Incremental Learning In Online Scenario,CVPR_2020,2,"Max Welling. Herding dynamical weights to learn. Proceed- ings of the International Conference on Machine Learning , pages 1121–1128, 2009.",2
Incremental Learning In Online Scenario,CVPR_2020,3,"Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 – mining discriminative components with random forests. Proceedings of the European Conference on Com- puter Vision , 2014.",3
Incremental Learning In Online Scenario,CVPR_2020,4,"Francisco M. Castro, Manuel J. Marin-Jimenez, Nicolas Guil, Cordelia Schmid, and Karteek Alahari. End-to-end in- cremental learning. Proceedings of the European Conference on Computer Vision , September 2018.",2
Incremental Learning In Online Scenario,CVPR_2020,5,"Gert Cauwenberghs and Tomaso Poggio. Incremental and decremental support vector machine learning. Proceedings of the Advances in Neural Information Processing Systems , pages 409–415, 2001.",1
RevealNet: Seeing Behind Objects in RGB-D Scans,CVPR_2020,1,"Ji Hou, Angela Dai, and Matthias Nießner. 3d-sis: 3d seman- tic instance segmentation of rgb-d scans. In Proc. ComputerVision and Pattern Recognition (CVPR), IEEE , 2019. 2,3, 4,5,7,8",3
RevealNet: Seeing Behind Objects in RGB-D Scans,CVPR_2020,2,"Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Hal- ber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGB- D data in indoor environments. International Conference on 3D Vision (3DV) , 2017. 2",9
RevealNet: Seeing Behind Objects in RGB-D Scans,CVPR_2020,3,"Angela Dai, Angel X. Chang, Manolis Savva, Maciej Hal- ber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE , 2017. 2,5,6,7,8",2
RevealNet: Seeing Behind Objects in RGB-D Scans,CVPR_2020,4,"Brian Curless and Marc Levoy. A volumetric method for building complex models from range images. In Proceedings of the 23rd annual conference on Computer graphics and interactive techniques , pages 303–312. ACM, 1996. 5",1
RevealNet: Seeing Behind Objects in RGB-D Scans,CVPR_2020,5,"Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X. Chang, and Matthias Nießner. Scan2cad: Learning cad model alignment in rgb-d scans. In Proc. Com- puter Vision and Pattern Recognition (CVPR), IEEE , 2019. 2,5,6,8",5
Time Flies: Animating a Still Image with Time-Lapse Video as Reference,CVPR_2020,1,"Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pat- tern Recognition (CVPR) , 2009.",6
Time Flies: Animating a Still Image with Time-Lapse Video as Reference,CVPR_2020,2,"Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems (NIPS) , 2014.",8
Time Flies: Animating a Still Image with Time-Lapse Video as Reference,CVPR_2020,3,"Leon A Gatys, Alexander S Ecker, and Matthias Bethge. Im- age style transfer using convolutional neural networks. In IEEE Conference on Computer Vision and Pattern Recogni- tion (CVPR) , 2016.",3
Time Flies: Animating a Still Image with Time-Lapse Video as Reference,CVPR_2020,4,"Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In IEEE In- ternational Conference on Computer Vision (ICCV) , 2017.",1
Time Flies: Animating a Still Image with Time-Lapse Video as Reference,CVPR_2020,5,"Aliaksandr Siarohin, St ´ephane Lathuili `ere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Animating arbitrary objects via deep motion transfer. In IEEE Conference on Computer Vi- sion and Pattern Recognition (CVPR) , 2019.",5
Attribute Aware Filter-Drop for Bias-Invariant Classiﬁcation,CVPR_2020,1,"Manish Raghavan, Solon Barocas, Jon Kleinberg, and Karen Levy. Mitigating bias in algorithmic hiring: Evaluating claims and practices. In Conference on Fairness, Account- ability, and Transparency , page 469–481, 2020.",4
Attribute Aware Filter-Drop for Bias-Invariant Classiﬁcation,CVPR_2020,2,"Mohsan S. Alvi, Andrew Zisserman, and Christoffer Nell˚aker. Turning a blind eye: Explicit removal of biases and variation from deep neural network embeddings. In Laura Leal-Taix ´e and Stefan Roth, editors, European Conference on Computer Vision Workshops , pages 556–572, 2018.",2
Attribute Aware Filter-Drop for Bias-Invariant Classiﬁcation,CVPR_2020,3,"Alexander Amini, Ava P. Soleimany, Wilko Schwarting, Sangeeta N. Bhatia, and Daniela Rus. Uncovering and mit- igating algorithmic bias through learned latent structure. In AAAI/ACM Conference on AI, Ethics, and Society , page 289–295, 2019.",2
Attribute Aware Filter-Drop for Bias-Invariant Classiﬁcation,CVPR_2020,4,"Solon Barocas and Andrew D Selbst. Big data’s disparate impact. California Law Review , 104:671, 2016.",1
Attribute Aware Filter-Drop for Bias-Invariant Classiﬁcation,CVPR_2020,5,"Joy Buolamwini and Timnit Gebru. Gender shades: Inter- sectional accuracy disparities in commercial gender classiﬁ- cation. In Conference on Fairness, Accountability and Trans- parency , pages 77–91, 2018.",1
Density-Aware Feature Embedding for Face Clustering,CVPR_2020,1,"Bryan Perozzi, Rami Alrfou, and Steven Skiena. Deepwalk: Online learning of social representations. In Acm Sigkdd In- ternational Conference on Knowledge Discovery and Data Mining , 2014. 3",3
Density-Aware Feature Embedding for Face Clustering,CVPR_2020,2,"Robin Sibson. Slink: an optimally efﬁcient algorithm for the single-link cluster method. The computer journal , 16(1):30– 34, 1973. 7,8",2
Density-Aware Feature Embedding for Face Clustering,CVPR_2020,3,"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems , pages 5998–6008, 2017. 4",8
Density-Aware Feature Embedding for Face Clustering,CVPR_2020,4,"Kiran K Thekumparampil, Chong Wang, Sewoong Oh, and Li-Jia Li. Attention-based graph neural network for semi- supervised learning. arXiv preprint arXiv:1803.03735 , 2018. 2",4
Density-Aware Feature Embedding for Face Clustering,CVPR_2020,5,"Petar Veli ˇckovi ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph at- tention networks. In International Conference on Learning Representations , 2018. 2",6
Learning Physics-guided Face Relighting under Directional Light,CVPR_2020,1,"Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial net- works. In IEEE Conference on Computer Vision and Pattern Recognition , 2017. 1,2,5,6",4
Learning Physics-guided Face Relighting under Directional Light,CVPR_2020,2,"Balazs Kovacs, Sean Bell, Noah Snavely, and Kavita Bala. Shading annotations in the wild. In IEEE Conference on Computer Vision and Pattern Recognition , 2017. 2",4
Learning Physics-guided Face Relighting under Directional Light,CVPR_2020,3,"Chen Li, Kun Zhou, and Stephen Lin. Intrinsic face image de- composition with human face priors. In European Conference on Computer Vision , 2014. 2",3
Learning Physics-guided Face Relighting under Directional Light,CVPR_2020,4,"Sameer Agarwal, Ravi Ramamoorthi, Serge Belongie, and Henrik Wann Jensen. Structured importance sampling of environment maps. ACM Transactions on Graphics (TOG) , 22(3):605–612, 2003. 7",4
Learning Physics-guided Face Relighting under Directional Light,CVPR_2020,5,"Jonathan T Barron and Jitendra Malik. Shape, illumination, and reﬂectance from shading. IEEE Transactions on Pattern Analysis and Machine Intelligence , 37(8):1670–1687, 2015. 1,2",1
